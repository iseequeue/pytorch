{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1rm10nr8psZIUdEBRcVSj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iseequeue/pytorch/blob/main/01_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dcmJzceEp5-5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "what_were_covering = {1: \"data (prepare and load)\",\n",
        "                      2: \"build model\",\n",
        "                      3: \"fitting the model to data (training)\",\n",
        "                      4: \"making predictions and evaluating a model (inference)\",\n",
        "                      5: \"saving and loading model\",\n",
        "                      6: \"putting all together\"}\n",
        "what_were_covering"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB-qrKaUrDKp",
        "outputId": "5a5fd4f0-6876-4e86-c999-afcf001dbe61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'data (prepare and load)',\n",
              " 2: 'build model',\n",
              " 3: 'fitting the model to data (training)',\n",
              " 4: 'making predictions and evaluating a model (inference)',\n",
              " 5: 'saving and loading model',\n",
              " 6: 'putting all together'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn \n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XIHPE7E0rgNm",
        "outputId": "f35a1c3d-36ae-4067-aef1-02d2c85b756c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1+cu116'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data (preparing and loading)"
      ],
      "metadata": {
        "id": "C5eyOKlCs3Za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create known parameters\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "\n",
        "#create data\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
        "y = weight * X + bias\n",
        "\n",
        "X[:10], y[10:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKtiTnEYsewz",
        "outputId": "2ee0c4ca-d9a7-49d0-a96b-2aa186bcdd32"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0000],\n",
              "         [0.0200],\n",
              "         [0.0400],\n",
              "         [0.0600],\n",
              "         [0.0800],\n",
              "         [0.1000],\n",
              "         [0.1200],\n",
              "         [0.1400],\n",
              "         [0.1600],\n",
              "         [0.1800]]), tensor([[0.4400],\n",
              "         [0.4540],\n",
              "         [0.4680],\n",
              "         [0.4820],\n",
              "         [0.4960],\n",
              "         [0.5100],\n",
              "         [0.5240],\n",
              "         [0.5380],\n",
              "         [0.5520],\n",
              "         [0.5660],\n",
              "         [0.5800],\n",
              "         [0.5940],\n",
              "         [0.6080],\n",
              "         [0.6220],\n",
              "         [0.6360],\n",
              "         [0.6500],\n",
              "         [0.6640],\n",
              "         [0.6780],\n",
              "         [0.6920],\n",
              "         [0.7060],\n",
              "         [0.7200],\n",
              "         [0.7340],\n",
              "         [0.7480],\n",
              "         [0.7620],\n",
              "         [0.7760],\n",
              "         [0.7900],\n",
              "         [0.8040],\n",
              "         [0.8180],\n",
              "         [0.8320],\n",
              "         [0.8460],\n",
              "         [0.8600],\n",
              "         [0.8740],\n",
              "         [0.8880],\n",
              "         [0.9020],\n",
              "         [0.9160],\n",
              "         [0.9300],\n",
              "         [0.9440],\n",
              "         [0.9580],\n",
              "         [0.9720],\n",
              "         [0.9860]]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X), len(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IbHdWSRs0VY",
        "outputId": "5fe03dbc-c563-4be3-ead3-2db98e4ce548"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Splitting data in train and test\n",
        "train_split = int(0.8 * len(X))\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDXrpS5o1S-8",
        "outputId": "cd2b3722-b940-4dc0-d015-36e74d3eae84"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 40, 10, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(train_data = X_train,\n",
        "                     train_labels=y_train, \n",
        "                     test_data=X_test,\n",
        "                     test_labels=y_test,\n",
        "                     predictions=None):\n",
        "  '''\n",
        "  Plots training data, test data and compares predictions\n",
        "  '''\n",
        "  plt.figure(figsize=(10, 7))\n",
        "  #Plot training data in blue\n",
        "  plt.scatter(train_data, train_labels, c='b', s=4, label=\"Training data\")\n",
        "  #Plot test data in green\n",
        "  plt.scatter(test_data, test_labels, c='green', s=4, label=\"Test data\")\n",
        "\n",
        "  if predictions is not None:\n",
        "    plt.scatter(test_data, predictions, c='red', s=4, label=\"Predictions\")\n",
        "  plt.legend(prop={\"size\":14});\n",
        "\n"
      ],
      "metadata": {
        "id": "XnE__sMu2NXN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "-8t1-Y8W4OJy",
        "outputId": "36d70bab-063b-48ad-f349-d7faf3cefce0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. build pytorch model\n",
        "class LinearRegressionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.weights = nn.Parameter(torch.randn(1,\n",
        "                                            requires_grad=True,\n",
        "                                            dtype=torch.float))\n",
        "    self.bias = nn.Parameter(torch.randn(1,\n",
        "                                          requires_grad=True,\n",
        "                                          dtype=torch.float))\n",
        "  #Forward is to define computation in the model\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    return self.weights * x + self.bias\n",
        "    \n"
      ],
      "metadata": {
        "id": "MWZmrVZj4SG2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### checking contents\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model0 = LinearRegressionModel()\n",
        "list(model0.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbWkw6my7YLM",
        "outputId": "aff26a62-5e2b-4a25-f0ab-313f1b867127"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([0.3367], requires_grad=True), Parameter containing:\n",
              " tensor([0.1288], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPuREcctAKQo",
        "outputId": "5c9b9687-e2ac-4169-c456-4689aeda1a7b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions with model"
      ],
      "metadata": {
        "id": "90Q_X1M5Cd6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtmJUIgqC1mF",
        "outputId": "c247d1e2-2156-4a02-ad46-6ad568955612"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.8000],\n",
              "         [0.8200],\n",
              "         [0.8400],\n",
              "         [0.8600],\n",
              "         [0.8800],\n",
              "         [0.9000],\n",
              "         [0.9200],\n",
              "         [0.9400],\n",
              "         [0.9600],\n",
              "         [0.9800]]), tensor([[0.8600],\n",
              "         [0.8740],\n",
              "         [0.8880],\n",
              "         [0.9020],\n",
              "         [0.9160],\n",
              "         [0.9300],\n",
              "         [0.9440],\n",
              "         [0.9580],\n",
              "         [0.9720],\n",
              "         [0.9860]]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode(): #with torch.no_grad()\n",
        "  y_preds = model0(X_test)\n",
        "y_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ4IiuX5ChBT",
        "outputId": "a5ba24d7-c95a-46a8-b59e-085f43b04d62"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3982],\n",
              "        [0.4049],\n",
              "        [0.4116],\n",
              "        [0.4184],\n",
              "        [0.4251],\n",
              "        [0.4318],\n",
              "        [0.4386],\n",
              "        [0.4453],\n",
              "        [0.4520],\n",
              "        [0.4588]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD8ZFW3DANNg",
        "outputId": "43b00cf7-d32d-418f-c37c-bd74eaf42b8c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8600],\n",
              "        [0.8740],\n",
              "        [0.8880],\n",
              "        [0.9020],\n",
              "        [0.9160],\n",
              "        [0.9300],\n",
              "        [0.9440],\n",
              "        [0.9580],\n",
              "        [0.9720],\n",
              "        [0.9860]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(predictions = y_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "VOekNYmDEVaE",
        "outputId": "daee161a-9462-4ce4-cbd5-2b7b844bf3fb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "netSGXEAfsAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "optimizer_fn = torch.optim.Adam(model0.parameters())\n",
        "\n"
      ],
      "metadata": {
        "id": "HUjhYJJBEgyB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a training loop\n",
        "\n",
        "0. Loop through the data\n",
        "1. Forward pass\n",
        "2. Calculate the loss\n",
        "3. Optimizer zero grad\n",
        "4. Loss backward\n",
        "5. Optimizer step"
      ],
      "metadata": {
        "id": "zz1pLZxlkAqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(model0.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik4nYgxqky5c",
        "outputId": "2047825b-b3aa-4bf1-aa45-6a587fa5bac5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([0.3367], requires_grad=True), Parameter containing:\n",
              " tensor([0.1288], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  print(list(model0.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tcpvxav_lI23",
        "outputId": "d13982e3-5a18-4928-b5e2-a57653f4ddc4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([0.3367], requires_grad=True), Parameter containing:\n",
            "tensor([0.1288], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "model0 = LinearRegressionModel()\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "optimizer_fn = torch.optim.Adam(model0.parameters())\n",
        "\n",
        "epochs = 1000\n",
        "\n",
        "epoch_count=[]\n",
        "loss_values=[]\n",
        "test_loss_values = []\n",
        "\n",
        "\n",
        "### Training\n",
        "#0. loop through the data\n",
        "for epoch in range(epochs):\n",
        "  model0.train()\n",
        "\n",
        "  #1. forward\n",
        "  y_pred = model0(X_train)\n",
        "  #2. calc loss\n",
        "  loss = loss_fn(y_pred, y_train)\n",
        "  #3. optimizer zero grad\n",
        "  optimizer_fn.zero_grad()\n",
        "  #4. Backpropagation\n",
        "  loss.backward()\n",
        "  #5. Step optimizer\n",
        "  optimizer_fn.step()\n",
        "  #Testing\n",
        "  model0.eval()\n",
        "  #print(list(model0.parameters()))\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    test_pred = model0(X_test)\n",
        "    test_loss=loss_fn(test_pred, y_test)\n",
        "  print(f\"Epoch: {epoch :<3} | Loss: {loss:<25} | Test loss: {test_loss}\")\n",
        "\n",
        "  epoch_count.append(epoch)\n",
        "  loss_values.append(loss)\n",
        "  test_loss_values.append(test_loss)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvGhXvxThpER",
        "outputId": "6123a386-3a98-4695-aca6-9a2196feea83"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0   | Loss: 0.31288138031959534       | Test loss: 0.49264615774154663\n",
            "Epoch: 1   | Loss: 0.31149134039878845       | Test loss: 0.49075621366500854\n",
            "Epoch: 2   | Loss: 0.31010133028030396       | Test loss: 0.4888661801815033\n",
            "Epoch: 3   | Loss: 0.30871134996414185       | Test loss: 0.4869762361049652\n",
            "Epoch: 4   | Loss: 0.30732136964797974       | Test loss: 0.48508620262145996\n",
            "Epoch: 5   | Loss: 0.3059313893318176        | Test loss: 0.4831962585449219\n",
            "Epoch: 6   | Loss: 0.30454137921333313       | Test loss: 0.4813062250614166\n",
            "Epoch: 7   | Loss: 0.30315136909484863       | Test loss: 0.47941628098487854\n",
            "Epoch: 8   | Loss: 0.30176135897636414       | Test loss: 0.4775262773036957\n",
            "Epoch: 9   | Loss: 0.30037134885787964       | Test loss: 0.4756363034248352\n",
            "Epoch: 10  | Loss: 0.29898136854171753       | Test loss: 0.47374624013900757\n",
            "Epoch: 11  | Loss: 0.2975913882255554        | Test loss: 0.4718562960624695\n",
            "Epoch: 12  | Loss: 0.29620134830474854       | Test loss: 0.46996623277664185\n",
            "Epoch: 13  | Loss: 0.2948114275932312        | Test loss: 0.46807631850242615\n",
            "Epoch: 14  | Loss: 0.2934213876724243        | Test loss: 0.4661863446235657\n",
            "Epoch: 15  | Loss: 0.2920313775539398        | Test loss: 0.46429628133773804\n",
            "Epoch: 16  | Loss: 0.2906413972377777        | Test loss: 0.46240633726119995\n",
            "Epoch: 17  | Loss: 0.2892513871192932        | Test loss: 0.4605163037776947\n",
            "Epoch: 18  | Loss: 0.2878614068031311        | Test loss: 0.4586263597011566\n",
            "Epoch: 19  | Loss: 0.286471426486969         | Test loss: 0.45673638582229614\n",
            "Epoch: 20  | Loss: 0.2850814163684845        | Test loss: 0.4548463821411133\n",
            "Epoch: 21  | Loss: 0.28369140625             | Test loss: 0.45295634865760803\n",
            "Epoch: 22  | Loss: 0.2823014259338379        | Test loss: 0.45106640458106995\n",
            "Epoch: 23  | Loss: 0.280911386013031         | Test loss: 0.4491764008998871\n",
            "Epoch: 24  | Loss: 0.2795214056968689        | Test loss: 0.4472864270210266\n",
            "Epoch: 25  | Loss: 0.2781314253807068        | Test loss: 0.44539642333984375\n",
            "Epoch: 26  | Loss: 0.2767414152622223        | Test loss: 0.4435064196586609\n",
            "Epoch: 27  | Loss: 0.2753514349460602        | Test loss: 0.4416164457798004\n",
            "Epoch: 28  | Loss: 0.2739613950252533        | Test loss: 0.43972641229629517\n",
            "Epoch: 29  | Loss: 0.27257147431373596       | Test loss: 0.4378364682197571\n",
            "Epoch: 30  | Loss: 0.2711814343929291        | Test loss: 0.4359464645385742\n",
            "Epoch: 31  | Loss: 0.26979145407676697       | Test loss: 0.43405646085739136\n",
            "Epoch: 32  | Loss: 0.26840144395828247       | Test loss: 0.4321664273738861\n",
            "Epoch: 33  | Loss: 0.26701146364212036       | Test loss: 0.430276483297348\n",
            "Epoch: 34  | Loss: 0.26562145352363586       | Test loss: 0.42838650941848755\n",
            "Epoch: 35  | Loss: 0.26423147320747375       | Test loss: 0.4264965057373047\n",
            "Epoch: 36  | Loss: 0.26284143328666687       | Test loss: 0.4246065020561218\n",
            "Epoch: 37  | Loss: 0.26145145297050476       | Test loss: 0.42271652817726135\n",
            "Epoch: 38  | Loss: 0.26006144285202026       | Test loss: 0.4208265244960785\n",
            "Epoch: 39  | Loss: 0.25867146253585815       | Test loss: 0.418936550617218\n",
            "Epoch: 40  | Loss: 0.25728148221969604       | Test loss: 0.41704654693603516\n",
            "Epoch: 41  | Loss: 0.25589147210121155       | Test loss: 0.4151565432548523\n",
            "Epoch: 42  | Loss: 0.25450146198272705       | Test loss: 0.4132665693759918\n",
            "Epoch: 43  | Loss: 0.25311148166656494       | Test loss: 0.41137656569480896\n",
            "Epoch: 44  | Loss: 0.25172150135040283       | Test loss: 0.4094866216182709\n",
            "Epoch: 45  | Loss: 0.25033149123191833       | Test loss: 0.4075965881347656\n",
            "Epoch: 46  | Loss: 0.24894149601459503       | Test loss: 0.40570664405822754\n",
            "Epoch: 47  | Loss: 0.24755148589611053       | Test loss: 0.4038166403770447\n",
            "Epoch: 48  | Loss: 0.24616149067878723       | Test loss: 0.4019266664981842\n",
            "Epoch: 49  | Loss: 0.24477151036262512       | Test loss: 0.40003663301467896\n",
            "Epoch: 50  | Loss: 0.24338150024414062       | Test loss: 0.3981466591358185\n",
            "Epoch: 51  | Loss: 0.24199151992797852       | Test loss: 0.396256685256958\n",
            "Epoch: 52  | Loss: 0.24060149490833282       | Test loss: 0.39436668157577515\n",
            "Epoch: 53  | Loss: 0.23921151459217072       | Test loss: 0.3924767076969147\n",
            "Epoch: 54  | Loss: 0.23782150447368622       | Test loss: 0.3905867040157318\n",
            "Epoch: 55  | Loss: 0.23643150925636292       | Test loss: 0.38869670033454895\n",
            "Epoch: 56  | Loss: 0.2350415289402008        | Test loss: 0.3868066966533661\n",
            "Epoch: 57  | Loss: 0.2336515188217163        | Test loss: 0.3849167227745056\n",
            "Epoch: 58  | Loss: 0.2322615385055542        | Test loss: 0.38302671909332275\n",
            "Epoch: 59  | Loss: 0.2308715283870697        | Test loss: 0.3811367452144623\n",
            "Epoch: 60  | Loss: 0.2294815331697464        | Test loss: 0.3792467415332794\n",
            "Epoch: 61  | Loss: 0.2280915230512619        | Test loss: 0.37735676765441895\n",
            "Epoch: 62  | Loss: 0.2267015427350998        | Test loss: 0.3754667639732361\n",
            "Epoch: 63  | Loss: 0.2253115475177765        | Test loss: 0.37357673048973083\n",
            "Epoch: 64  | Loss: 0.2239215075969696        | Test loss: 0.37168678641319275\n",
            "Epoch: 65  | Loss: 0.2225315272808075        | Test loss: 0.3697967827320099\n",
            "Epoch: 66  | Loss: 0.22114154696464539       | Test loss: 0.3679068088531494\n",
            "Epoch: 67  | Loss: 0.21975155174732208       | Test loss: 0.36601680517196655\n",
            "Epoch: 68  | Loss: 0.21836154162883759       | Test loss: 0.3641268312931061\n",
            "Epoch: 69  | Loss: 0.21697156131267548       | Test loss: 0.3622368276119232\n",
            "Epoch: 70  | Loss: 0.21558156609535217       | Test loss: 0.36034685373306274\n",
            "Epoch: 71  | Loss: 0.21419155597686768       | Test loss: 0.35845687985420227\n",
            "Epoch: 72  | Loss: 0.21280157566070557       | Test loss: 0.356566846370697\n",
            "Epoch: 73  | Loss: 0.21141156554222107       | Test loss: 0.35467687249183655\n",
            "Epoch: 74  | Loss: 0.21002157032489777       | Test loss: 0.3527868688106537\n",
            "Epoch: 75  | Loss: 0.20863159000873566       | Test loss: 0.3508968651294708\n",
            "Epoch: 76  | Loss: 0.20724157989025116       | Test loss: 0.34900689125061035\n",
            "Epoch: 77  | Loss: 0.20585159957408905       | Test loss: 0.3471168875694275\n",
            "Epoch: 78  | Loss: 0.20446157455444336       | Test loss: 0.3452269434928894\n",
            "Epoch: 79  | Loss: 0.20307156443595886       | Test loss: 0.34333691000938416\n",
            "Epoch: 80  | Loss: 0.20168156921863556       | Test loss: 0.3414469063282013\n",
            "Epoch: 81  | Loss: 0.20029160380363464       | Test loss: 0.3395569324493408\n",
            "Epoch: 82  | Loss: 0.19890159368515015       | Test loss: 0.33766692876815796\n",
            "Epoch: 83  | Loss: 0.19751158356666565       | Test loss: 0.3357769548892975\n",
            "Epoch: 84  | Loss: 0.19612160325050354       | Test loss: 0.3338869512081146\n",
            "Epoch: 85  | Loss: 0.19473162293434143       | Test loss: 0.33199697732925415\n",
            "Epoch: 86  | Loss: 0.19334158301353455       | Test loss: 0.3301070034503937\n",
            "Epoch: 87  | Loss: 0.19195160269737244       | Test loss: 0.3282169699668884\n",
            "Epoch: 88  | Loss: 0.19056162238121033       | Test loss: 0.32632702589035034\n",
            "Epoch: 89  | Loss: 0.18917161226272583       | Test loss: 0.3244370222091675\n",
            "Epoch: 90  | Loss: 0.18778161704540253       | Test loss: 0.3225470185279846\n",
            "Epoch: 91  | Loss: 0.18639162182807922       | Test loss: 0.32065707445144653\n",
            "Epoch: 92  | Loss: 0.18500164151191711       | Test loss: 0.3187670409679413\n",
            "Epoch: 93  | Loss: 0.18361161649227142       | Test loss: 0.3168770670890808\n",
            "Epoch: 94  | Loss: 0.18222163617610931       | Test loss: 0.31498706340789795\n",
            "Epoch: 95  | Loss: 0.180831640958786         | Test loss: 0.3130970895290375\n",
            "Epoch: 96  | Loss: 0.1794416457414627        | Test loss: 0.311207115650177\n",
            "Epoch: 97  | Loss: 0.1780516505241394        | Test loss: 0.30931711196899414\n",
            "Epoch: 98  | Loss: 0.1766616404056549        | Test loss: 0.30742713809013367\n",
            "Epoch: 99  | Loss: 0.17527161538600922       | Test loss: 0.3055371344089508\n",
            "Epoch: 100 | Loss: 0.1738816499710083        | Test loss: 0.30364710092544556\n",
            "Epoch: 101 | Loss: 0.1724916696548462        | Test loss: 0.3017571270465851\n",
            "Epoch: 102 | Loss: 0.1711016595363617        | Test loss: 0.2998671233654022\n",
            "Epoch: 103 | Loss: 0.16971167922019958       | Test loss: 0.29797711968421936\n",
            "Epoch: 104 | Loss: 0.1683216542005539        | Test loss: 0.2960871458053589\n",
            "Epoch: 105 | Loss: 0.1669316589832306        | Test loss: 0.2941971719264984\n",
            "Epoch: 106 | Loss: 0.1655416637659073        | Test loss: 0.2923072278499603\n",
            "Epoch: 107 | Loss: 0.16415166854858398       | Test loss: 0.2904171943664551\n",
            "Epoch: 108 | Loss: 0.16276167333126068       | Test loss: 0.2885271906852722\n",
            "Epoch: 109 | Loss: 0.16137167811393738       | Test loss: 0.28663724660873413\n",
            "Epoch: 110 | Loss: 0.15998168289661407       | Test loss: 0.2847472131252289\n",
            "Epoch: 111 | Loss: 0.15859167277812958       | Test loss: 0.2828572392463684\n",
            "Epoch: 112 | Loss: 0.15720167756080627       | Test loss: 0.28096723556518555\n",
            "Epoch: 113 | Loss: 0.15581169724464417       | Test loss: 0.2790772616863251\n",
            "Epoch: 114 | Loss: 0.15442170202732086       | Test loss: 0.2771872580051422\n",
            "Epoch: 115 | Loss: 0.15303169190883636       | Test loss: 0.27529722452163696\n",
            "Epoch: 116 | Loss: 0.15164169669151306       | Test loss: 0.2734072804450989\n",
            "Epoch: 117 | Loss: 0.15025168657302856       | Test loss: 0.271517276763916\n",
            "Epoch: 118 | Loss: 0.14886170625686646       | Test loss: 0.26962730288505554\n",
            "Epoch: 119 | Loss: 0.14747169613838196       | Test loss: 0.26773732900619507\n",
            "Epoch: 120 | Loss: 0.14608171582221985       | Test loss: 0.2658473253250122\n",
            "Epoch: 121 | Loss: 0.14469173550605774       | Test loss: 0.26395732164382935\n",
            "Epoch: 122 | Loss: 0.14330172538757324       | Test loss: 0.26206734776496887\n",
            "Epoch: 123 | Loss: 0.14191174507141113       | Test loss: 0.2601773738861084\n",
            "Epoch: 124 | Loss: 0.14052176475524902       | Test loss: 0.2582874000072479\n",
            "Epoch: 125 | Loss: 0.13913178443908691       | Test loss: 0.25639742612838745\n",
            "Epoch: 126 | Loss: 0.1377418041229248        | Test loss: 0.254507452249527\n",
            "Epoch: 127 | Loss: 0.1363518238067627        | Test loss: 0.2526174783706665\n",
            "Epoch: 128 | Loss: 0.13496185839176178       | Test loss: 0.25072747468948364\n",
            "Epoch: 129 | Loss: 0.13357186317443848       | Test loss: 0.24883751571178436\n",
            "Epoch: 130 | Loss: 0.13218188285827637       | Test loss: 0.2469475269317627\n",
            "Epoch: 131 | Loss: 0.13079190254211426       | Test loss: 0.2450575828552246\n",
            "Epoch: 132 | Loss: 0.12940189242362976       | Test loss: 0.24316760897636414\n",
            "Epoch: 133 | Loss: 0.12801194190979004       | Test loss: 0.24127764999866486\n",
            "Epoch: 134 | Loss: 0.12662193179130554       | Test loss: 0.2393876612186432\n",
            "Epoch: 135 | Loss: 0.12523195147514343       | Test loss: 0.23749768733978271\n",
            "Epoch: 136 | Loss: 0.12384197860956192       | Test loss: 0.23560769855976105\n",
            "Epoch: 137 | Loss: 0.12245200574398041       | Test loss: 0.23371772468090057\n",
            "Epoch: 138 | Loss: 0.1210620179772377        | Test loss: 0.2318277657032013\n",
            "Epoch: 139 | Loss: 0.11967203766107559       | Test loss: 0.22993779182434082\n",
            "Epoch: 140 | Loss: 0.11828205734491348       | Test loss: 0.22804780304431915\n",
            "Epoch: 141 | Loss: 0.11689207702875137       | Test loss: 0.22615781426429749\n",
            "Epoch: 142 | Loss: 0.11550208181142807       | Test loss: 0.224267840385437\n",
            "Epoch: 143 | Loss: 0.11411210149526596       | Test loss: 0.22237786650657654\n",
            "Epoch: 144 | Loss: 0.11272212117910385       | Test loss: 0.22048787772655487\n",
            "Epoch: 145 | Loss: 0.11133214086294174       | Test loss: 0.2185979187488556\n",
            "Epoch: 146 | Loss: 0.10994216054677963       | Test loss: 0.21670794486999512\n",
            "Epoch: 147 | Loss: 0.10855218023061752       | Test loss: 0.21481795608997345\n",
            "Epoch: 148 | Loss: 0.10716219991445541       | Test loss: 0.21292801201343536\n",
            "Epoch: 149 | Loss: 0.1057722344994545        | Test loss: 0.2110380381345749\n",
            "Epoch: 150 | Loss: 0.1043822318315506        | Test loss: 0.20914804935455322\n",
            "Epoch: 151 | Loss: 0.10299225151538849       | Test loss: 0.20725807547569275\n",
            "Epoch: 152 | Loss: 0.10160227119922638       | Test loss: 0.2053680717945099\n",
            "Epoch: 153 | Loss: 0.10021229088306427       | Test loss: 0.20347809791564941\n",
            "Epoch: 154 | Loss: 0.09882231056690216       | Test loss: 0.20158810913562775\n",
            "Epoch: 155 | Loss: 0.09743231534957886       | Test loss: 0.19969816505908966\n",
            "Epoch: 156 | Loss: 0.09604234248399734       | Test loss: 0.197808176279068\n",
            "Epoch: 157 | Loss: 0.09465235471725464       | Test loss: 0.1959182173013687\n",
            "Epoch: 158 | Loss: 0.09326237440109253       | Test loss: 0.19402822852134705\n",
            "Epoch: 159 | Loss: 0.09187239408493042       | Test loss: 0.19213826954364777\n",
            "Epoch: 160 | Loss: 0.09048241376876831       | Test loss: 0.1902482807636261\n",
            "Epoch: 161 | Loss: 0.0890924260020256        | Test loss: 0.188358336687088\n",
            "Epoch: 162 | Loss: 0.0877024382352829        | Test loss: 0.18646833300590515\n",
            "Epoch: 163 | Loss: 0.08631246536970139       | Test loss: 0.18457835912704468\n",
            "Epoch: 164 | Loss: 0.08492248505353928       | Test loss: 0.1826884001493454\n",
            "Epoch: 165 | Loss: 0.08353249728679657       | Test loss: 0.18079841136932373\n",
            "Epoch: 166 | Loss: 0.08214251697063446       | Test loss: 0.17890843749046326\n",
            "Epoch: 167 | Loss: 0.08075253665447235       | Test loss: 0.1770184487104416\n",
            "Epoch: 168 | Loss: 0.07936255633831024       | Test loss: 0.1751284897327423\n",
            "Epoch: 169 | Loss: 0.07797257602214813       | Test loss: 0.17323848605155945\n",
            "Epoch: 170 | Loss: 0.07658259570598602       | Test loss: 0.17134851217269897\n",
            "Epoch: 171 | Loss: 0.07519260793924332       | Test loss: 0.1694585531949997\n",
            "Epoch: 172 | Loss: 0.0738430768251419        | Test loss: 0.1675732433795929\n",
            "Epoch: 173 | Loss: 0.07250753790140152       | Test loss: 0.16569216549396515\n",
            "Epoch: 174 | Loss: 0.07117599248886108       | Test loss: 0.16381482779979706\n",
            "Epoch: 175 | Loss: 0.06984884291887283       | Test loss: 0.16194577515125275\n",
            "Epoch: 176 | Loss: 0.06857859343290329       | Test loss: 0.16008412837982178\n",
            "Epoch: 177 | Loss: 0.06731493771076202       | Test loss: 0.15822914242744446\n",
            "Epoch: 178 | Loss: 0.06605717539787292       | Test loss: 0.15638010203838348\n",
            "Epoch: 179 | Loss: 0.06481904536485672       | Test loss: 0.15454153716564178\n",
            "Epoch: 180 | Loss: 0.06362520158290863       | Test loss: 0.15271231532096863\n",
            "Epoch: 181 | Loss: 0.06243913248181343       | Test loss: 0.1508914679288864\n",
            "Epoch: 182 | Loss: 0.06125998497009277       | Test loss: 0.14907817542552948\n",
            "Epoch: 183 | Loss: 0.06011655181646347       | Test loss: 0.14727696776390076\n",
            "Epoch: 184 | Loss: 0.059002239257097244      | Test loss: 0.14548662304878235\n",
            "Epoch: 185 | Loss: 0.05789627879858017       | Test loss: 0.14370596408843994\n",
            "Epoch: 186 | Loss: 0.05679776147007942       | Test loss: 0.141933873295784\n",
            "Epoch: 187 | Loss: 0.05575118213891983       | Test loss: 0.1401752531528473\n",
            "Epoch: 188 | Loss: 0.054717253893613815      | Test loss: 0.13842862844467163\n",
            "Epoch: 189 | Loss: 0.053691912442445755      | Test loss: 0.13669273257255554\n",
            "Epoch: 190 | Loss: 0.052687883377075195      | Test loss: 0.13497228920459747\n",
            "Epoch: 191 | Loss: 0.05172818899154663       | Test loss: 0.13326576352119446\n",
            "Epoch: 192 | Loss: 0.05077768489718437       | Test loss: 0.13157162070274353\n",
            "Epoch: 193 | Loss: 0.049835383892059326      | Test loss: 0.1298886090517044\n",
            "Epoch: 194 | Loss: 0.04892950877547264       | Test loss: 0.12822170555591583\n",
            "Epoch: 195 | Loss: 0.048050813376903534      | Test loss: 0.1265692263841629\n",
            "Epoch: 196 | Loss: 0.04718093201518059       | Test loss: 0.12492956966161728\n",
            "Epoch: 197 | Loss: 0.0463189035654068        | Test loss: 0.1233014315366745\n",
            "Epoch: 198 | Loss: 0.04550682753324509       | Test loss: 0.12169003486633301\n",
            "Epoch: 199 | Loss: 0.044706277549266815      | Test loss: 0.12009360641241074\n",
            "Epoch: 200 | Loss: 0.04391415789723396       | Test loss: 0.11851052939891815\n",
            "Epoch: 201 | Loss: 0.04314097762107849       | Test loss: 0.11694624274969101\n",
            "Epoch: 202 | Loss: 0.042410172522068024      | Test loss: 0.11539864540100098\n",
            "Epoch: 203 | Loss: 0.04168803244829178       | Test loss: 0.11386604607105255\n",
            "Epoch: 204 | Loss: 0.04097364470362663       | Test loss: 0.11234678328037262\n",
            "Epoch: 205 | Loss: 0.04028810188174248       | Test loss: 0.11084647476673126\n",
            "Epoch: 206 | Loss: 0.03963200002908707       | Test loss: 0.10936317592859268\n",
            "Epoch: 207 | Loss: 0.03898395225405693       | Test loss: 0.10789506137371063\n",
            "Epoch: 208 | Loss: 0.03834309056401253       | Test loss: 0.10644049942493439\n",
            "Epoch: 209 | Loss: 0.037738122045993805      | Test loss: 0.10500530898571014\n",
            "Epoch: 210 | Loss: 0.037152744829654694      | Test loss: 0.10358761250972748\n",
            "Epoch: 211 | Loss: 0.03657488152384758       | Test loss: 0.1021854430437088\n",
            "Epoch: 212 | Loss: 0.0360037162899971        | Test loss: 0.10079710185527802\n",
            "Epoch: 213 | Loss: 0.03547234460711479       | Test loss: 0.09942878782749176\n",
            "Epoch: 214 | Loss: 0.034953925758600235      | Test loss: 0.0980784147977829\n",
            "Epoch: 215 | Loss: 0.0344424843788147        | Test loss: 0.09674405306577682\n",
            "Epoch: 216 | Loss: 0.03393731266260147       | Test loss: 0.09542402625083923\n",
            "Epoch: 217 | Loss: 0.03347234055399895       | Test loss: 0.09412471204996109\n",
            "Epoch: 218 | Loss: 0.03301713988184929       | Test loss: 0.09284397959709167\n",
            "Epoch: 219 | Loss: 0.032568443566560745      | Test loss: 0.09157978743314743\n",
            "Epoch: 220 | Loss: 0.03212554752826691       | Test loss: 0.09033040702342987\n",
            "Epoch: 221 | Loss: 0.03171933814883232       | Test loss: 0.08910256624221802\n",
            "Epoch: 222 | Loss: 0.03132355958223343       | Test loss: 0.08789394795894623\n",
            "Epoch: 223 | Loss: 0.030933773145079613      | Test loss: 0.08670252561569214\n",
            "Epoch: 224 | Loss: 0.030549341812729836      | Test loss: 0.0855264961719513\n",
            "Epoch: 225 | Loss: 0.03019360639154911       | Test loss: 0.08437280356884003\n",
            "Epoch: 226 | Loss: 0.02985328435897827       | Test loss: 0.08323909342288971\n",
            "Epoch: 227 | Loss: 0.029518425464630127      | Test loss: 0.08212330937385559\n",
            "Epoch: 228 | Loss: 0.029188454151153564      | Test loss: 0.08102347701787949\n",
            "Epoch: 229 | Loss: 0.028874153271317482      | Test loss: 0.07994689047336578\n",
            "Epoch: 230 | Loss: 0.02858511544764042       | Test loss: 0.07889106869697571\n",
            "Epoch: 231 | Loss: 0.028300955891609192      | Test loss: 0.07785386592149734\n",
            "Epoch: 232 | Loss: 0.02802119590342045       | Test loss: 0.07683338224887848\n",
            "Epoch: 233 | Loss: 0.027745366096496582      | Test loss: 0.07582782953977585\n",
            "Epoch: 234 | Loss: 0.027494311332702637      | Test loss: 0.07484482228755951\n",
            "Epoch: 235 | Loss: 0.02725469507277012       | Test loss: 0.07388199120759964\n",
            "Epoch: 236 | Loss: 0.02701910398900509       | Test loss: 0.07293729484081268\n",
            "Epoch: 237 | Loss: 0.026787113398313522      | Test loss: 0.07200887799263\n",
            "Epoch: 238 | Loss: 0.02655833028256893       | Test loss: 0.07109494507312775\n",
            "Epoch: 239 | Loss: 0.026353418827056885      | Test loss: 0.07020356506109238\n",
            "Epoch: 240 | Loss: 0.02615661919116974       | Test loss: 0.06933246552944183\n",
            "Epoch: 241 | Loss: 0.025963108986616135      | Test loss: 0.06847946345806122\n",
            "Epoch: 242 | Loss: 0.025772541761398315      | Test loss: 0.06764276325702667\n",
            "Epoch: 243 | Loss: 0.025584593415260315      | Test loss: 0.06682060658931732\n",
            "Epoch: 244 | Loss: 0.025409316644072533      | Test loss: 0.0660213977098465\n",
            "Epoch: 245 | Loss: 0.025249222293496132      | Test loss: 0.06524275243282318\n",
            "Epoch: 246 | Loss: 0.025091728195548058      | Test loss: 0.06448262929916382\n",
            "Epoch: 247 | Loss: 0.02493656612932682       | Test loss: 0.06373909115791321\n",
            "Epoch: 248 | Loss: 0.024783482775092125      | Test loss: 0.06301042437553406\n",
            "Epoch: 249 | Loss: 0.024632250890135765      | Test loss: 0.06229502707719803\n",
            "Epoch: 250 | Loss: 0.02449045702815056       | Test loss: 0.061601847410202026\n",
            "Epoch: 251 | Loss: 0.02436203323304653       | Test loss: 0.060928624123334885\n",
            "Epoch: 252 | Loss: 0.024235406890511513      | Test loss: 0.06027330085635185\n",
            "Epoch: 253 | Loss: 0.024110395461320877      | Test loss: 0.05963403731584549\n",
            "Epoch: 254 | Loss: 0.023986805230379105      | Test loss: 0.05900920182466507\n",
            "Epoch: 255 | Loss: 0.023864498361945152      | Test loss: 0.05839730426669121\n",
            "Epoch: 256 | Loss: 0.02374332584440708       | Test loss: 0.057796984910964966\n",
            "Epoch: 257 | Loss: 0.023627139627933502      | Test loss: 0.05721776932477951\n",
            "Epoch: 258 | Loss: 0.023523421958088875      | Test loss: 0.05665745213627815\n",
            "Epoch: 259 | Loss: 0.02342074178159237       | Test loss: 0.056114085018634796\n",
            "Epoch: 260 | Loss: 0.02331896685063839       | Test loss: 0.05558601766824722\n",
            "Epoch: 261 | Loss: 0.023218007758259773      | Test loss: 0.05507168918848038\n",
            "Epoch: 262 | Loss: 0.02311776578426361       | Test loss: 0.05456967279314995\n",
            "Epoch: 263 | Loss: 0.023018155246973038      | Test loss: 0.05407872796058655\n",
            "Epoch: 264 | Loss: 0.022919103503227234      | Test loss: 0.05359767749905586\n",
            "Epoch: 265 | Loss: 0.022820521146059036      | Test loss: 0.05312545225024223\n",
            "Epoch: 266 | Loss: 0.022722657769918442      | Test loss: 0.052672334015369415\n",
            "Epoch: 267 | Loss: 0.022635729983448982      | Test loss: 0.05223633721470833\n",
            "Epoch: 268 | Loss: 0.022549128159880638      | Test loss: 0.051815759390592575\n",
            "Epoch: 269 | Loss: 0.02246280573308468       | Test loss: 0.0514090359210968\n",
            "Epoch: 270 | Loss: 0.022376710548996925      | Test loss: 0.051014792174100876\n",
            "Epoch: 271 | Loss: 0.02229081094264984       | Test loss: 0.05063173919916153\n",
            "Epoch: 272 | Loss: 0.022205082699656487      | Test loss: 0.05025876313447952\n",
            "Epoch: 273 | Loss: 0.02211948297917843       | Test loss: 0.049894820898771286\n",
            "Epoch: 274 | Loss: 0.02203398197889328       | Test loss: 0.04953894019126892\n",
            "Epoch: 275 | Loss: 0.021948572248220444      | Test loss: 0.049190323799848557\n",
            "Epoch: 276 | Loss: 0.02186320163309574       | Test loss: 0.04884826019406319\n",
            "Epoch: 277 | Loss: 0.021777886897325516      | Test loss: 0.048512041568756104\n",
            "Epoch: 278 | Loss: 0.021692585200071335      | Test loss: 0.0481809601187706\n",
            "Epoch: 279 | Loss: 0.02160729095339775       | Test loss: 0.047854579985141754\n",
            "Epoch: 280 | Loss: 0.02152198553085327       | Test loss: 0.04753236100077629\n",
            "Epoch: 281 | Loss: 0.021436655893921852      | Test loss: 0.04721388220787048\n",
            "Epoch: 282 | Loss: 0.021356074139475822      | Test loss: 0.04691040515899658\n",
            "Epoch: 283 | Loss: 0.021275753155350685      | Test loss: 0.04662046954035759\n",
            "Epoch: 284 | Loss: 0.021195188164711         | Test loss: 0.04634271189570427\n",
            "Epoch: 285 | Loss: 0.021114403381943703      | Test loss: 0.04607592150568962\n",
            "Epoch: 286 | Loss: 0.0210333913564682        | Test loss: 0.04581896588206291\n",
            "Epoch: 287 | Loss: 0.020952148362994194      | Test loss: 0.045570842921733856\n",
            "Epoch: 288 | Loss: 0.02087070234119892       | Test loss: 0.045330703258514404\n",
            "Epoch: 289 | Loss: 0.02078905701637268       | Test loss: 0.04509773850440979\n",
            "Epoch: 290 | Loss: 0.020707203075289726      | Test loss: 0.04487118124961853\n",
            "Epoch: 291 | Loss: 0.020625168457627296      | Test loss: 0.04465040564537048\n",
            "Epoch: 292 | Loss: 0.020542945712804794      | Test loss: 0.044434793293476105\n",
            "Epoch: 293 | Loss: 0.02046053484082222       | Test loss: 0.04422391206026077\n",
            "Epoch: 294 | Loss: 0.020377950742840767      | Test loss: 0.04401721432805061\n",
            "Epoch: 295 | Loss: 0.020295193418860435      | Test loss: 0.04381421208381653\n",
            "Epoch: 296 | Loss: 0.020212268456816673      | Test loss: 0.043614573776721954\n",
            "Epoch: 297 | Loss: 0.02012915536761284       | Test loss: 0.043417949229478836\n",
            "Epoch: 298 | Loss: 0.020045891404151917      | Test loss: 0.04322398453950882\n",
            "Epoch: 299 | Loss: 0.019962456077337265      | Test loss: 0.04303243011236191\n",
            "Epoch: 300 | Loss: 0.01987886242568493       | Test loss: 0.042843032628297806\n",
            "Epoch: 301 | Loss: 0.019795101135969162      | Test loss: 0.04265553504228592\n",
            "Epoch: 302 | Loss: 0.019711187109351158      | Test loss: 0.042469728738069534\n",
            "Epoch: 303 | Loss: 0.019627109169960022      | Test loss: 0.04228544980287552\n",
            "Epoch: 304 | Loss: 0.01954289712011814       | Test loss: 0.04210250824689865\n",
            "Epoch: 305 | Loss: 0.01945851929485798       | Test loss: 0.041920799762010574\n",
            "Epoch: 306 | Loss: 0.019374007359147072      | Test loss: 0.04174010828137398\n",
            "Epoch: 307 | Loss: 0.019289324060082436      | Test loss: 0.041560303419828415\n",
            "Epoch: 308 | Loss: 0.01920449547469616       | Test loss: 0.04138138145208359\n",
            "Epoch: 309 | Loss: 0.019119521602988243      | Test loss: 0.04120311141014099\n",
            "Epoch: 310 | Loss: 0.019034400582313538      | Test loss: 0.04102548956871033\n",
            "Epoch: 311 | Loss: 0.018949130550026894      | Test loss: 0.040848392993211746\n",
            "Epoch: 312 | Loss: 0.018863724544644356      | Test loss: 0.040671758353710175\n",
            "Epoch: 313 | Loss: 0.018778162077069283      | Test loss: 0.040495533496141434\n",
            "Epoch: 314 | Loss: 0.018692459911108017      | Test loss: 0.040319718420505524\n",
            "Epoch: 315 | Loss: 0.01860661990940571       | Test loss: 0.04014410823583603\n",
            "Epoch: 316 | Loss: 0.018520627170801163      | Test loss: 0.039968788623809814\n",
            "Epoch: 317 | Loss: 0.01843450218439102       | Test loss: 0.03979365900158882\n",
            "Epoch: 318 | Loss: 0.01834823191165924       | Test loss: 0.03961864113807678\n",
            "Epoch: 319 | Loss: 0.01826181635260582       | Test loss: 0.03944377228617668\n",
            "Epoch: 320 | Loss: 0.018175268545746803      | Test loss: 0.03926900774240494\n",
            "Epoch: 321 | Loss: 0.0180885698646307        | Test loss: 0.03909428045153618\n",
            "Epoch: 322 | Loss: 0.018001738935709         | Test loss: 0.03891956806182861\n",
            "Epoch: 323 | Loss: 0.01791476458311081       | Test loss: 0.03874487429857254\n",
            "Epoch: 324 | Loss: 0.017827654257416725      | Test loss: 0.03857015445828438\n",
            "Epoch: 325 | Loss: 0.017740407958626747      | Test loss: 0.038395415991544724\n",
            "Epoch: 326 | Loss: 0.017653007060289383      | Test loss: 0.03822063282132149\n",
            "Epoch: 327 | Loss: 0.017565473914146423      | Test loss: 0.03804578259587288\n",
            "Epoch: 328 | Loss: 0.017477814108133316      | Test loss: 0.037870876491069794\n",
            "Epoch: 329 | Loss: 0.017390014603734016      | Test loss: 0.03769587725400925\n",
            "Epoch: 330 | Loss: 0.017302077263593674      | Test loss: 0.03752077370882034\n",
            "Epoch: 331 | Loss: 0.01721399836242199       | Test loss: 0.03734556958079338\n",
            "Epoch: 332 | Loss: 0.017125790938735008      | Test loss: 0.03717021271586418\n",
            "Epoch: 333 | Loss: 0.017037440091371536      | Test loss: 0.03699479624629021\n",
            "Epoch: 334 | Loss: 0.016948958858847618      | Test loss: 0.0368192121386528\n",
            "Epoch: 335 | Loss: 0.01686033420264721       | Test loss: 0.03664345666766167\n",
            "Epoch: 336 | Loss: 0.016771584749221802      | Test loss: 0.036467619240283966\n",
            "Epoch: 337 | Loss: 0.016682695597410202      | Test loss: 0.036291610449552536\n",
            "Epoch: 338 | Loss: 0.016593674197793007      | Test loss: 0.03611543029546738\n",
            "Epoch: 339 | Loss: 0.016504516825079918      | Test loss: 0.0359390564262867\n",
            "Epoch: 340 | Loss: 0.016415225341916084      | Test loss: 0.035762567073106766\n",
            "Epoch: 341 | Loss: 0.01632579043507576       | Test loss: 0.03558588773012161\n",
            "Epoch: 342 | Loss: 0.01623621955513954       | Test loss: 0.03540906310081482\n",
            "Epoch: 343 | Loss: 0.016146527603268623      | Test loss: 0.0352320671081543\n",
            "Epoch: 344 | Loss: 0.01605670154094696       | Test loss: 0.03505489230155945\n",
            "Epoch: 345 | Loss: 0.015966743230819702      | Test loss: 0.03487754613161087\n",
            "Epoch: 346 | Loss: 0.01587664522230625       | Test loss: 0.03470000624656677\n",
            "Epoch: 347 | Loss: 0.015786413103342056      | Test loss: 0.03452233597636223\n",
            "Epoch: 348 | Loss: 0.015696048736572266      | Test loss: 0.03434441611170769\n",
            "Epoch: 349 | Loss: 0.015605544671416283      | Test loss: 0.034166328608989716\n",
            "Epoch: 350 | Loss: 0.015514915809035301      | Test loss: 0.03398812934756279\n",
            "Epoch: 351 | Loss: 0.015424156561493874      | Test loss: 0.03380970284342766\n",
            "Epoch: 352 | Loss: 0.015333278104662895      | Test loss: 0.03363111615180969\n",
            "Epoch: 353 | Loss: 0.015242251567542553      | Test loss: 0.03345232456922531\n",
            "Epoch: 354 | Loss: 0.015151100233197212      | Test loss: 0.0332733578979969\n",
            "Epoch: 355 | Loss: 0.015059806406497955      | Test loss: 0.033094197511672974\n",
            "Epoch: 356 | Loss: 0.0149683877825737        | Test loss: 0.03291483595967293\n",
            "Epoch: 357 | Loss: 0.0148768350481987        | Test loss: 0.03273535519838333\n",
            "Epoch: 358 | Loss: 0.014785155653953552      | Test loss: 0.03255566954612732\n",
            "Epoch: 359 | Loss: 0.014693346805870533      | Test loss: 0.03237580135464668\n",
            "Epoch: 360 | Loss: 0.014601712115108967      | Test loss: 0.03218240663409233\n",
            "Epoch: 361 | Loss: 0.01451012771576643       | Test loss: 0.0319768488407135\n",
            "Epoch: 362 | Loss: 0.014418045058846474      | Test loss: 0.03176029399037361\n",
            "Epoch: 363 | Loss: 0.014325511641800404      | Test loss: 0.03153378516435623\n",
            "Epoch: 364 | Loss: 0.014233211055397987      | Test loss: 0.031311653554439545\n",
            "Epoch: 365 | Loss: 0.014140933752059937      | Test loss: 0.031093424186110497\n",
            "Epoch: 366 | Loss: 0.01404848974198103       | Test loss: 0.0308787040412426\n",
            "Epoch: 367 | Loss: 0.013955888338387012      | Test loss: 0.030667120590806007\n",
            "Epoch: 368 | Loss: 0.013863136060535908      | Test loss: 0.03045837953686714\n",
            "Epoch: 369 | Loss: 0.013770240359008312      | Test loss: 0.030252188444137573\n",
            "Epoch: 370 | Loss: 0.013677187263965607      | Test loss: 0.03004823997616768\n",
            "Epoch: 371 | Loss: 0.013584012165665627      | Test loss: 0.029846329241991043\n",
            "Epoch: 372 | Loss: 0.01349068246781826       | Test loss: 0.029646247625350952\n",
            "Epoch: 373 | Loss: 0.013397211208939552      | Test loss: 0.029447728767991066\n",
            "Epoch: 374 | Loss: 0.013303592801094055      | Test loss: 0.029250621795654297\n",
            "Epoch: 375 | Loss: 0.01320983748883009       | Test loss: 0.02905482053756714\n",
            "Epoch: 376 | Loss: 0.013115944340825081      | Test loss: 0.028860140591859818\n",
            "Epoch: 377 | Loss: 0.013021925464272499      | Test loss: 0.028666460886597633\n",
            "Epoch: 378 | Loss: 0.012928095646202564      | Test loss: 0.028459995985031128\n",
            "Epoch: 379 | Loss: 0.012833978049457073      | Test loss: 0.02824201062321663\n",
            "Epoch: 380 | Loss: 0.012739407829940319      | Test loss: 0.028013665229082108\n",
            "Epoch: 381 | Loss: 0.012645048089325428      | Test loss: 0.027789587154984474\n",
            "Epoch: 382 | Loss: 0.012550607323646545      | Test loss: 0.027569323778152466\n",
            "Epoch: 383 | Loss: 0.012456009164452553      | Test loss: 0.027352482080459595\n",
            "Epoch: 384 | Loss: 0.012361266650259495      | Test loss: 0.027138739824295044\n",
            "Epoch: 385 | Loss: 0.012266376987099648      | Test loss: 0.0269277635961771\n",
            "Epoch: 386 | Loss: 0.01217133179306984       | Test loss: 0.026719266548752785\n",
            "Epoch: 387 | Loss: 0.012076136656105518      | Test loss: 0.02651296928524971\n",
            "Epoch: 388 | Loss: 0.011980807408690453      | Test loss: 0.026308666914701462\n",
            "Epoch: 389 | Loss: 0.011885331943631172      | Test loss: 0.02610616758465767\n",
            "Epoch: 390 | Loss: 0.011789723299443722      | Test loss: 0.02590523287653923\n",
            "Epoch: 391 | Loss: 0.011693974956870079      | Test loss: 0.025705665349960327\n",
            "Epoch: 392 | Loss: 0.01159808598458767       | Test loss: 0.025507336482405663\n",
            "Epoch: 393 | Loss: 0.011502442881464958      | Test loss: 0.02529623545706272\n",
            "Epoch: 394 | Loss: 0.011406466364860535      | Test loss: 0.025073522701859474\n",
            "Epoch: 395 | Loss: 0.011310018599033356      | Test loss: 0.0248403362929821\n",
            "Epoch: 396 | Loss: 0.011213814839720726      | Test loss: 0.024611663073301315\n",
            "Epoch: 397 | Loss: 0.011117534711956978      | Test loss: 0.024387061595916748\n",
            "Epoch: 398 | Loss: 0.011021096259355545      | Test loss: 0.024166107177734375\n",
            "Epoch: 399 | Loss: 0.010924512520432472      | Test loss: 0.023948419839143753\n",
            "Epoch: 400 | Loss: 0.010827784426510334      | Test loss: 0.023733656853437424\n",
            "Epoch: 401 | Loss: 0.01073089987039566       | Test loss: 0.023521501570940018\n",
            "Epoch: 402 | Loss: 0.010633869096636772      | Test loss: 0.023311669006943703\n",
            "Epoch: 403 | Loss: 0.01053669024258852       | Test loss: 0.023103971034288406\n",
            "Epoch: 404 | Loss: 0.010439381003379822      | Test loss: 0.02289816178381443\n",
            "Epoch: 405 | Loss: 0.010341928340494633      | Test loss: 0.022694002836942673\n",
            "Epoch: 406 | Loss: 0.010244349017739296      | Test loss: 0.022491376847028732\n",
            "Epoch: 407 | Loss: 0.010146623477339745      | Test loss: 0.022290069609880447\n",
            "Epoch: 408 | Loss: 0.010049315169453621      | Test loss: 0.022075766697525978\n",
            "Epoch: 409 | Loss: 0.00995154120028019       | Test loss: 0.021849697455763817\n",
            "Epoch: 410 | Loss: 0.009853307157754898      | Test loss: 0.02161300741136074\n",
            "Epoch: 411 | Loss: 0.009755073115229607      | Test loss: 0.02138090692460537\n",
            "Epoch: 412 | Loss: 0.009656984359025955      | Test loss: 0.021153021603822708\n",
            "Epoch: 413 | Loss: 0.009558732621371746      | Test loss: 0.0209288839250803\n",
            "Epoch: 414 | Loss: 0.009460333734750748      | Test loss: 0.020708125084638596\n",
            "Epoch: 415 | Loss: 0.009361792355775833      | Test loss: 0.020490378141403198\n",
            "Epoch: 416 | Loss: 0.009263096377253532      | Test loss: 0.020275305956602097\n",
            "Epoch: 417 | Loss: 0.00916426070034504       | Test loss: 0.020062696188688278\n",
            "Epoch: 418 | Loss: 0.009065285325050354      | Test loss: 0.01985221542418003\n",
            "Epoch: 419 | Loss: 0.008966167457401752      | Test loss: 0.01964368298649788\n",
            "Epoch: 420 | Loss: 0.00886690802872181       | Test loss: 0.019436847418546677\n",
            "Epoch: 421 | Loss: 0.008767572231590748      | Test loss: 0.019217144697904587\n",
            "Epoch: 422 | Loss: 0.008668199181556702      | Test loss: 0.018985802307724953\n",
            "Epoch: 423 | Loss: 0.008568691089749336      | Test loss: 0.018758375197649002\n",
            "Epoch: 424 | Loss: 0.008469109423458576      | Test loss: 0.018534529954195023\n",
            "Epoch: 425 | Loss: 0.00836938340216875       | Test loss: 0.018313884735107422\n",
            "Epoch: 426 | Loss: 0.00826951302587986       | Test loss: 0.01809605397284031\n",
            "Epoch: 427 | Loss: 0.008169499225914478      | Test loss: 0.017880767583847046\n",
            "Epoch: 428 | Loss: 0.00806933268904686       | Test loss: 0.01766781136393547\n",
            "Epoch: 429 | Loss: 0.007969135418534279      | Test loss: 0.017442267388105392\n",
            "Epoch: 430 | Loss: 0.007868723943829536      | Test loss: 0.017220061272382736\n",
            "Epoch: 431 | Loss: 0.007768266834318638      | Test loss: 0.017000775784254074\n",
            "Epoch: 432 | Loss: 0.007667799945920706      | Test loss: 0.016769468784332275\n",
            "Epoch: 433 | Loss: 0.007567057851701975      | Test loss: 0.016542011871933937\n",
            "Epoch: 434 | Loss: 0.007466289214789867      | Test loss: 0.016317909583449364\n",
            "Epoch: 435 | Loss: 0.007365374825894833      | Test loss: 0.016096938401460648\n",
            "Epoch: 436 | Loss: 0.00726430956274271       | Test loss: 0.015878712758421898\n",
            "Epoch: 437 | Loss: 0.007163109723478556      | Test loss: 0.015655547380447388\n",
            "Epoch: 438 | Loss: 0.007061876356601715      | Test loss: 0.015420610085129738\n",
            "Epoch: 439 | Loss: 0.006960534490644932      | Test loss: 0.01518976129591465\n",
            "Epoch: 440 | Loss: 0.006859088782221079      | Test loss: 0.014962571673095226\n",
            "Epoch: 441 | Loss: 0.006757488939911127      | Test loss: 0.014738720841705799\n",
            "Epoch: 442 | Loss: 0.006655744276940823      | Test loss: 0.014517778530716896\n",
            "Epoch: 443 | Loss: 0.006553863640874624      | Test loss: 0.014299506321549416\n",
            "Epoch: 444 | Loss: 0.006451824214309454      | Test loss: 0.014083653688430786\n",
            "Epoch: 445 | Loss: 0.00634996872395277       | Test loss: 0.013854986056685448\n",
            "Epoch: 446 | Loss: 0.006247711833566427      | Test loss: 0.013614838942885399\n",
            "Epoch: 447 | Loss: 0.006145312450826168      | Test loss: 0.01337923388928175\n",
            "Epoch: 448 | Loss: 0.0060429745353758335     | Test loss: 0.013147681951522827\n",
            "Epoch: 449 | Loss: 0.005940479692071676      | Test loss: 0.012919706292450428\n",
            "Epoch: 450 | Loss: 0.0058378418907523155     | Test loss: 0.01269506849348545\n",
            "Epoch: 451 | Loss: 0.005735055543482304      | Test loss: 0.012473327107727528\n",
            "Epoch: 452 | Loss: 0.005632124841213226      | Test loss: 0.012254220433533192\n",
            "Epoch: 453 | Loss: 0.005529058165848255      | Test loss: 0.012037438340485096\n",
            "Epoch: 454 | Loss: 0.005426215473562479      | Test loss: 0.01180773414671421\n",
            "Epoch: 455 | Loss: 0.00532298069447279       | Test loss: 0.011566365137696266\n",
            "Epoch: 456 | Loss: 0.005219439510256052      | Test loss: 0.011329513974487782\n",
            "Epoch: 457 | Loss: 0.005116062704473734      | Test loss: 0.011096751317381859\n",
            "Epoch: 458 | Loss: 0.005012549459934235      | Test loss: 0.010867590084671974\n",
            "Epoch: 459 | Loss: 0.004908875562250614      | Test loss: 0.010641741566359997\n",
            "Epoch: 460 | Loss: 0.004805061034858227      | Test loss: 0.010418826714158058\n",
            "Epoch: 461 | Loss: 0.004701096564531326      | Test loss: 0.010198533535003662\n",
            "Epoch: 462 | Loss: 0.004597307648509741      | Test loss: 0.009965425357222557\n",
            "Epoch: 463 | Loss: 0.004493123386055231      | Test loss: 0.009720760397613049\n",
            "Epoch: 464 | Loss: 0.004388801753520966      | Test loss: 0.009480863809585571\n",
            "Epoch: 465 | Loss: 0.004284548573195934      | Test loss: 0.009245241060853004\n",
            "Epoch: 466 | Loss: 0.004180136136710644      | Test loss: 0.009013419970870018\n",
            "Epoch: 467 | Loss: 0.004075573291629553      | Test loss: 0.008785051293671131\n",
            "Epoch: 468 | Loss: 0.00397086888551712       | Test loss: 0.008559775538742542\n",
            "Epoch: 469 | Loss: 0.003866019193083048      | Test loss: 0.008337264880537987\n",
            "Epoch: 470 | Loss: 0.0037610246799886227     | Test loss: 0.00810192245990038\n",
            "Epoch: 471 | Loss: 0.003656037850305438      | Test loss: 0.007870351895689964\n",
            "Epoch: 472 | Loss: 0.0035509064327925444     | Test loss: 0.007642143871635199\n",
            "Epoch: 473 | Loss: 0.00344577943906188       | Test loss: 0.007401555776596069\n",
            "Epoch: 474 | Loss: 0.003340343479067087      | Test loss: 0.007165193557739258\n",
            "Epoch: 475 | Loss: 0.0032349140383303165     | Test loss: 0.006932639982551336\n",
            "Epoch: 476 | Loss: 0.0031293369829654694     | Test loss: 0.006703495979309082\n",
            "Epoch: 477 | Loss: 0.0030236460734158754     | Test loss: 0.00646203150972724\n",
            "Epoch: 478 | Loss: 0.0029179013799875975     | Test loss: 0.006224828772246838\n",
            "Epoch: 479 | Loss: 0.0028120323549956083     | Test loss: 0.005991464946419001\n",
            "Epoch: 480 | Loss: 0.0027060091961175203     | Test loss: 0.005761533975601196\n",
            "Epoch: 481 | Loss: 0.002599830972030759      | Test loss: 0.005534738302230835\n",
            "Epoch: 482 | Loss: 0.0024937926791608334     | Test loss: 0.005295139737427235\n",
            "Epoch: 483 | Loss: 0.002387356711551547      | Test loss: 0.005044108722358942\n",
            "Epoch: 484 | Loss: 0.002280939370393753      | Test loss: 0.004798239562660456\n",
            "Epoch: 485 | Loss: 0.0021744847763329744     | Test loss: 0.0045570433139801025\n",
            "Epoch: 486 | Loss: 0.002067870693281293      | Test loss: 0.004319888539612293\n",
            "Epoch: 487 | Loss: 0.0019610964227467775     | Test loss: 0.004086571745574474\n",
            "Epoch: 488 | Loss: 0.0018541790777817369     | Test loss: 0.003856575582176447\n",
            "Epoch: 489 | Loss: 0.0017471216851845384     | Test loss: 0.003629577113315463\n",
            "Epoch: 490 | Loss: 0.0016399167943745852     | Test loss: 0.003405338618904352\n",
            "Epoch: 491 | Loss: 0.001532878028228879      | Test loss: 0.003167825983837247\n",
            "Epoch: 492 | Loss: 0.0014255695277824998     | Test loss: 0.0029183984734117985\n",
            "Epoch: 493 | Loss: 0.0013179496163502336     | Test loss: 0.0026738345623016357\n",
            "Epoch: 494 | Loss: 0.0012104734778404236     | Test loss: 0.0024337233044207096\n",
            "Epoch: 495 | Loss: 0.0011028542649000883     | Test loss: 0.0021975338459014893\n",
            "Epoch: 496 | Loss: 0.000995074980892241      | Test loss: 0.0019649327732622623\n",
            "Epoch: 497 | Loss: 0.000887148838955909      | Test loss: 0.0017355263698846102\n",
            "Epoch: 498 | Loss: 0.0007793985423631966     | Test loss: 0.001493173884227872\n",
            "Epoch: 499 | Loss: 0.0006712466711178422     | Test loss: 0.0012391686905175447\n",
            "Epoch: 500 | Loss: 0.0005629874649457633     | Test loss: 0.0009904683101922274\n",
            "Epoch: 501 | Loss: 0.0004547901335172355     | Test loss: 0.0007465124363079667\n",
            "Epoch: 502 | Loss: 0.00034643709659576416    | Test loss: 0.0005068480968475342\n",
            "Epoch: 503 | Loss: 0.00023792311549186707    | Test loss: 0.0002709925174713135\n",
            "Epoch: 504 | Loss: 0.00012925639748573303    | Test loss: 3.855824616039172e-05\n",
            "Epoch: 505 | Loss: 2.06679105758667e-05      | Test loss: 0.00015794634236954153\n",
            "Epoch: 506 | Loss: 9.052381210494787e-05     | Test loss: 0.00036496520624496043\n",
            "Epoch: 507 | Loss: 0.0001759611041052267     | Test loss: 0.0005477011436596513\n",
            "Epoch: 508 | Loss: 0.00024370029859710485    | Test loss: 0.0006761848926544189\n",
            "Epoch: 509 | Loss: 0.0002943865838460624     | Test loss: 0.0007557868957519531\n",
            "Epoch: 510 | Loss: 0.0003287754952907562     | Test loss: 0.0007912814617156982\n",
            "Epoch: 511 | Loss: 0.00034848079667426646    | Test loss: 0.0007870972040109336\n",
            "Epoch: 512 | Loss: 0.0003549419343471527     | Test loss: 0.0007471620920114219\n",
            "Epoch: 513 | Loss: 0.00035060569643974304    | Test loss: 0.0006910860538482666\n",
            "Epoch: 514 | Loss: 0.00033605992211960256    | Test loss: 0.0006368577596731484\n",
            "Epoch: 515 | Loss: 0.0003116026637144387     | Test loss: 0.0005842983955517411\n",
            "Epoch: 516 | Loss: 0.00027784929261542857    | Test loss: 0.0005168497445993125\n",
            "Epoch: 517 | Loss: 0.00023682042956352234    | Test loss: 0.0004359424056019634\n",
            "Epoch: 518 | Loss: 0.0001894533634185791     | Test loss: 0.00032671092776581645\n",
            "Epoch: 519 | Loss: 0.00013608112931251526    | Test loss: 0.00017624496831558645\n",
            "Epoch: 520 | Loss: 7.552430179202929e-05     | Test loss: 4.345178695075447e-06\n",
            "Epoch: 521 | Loss: 1.657828761381097e-05     | Test loss: 0.00015631318092346191\n",
            "Epoch: 522 | Loss: 0.0002112179936375469     | Test loss: 2.18868262891192e-05\n",
            "Epoch: 523 | Loss: 0.00018376186199020594    | Test loss: 0.00047115684719756246\n",
            "Epoch: 524 | Loss: 0.00016763732128310949    | Test loss: 0.0007436334853991866\n",
            "Epoch: 525 | Loss: 0.0002665586653165519     | Test loss: 0.0007853865390643477\n",
            "Epoch: 526 | Loss: 0.0002780236245598644     | Test loss: 0.0006317555671557784\n",
            "Epoch: 527 | Loss: 0.00022889002866577357    | Test loss: 0.0003817141114268452\n",
            "Epoch: 528 | Loss: 0.00021172389097046107    | Test loss: 0.00018680095672607422\n",
            "Epoch: 529 | Loss: 0.0002491474151611328     | Test loss: 0.00019102692021988332\n",
            "Epoch: 530 | Loss: 0.0002182297466788441     | Test loss: 0.00035463570384308696\n",
            "Epoch: 531 | Loss: 0.00015618577890563756    | Test loss: 0.0004657268582377583\n",
            "Epoch: 532 | Loss: 0.00016362741007469594    | Test loss: 0.0003866255283355713\n",
            "Epoch: 533 | Loss: 0.00014126673340797424    | Test loss: 8.866190910339355e-05\n",
            "Epoch: 534 | Loss: 4.887730028713122e-05     | Test loss: 0.00015791654004715383\n",
            "Epoch: 535 | Loss: 0.00012173801951576024    | Test loss: 7.378459122264758e-05\n",
            "Epoch: 536 | Loss: 4.2238087189616635e-05    | Test loss: 2.8353930247249082e-05\n",
            "Epoch: 537 | Loss: 0.00011100247502326965    | Test loss: 0.00029311777325347066\n",
            "Epoch: 538 | Loss: 0.00010487138933967799    | Test loss: 0.0004056096076965332\n",
            "Epoch: 539 | Loss: 0.00014309286780189723    | Test loss: 0.00031548141851089895\n",
            "Epoch: 540 | Loss: 0.00011533945507835597    | Test loss: 0.00012252926535438746\n",
            "Epoch: 541 | Loss: 0.00011269599053775892    | Test loss: 7.000565528869629e-05\n",
            "Epoch: 542 | Loss: 0.00010741278674686328    | Test loss: 0.00022261738195084035\n",
            "Epoch: 543 | Loss: 7.808357622707263e-05     | Test loss: 0.00019374489784240723\n",
            "Epoch: 544 | Loss: 8.227229409385473e-05     | Test loss: 0.00011997819092357531\n",
            "Epoch: 545 | Loss: 0.00010412484698463231    | Test loss: 9.693503670860082e-05\n",
            "Epoch: 546 | Loss: 4.040449857711792e-05     | Test loss: 0.00021050572104286402\n",
            "Epoch: 547 | Loss: 0.00023074596538208425    | Test loss: 0.00018274187459610403\n",
            "Epoch: 548 | Loss: 0.00024361610121559352    | Test loss: 0.00014553070650435984\n",
            "Epoch: 549 | Loss: 7.064342207740992e-05     | Test loss: 0.0004375278949737549\n",
            "Epoch: 550 | Loss: 0.00018942430324386805    | Test loss: 0.00040549636469222605\n",
            "Epoch: 551 | Loss: 0.00016390085511375219    | Test loss: 0.00011047125008190051\n",
            "Epoch: 552 | Loss: 8.039474778342992e-05     | Test loss: 7.236599776661023e-05\n",
            "Epoch: 553 | Loss: 0.00017656013369560242    | Test loss: 6.574988219654188e-05\n",
            "Epoch: 554 | Loss: 6.254017353057861e-05     | Test loss: 0.00030975937261246145\n",
            "Epoch: 555 | Loss: 0.00016319453425239772    | Test loss: 0.00022693276696372777\n",
            "Epoch: 556 | Loss: 0.0001289032370550558     | Test loss: 0.0001488983689341694\n",
            "Epoch: 557 | Loss: 0.00013013332500122488    | Test loss: 0.00018575787544250488\n",
            "Epoch: 558 | Loss: 0.0001350589154753834     | Test loss: 8.139610145008191e-05\n",
            "Epoch: 559 | Loss: 8.780360076343641e-05     | Test loss: 2.1678210032405332e-05\n",
            "Epoch: 560 | Loss: 6.112679693615064e-05     | Test loss: 0.00033121704473160207\n",
            "Epoch: 561 | Loss: 0.00018928200006484985    | Test loss: 0.0003496050776448101\n",
            "Epoch: 562 | Loss: 0.00018822253332473338    | Test loss: 6.806254532421008e-05\n",
            "Epoch: 563 | Loss: 5.110204074298963e-05     | Test loss: 0.00010415911674499512\n",
            "Epoch: 564 | Loss: 0.00016375929408241063    | Test loss: 3.8951635360717773e-05\n",
            "Epoch: 565 | Loss: 5.431249883258715e-05     | Test loss: 0.0003424108144827187\n",
            "Epoch: 566 | Loss: 0.00019274503574706614    | Test loss: 0.0003176867903675884\n",
            "Epoch: 567 | Loss: 0.00018714368343353271    | Test loss: 4.190206709608901e-06\n",
            "Epoch: 568 | Loss: 4.222691131872125e-05     | Test loss: 8.302927199110854e-06\n",
            "Epoch: 569 | Loss: 2.4440139895887114e-05    | Test loss: 0.00029162169084884226\n",
            "Epoch: 570 | Loss: 0.00020125806622672826    | Test loss: 0.00025078654289245605\n",
            "Epoch: 571 | Loss: 0.0001807786466088146     | Test loss: 8.08298573247157e-05\n",
            "Epoch: 572 | Loss: 6.0410799051169306e-05    | Test loss: 8.437037467956543e-05\n",
            "Epoch: 573 | Loss: 5.471483018482104e-05     | Test loss: 0.00020632744417525828\n",
            "Epoch: 574 | Loss: 0.00017235353880096227    | Test loss: 0.00017408728308510035\n",
            "Epoch: 575 | Loss: 0.0001547738938825205     | Test loss: 0.00014780760102439672\n",
            "Epoch: 576 | Loss: 8.217543654609472e-05     | Test loss: 0.0001446068345103413\n",
            "Epoch: 577 | Loss: 7.429197285091504e-05     | Test loss: 0.00015025734319351614\n",
            "Epoch: 578 | Loss: 0.00015317574434448034    | Test loss: 0.00012366175360511988\n",
            "Epoch: 579 | Loss: 0.00013752281665802002    | Test loss: 0.00019127727136947215\n",
            "Epoch: 580 | Loss: 9.616836905479431e-05     | Test loss: 0.00018367767916060984\n",
            "Epoch: 581 | Loss: 8.687302761245519e-05     | Test loss: 0.00011321902275085449\n",
            "Epoch: 582 | Loss: 0.0001403145433869213     | Test loss: 9.038448479259387e-05\n",
            "Epoch: 583 | Loss: 0.00012597143359016627    | Test loss: 0.00021935105905868113\n",
            "Epoch: 584 | Loss: 0.0001050606369972229     | Test loss: 0.00020894408226013184\n",
            "Epoch: 585 | Loss: 9.488761133980006e-05     | Test loss: 8.875727507984266e-05\n",
            "Epoch: 586 | Loss: 0.0001316651760134846     | Test loss: 6.839632987976074e-05\n",
            "Epoch: 587 | Loss: 0.00011818781786132604    | Test loss: 0.0002373576135141775\n",
            "Epoch: 588 | Loss: 0.00011060759425163269    | Test loss: 0.00022513270960189402\n",
            "Epoch: 589 | Loss: 9.987503290176392e-05     | Test loss: 7.243752770591527e-05\n",
            "Epoch: 590 | Loss: 0.00012575536675285548    | Test loss: 5.369782593334094e-05\n",
            "Epoch: 591 | Loss: 0.00011285990331089124    | Test loss: 0.0002488613245077431\n",
            "Epoch: 592 | Loss: 0.00011400655057514086    | Test loss: 0.00023546814918518066\n",
            "Epoch: 593 | Loss: 0.00010294392996001989    | Test loss: 5.3519011999014765e-05\n",
            "Epoch: 594 | Loss: 0.00011368021660018712    | Test loss: 2.866983413696289e-05\n",
            "Epoch: 595 | Loss: 9.400695853400975e-05     | Test loss: 0.0002777576446533203\n",
            "Epoch: 596 | Loss: 0.0001376420259475708     | Test loss: 0.00026943086413666606\n",
            "Epoch: 597 | Loss: 0.0001321643649134785     | Test loss: 2.136826515197754e-05\n",
            "Epoch: 598 | Loss: 8.610263466835022e-05     | Test loss: 6.937980742804939e-06\n",
            "Epoch: 599 | Loss: 6.924569606781006e-05     | Test loss: 0.000302046537399292\n",
            "Epoch: 600 | Loss: 0.00015857070684432983    | Test loss: 0.00029124616412445903\n",
            "Epoch: 601 | Loss: 0.0001509591966168955     | Test loss: 6.759166808478767e-06\n",
            "Epoch: 602 | Loss: 6.794855289626867e-05     | Test loss: 1.9174814951838925e-05\n",
            "Epoch: 603 | Loss: 5.3126364946365356e-05    | Test loss: 0.0002775907632894814\n",
            "Epoch: 604 | Loss: 0.00014567449397873133    | Test loss: 0.0002290427655680105\n",
            "Epoch: 605 | Loss: 0.00011290535621810704    | Test loss: 9.492039680480957e-05\n",
            "Epoch: 606 | Loss: 0.00012743845582008362    | Test loss: 0.00010613202903186902\n",
            "Epoch: 607 | Loss: 0.00013283714361023158    | Test loss: 0.00016342401795554906\n",
            "Epoch: 608 | Loss: 7.261559221660718e-05     | Test loss: 0.00012642741785384715\n",
            "Epoch: 609 | Loss: 4.929304122924805e-05     | Test loss: 0.0001437366008758545\n",
            "Epoch: 610 | Loss: 0.0001450762210879475     | Test loss: 0.0001079022913472727\n",
            "Epoch: 611 | Loss: 0.00010834932618308812    | Test loss: 0.0002026498259510845\n",
            "Epoch: 612 | Loss: 0.0001339539885520935     | Test loss: 0.00020381808280944824\n",
            "Epoch: 613 | Loss: 0.00014273225679062307    | Test loss: 7.280707359313965e-05\n",
            "Epoch: 614 | Loss: 5.8142097259406e-05       | Test loss: 4.416108276927844e-05\n",
            "Epoch: 615 | Loss: 3.023371027666144e-05     | Test loss: 0.00025853514671325684\n",
            "Epoch: 616 | Loss: 0.00020301267795730382    | Test loss: 0.0002539932611398399\n",
            "Epoch: 617 | Loss: 0.0002047404705081135     | Test loss: 2.630948984005954e-05\n",
            "Epoch: 618 | Loss: 1.0055303391709458e-05    | Test loss: 0.00020244717597961426\n",
            "Epoch: 619 | Loss: 0.0001490391732659191     | Test loss: 8.46445545903407e-05\n",
            "Epoch: 620 | Loss: 7.436797022819519e-05     | Test loss: 0.0002969980123452842\n",
            "Epoch: 621 | Loss: 0.0001999631494982168     | Test loss: 0.0003646552504505962\n",
            "Epoch: 622 | Loss: 0.00023960396356415004    | Test loss: 0.00015041232109069824\n",
            "Epoch: 623 | Loss: 6.856396794319153e-05     | Test loss: 0.00031688809394836426\n",
            "Epoch: 624 | Loss: 0.0002915196237154305     | Test loss: 0.0004625380097422749\n",
            "Epoch: 625 | Loss: 0.00040907785296440125    | Test loss: 0.00031941532506607473\n",
            "Epoch: 626 | Loss: 0.00030888692708685994    | Test loss: 8.30471544759348e-05\n",
            "Epoch: 627 | Loss: 3.9625167119083926e-05    | Test loss: 0.000425809615990147\n",
            "Epoch: 628 | Loss: 0.00025731325149536133    | Test loss: 0.000460207462310791\n",
            "Epoch: 629 | Loss: 0.00029495282797142863    | Test loss: 0.00021780133829452097\n",
            "Epoch: 630 | Loss: 0.0001235269010066986     | Test loss: 0.0002731204149313271\n",
            "Epoch: 631 | Loss: 0.00023554638028144836    | Test loss: 0.000441753858467564\n",
            "Epoch: 632 | Loss: 0.00035357699380256236    | Test loss: 0.0003209829446859658\n",
            "Epoch: 633 | Loss: 0.000255214428761974      | Test loss: 5.965829041088e-05\n",
            "Epoch: 634 | Loss: 3.7423520552692935e-05    | Test loss: 0.0001301109732594341\n",
            "Epoch: 635 | Loss: 9.65267390711233e-05      | Test loss: 7.800459570717067e-05\n",
            "Epoch: 636 | Loss: 5.40785476914607e-05      | Test loss: 6.014108748786384e-06\n",
            "Epoch: 637 | Loss: 1.4002621355757583e-05    | Test loss: 0.000189298385521397\n",
            "Epoch: 638 | Loss: 0.00012802929268218577    | Test loss: 9.434819367015734e-05\n",
            "Epoch: 639 | Loss: 5.2712857723236084e-05    | Test loss: 0.0002612113894429058\n",
            "Epoch: 640 | Loss: 0.00021769925660919398    | Test loss: 0.0003109633980784565\n",
            "Epoch: 641 | Loss: 0.00025830493541434407    | Test loss: 8.606314804637805e-05\n",
            "Epoch: 642 | Loss: 9.256303019355983e-05     | Test loss: 0.00038538576336577535\n",
            "Epoch: 643 | Loss: 0.00025840700254775584    | Test loss: 0.0005402326351031661\n",
            "Epoch: 644 | Loss: 0.00037214980693534017    | Test loss: 0.00041078924550674856\n",
            "Epoch: 645 | Loss: 0.00027291104197502136    | Test loss: 2.593994213384576e-05\n",
            "Epoch: 646 | Loss: 2.1859257685719058e-05    | Test loss: 0.00023235678963828832\n",
            "Epoch: 647 | Loss: 0.00019867121591232717    | Test loss: 0.00019626021094154567\n",
            "Epoch: 648 | Loss: 0.00016028880781959742    | Test loss: 0.00010420680337119848\n",
            "Epoch: 649 | Loss: 7.515028119087219e-05     | Test loss: 0.0001065909891622141\n",
            "Epoch: 650 | Loss: 8.609220094513148e-05     | Test loss: 0.0001587212027516216\n",
            "Epoch: 651 | Loss: 0.00010454356379341334    | Test loss: 0.00013012885756324977\n",
            "Epoch: 652 | Loss: 7.569864101242274e-05     | Test loss: 0.00016248822794295847\n",
            "Epoch: 653 | Loss: 0.00015025585889816284    | Test loss: 0.00015901922597549856\n",
            "Epoch: 654 | Loss: 0.00015361532859969884    | Test loss: 0.00011048912710975856\n",
            "Epoch: 655 | Loss: 4.40880648966413e-05      | Test loss: 0.00011805891699623317\n",
            "Epoch: 656 | Loss: 5.070641782367602e-05     | Test loss: 0.0001337945432169363\n",
            "Epoch: 657 | Loss: 0.00013447701348923147    | Test loss: 9.450316429138184e-05\n",
            "Epoch: 658 | Loss: 0.00010176673822570592    | Test loss: 0.00020624995522666723\n",
            "Epoch: 659 | Loss: 0.00012648552365135401    | Test loss: 0.00021150111570023\n",
            "Epoch: 660 | Loss: 0.00013306438631843776    | Test loss: 4.8643349146004766e-05\n",
            "Epoch: 661 | Loss: 5.9451907873153687e-05    | Test loss: 1.7970800399780273e-05\n",
            "Epoch: 662 | Loss: 3.4331529604969546e-05    | Test loss: 0.00027396081713959575\n",
            "Epoch: 663 | Loss: 0.0001862458884716034     | Test loss: 0.00027236342430114746\n",
            "Epoch: 664 | Loss: 0.00018677189655136317    | Test loss: 7.098913101799553e-06\n",
            "Epoch: 665 | Loss: 1.0804831617861055e-05    | Test loss: 6.0230493545532227e-05\n",
            "Epoch: 666 | Loss: 5.114153100294061e-05     | Test loss: 0.00014276504225563258\n",
            "Epoch: 667 | Loss: 0.00010950267460430041    | Test loss: 6.197691254783422e-05\n",
            "Epoch: 668 | Loss: 5.679354217136279e-05     | Test loss: 0.0002737223985604942\n",
            "Epoch: 669 | Loss: 0.00018754527263808995    | Test loss: 0.00031275153742171824\n",
            "Epoch: 670 | Loss: 0.00021046772599220276    | Test loss: 8.531213097739965e-05\n",
            "Epoch: 671 | Loss: 3.5008044505957514e-05    | Test loss: 0.00035896897315979004\n",
            "Epoch: 672 | Loss: 0.00029794350848533213    | Test loss: 0.0004963755491189659\n",
            "Epoch: 673 | Loss: 0.00040067656664177775    | Test loss: 0.0003580152988433838\n",
            "Epoch: 674 | Loss: 0.0002970285713672638     | Test loss: 2.8008222216158174e-05\n",
            "Epoch: 675 | Loss: 1.5538185834884644e-05    | Test loss: 0.0003996074083261192\n",
            "Epoch: 676 | Loss: 0.00028710439801216125    | Test loss: 0.0004721105215139687\n",
            "Epoch: 677 | Loss: 0.0003566771629266441     | Test loss: 0.0002758622285909951\n",
            "Epoch: 678 | Loss: 0.0002236194850411266     | Test loss: 0.0001617133675608784\n",
            "Epoch: 679 | Loss: 9.140148904407397e-05     | Test loss: 0.0002942800638265908\n",
            "Epoch: 680 | Loss: 0.00017944052524399012    | Test loss: 0.00015282034291885793\n",
            "Epoch: 681 | Loss: 6.40235812170431e-05      | Test loss: 0.00022016763978172094\n",
            "Epoch: 682 | Loss: 0.00022095069289207458    | Test loss: 0.0002953529474325478\n",
            "Epoch: 683 | Loss: 0.0002821207162924111     | Test loss: 0.00010296106484020129\n",
            "Epoch: 684 | Loss: 0.00014263093180488795    | Test loss: 0.00032981036929413676\n",
            "Epoch: 685 | Loss: 0.00017710104293655604    | Test loss: 0.00045940876589156687\n",
            "Epoch: 686 | Loss: 0.0002704553189687431     | Test loss: 0.00031667351140640676\n",
            "Epoch: 687 | Loss: 0.0001604802964720875     | Test loss: 7.070899300742894e-05\n",
            "Epoch: 688 | Loss: 0.00013213008060120046    | Test loss: 0.00016021131887100637\n",
            "Epoch: 689 | Loss: 0.00020167604088783264    | Test loss: 1.7863512766780332e-05\n",
            "Epoch: 690 | Loss: 7.084310345817357e-05     | Test loss: 0.0004363715706858784\n",
            "Epoch: 691 | Loss: 0.00023997054086066782    | Test loss: 0.0005545437452383339\n",
            "Epoch: 692 | Loss: 0.0003264211118221283     | Test loss: 0.00040289759635925293\n",
            "Epoch: 693 | Loss: 0.00021135508723091334    | Test loss: 1.1408328646211885e-05\n",
            "Epoch: 694 | Loss: 8.476302173221484e-05     | Test loss: 8.799433999229223e-05\n",
            "Epoch: 695 | Loss: 0.0001585431455168873     | Test loss: 8.216500282287598e-05\n",
            "Epoch: 696 | Loss: 5.168914867681451e-05     | Test loss: 0.00027343034162186086\n",
            "Epoch: 697 | Loss: 0.00012537538714241236    | Test loss: 0.0001882016658782959\n",
            "Epoch: 698 | Loss: 7.64191136113368e-05      | Test loss: 0.00012342333502601832\n",
            "Epoch: 699 | Loss: 0.00014056936197448522    | Test loss: 0.00014677047147415578\n",
            "Epoch: 700 | Loss: 0.0001425422669854015     | Test loss: 8.88645663508214e-05\n",
            "Epoch: 701 | Loss: 4.748925493913703e-05     | Test loss: 4.429221007740125e-05\n",
            "Epoch: 702 | Loss: 2.6705116397351958e-05    | Test loss: 0.00025200843811035156\n",
            "Epoch: 703 | Loss: 0.00018346607976127416    | Test loss: 0.0002624392509460449\n",
            "Epoch: 704 | Loss: 0.00018113925762008876    | Test loss: 1.602768861630466e-05\n",
            "Epoch: 705 | Loss: 1.4486909094557632e-05    | Test loss: 0.0001062810406438075\n",
            "Epoch: 706 | Loss: 9.892806701827794e-05     | Test loss: 3.945231583202258e-05\n",
            "Epoch: 707 | Loss: 1.5001744031906128e-05    | Test loss: 3.849864151561633e-05\n",
            "Epoch: 708 | Loss: 3.124475551885553e-05     | Test loss: 0.00014690756506752223\n",
            "Epoch: 709 | Loss: 0.0001189298927783966     | Test loss: 5.835294723510742e-05\n",
            "Epoch: 710 | Loss: 6.324648711597547e-05     | Test loss: 0.00027634500293061137\n",
            "Epoch: 711 | Loss: 0.00017735586152411997    | Test loss: 0.0003224789979867637\n",
            "Epoch: 712 | Loss: 0.0002033568889601156     | Test loss: 0.00010933280282188207\n",
            "Epoch: 713 | Loss: 4.055723547935486e-05     | Test loss: 0.00028190016746520996\n",
            "Epoch: 714 | Loss: 0.0002521417918615043     | Test loss: 0.00037938356399536133\n",
            "Epoch: 715 | Loss: 0.0003216937184333801     | Test loss: 0.00021281838417053223\n",
            "Epoch: 716 | Loss: 0.0001943349780049175     | Test loss: 0.00019096136384177953\n",
            "Epoch: 717 | Loss: 0.0001098908469430171     | Test loss: 0.00030028223409317434\n",
            "Epoch: 718 | Loss: 0.00019391105161048472    | Test loss: 0.00014497638039756566\n",
            "Epoch: 719 | Loss: 8.007138967514038e-05     | Test loss: 0.0002480327966623008\n",
            "Epoch: 720 | Loss: 0.00021152124099899083    | Test loss: 0.0003483116743154824\n",
            "Epoch: 721 | Loss: 0.00028468668460845947    | Test loss: 0.00018547177023719996\n",
            "Epoch: 722 | Loss: 0.0001615427463548258     | Test loss: 0.0002138078270945698\n",
            "Epoch: 723 | Loss: 0.0001379892200930044     | Test loss: 0.00032027362613007426\n",
            "Epoch: 724 | Loss: 0.00021872967772651464    | Test loss: 0.0001635968656046316\n",
            "Epoch: 725 | Loss: 0.00010286867473041639    | Test loss: 0.00022957325563766062\n",
            "Epoch: 726 | Loss: 0.00018963888578582555    | Test loss: 0.0003310978354420513\n",
            "Epoch: 727 | Loss: 0.00026451348094269633    | Test loss: 0.00017055272473953664\n",
            "Epoch: 728 | Loss: 0.00014385729446075857    | Test loss: 0.00022546053514815867\n",
            "Epoch: 729 | Loss: 0.0001525111438240856     | Test loss: 0.00033013225765898824\n",
            "Epoch: 730 | Loss: 0.00023130327463150024    | Test loss: 0.00017300844774581492\n",
            "Epoch: 731 | Loss: 0.00011461973190307617    | Test loss: 0.0002194046974182129\n",
            "Epoch: 732 | Loss: 0.000177726149559021      | Test loss: 0.000321376312058419\n",
            "Epoch: 733 | Loss: 0.00025333985104225576    | Test loss: 0.00016241669072769582\n",
            "Epoch: 734 | Loss: 0.00013427510566543788    | Test loss: 0.00023107528977561742\n",
            "Epoch: 735 | Loss: 0.00015975310816429555    | Test loss: 0.0003346264420542866\n",
            "Epoch: 736 | Loss: 0.00023737773881293833    | Test loss: 0.00017758012108970433\n",
            "Epoch: 737 | Loss: 0.00012051090743625537    | Test loss: 0.00021363496489357203\n",
            "Epoch: 738 | Loss: 0.000171104067703709      | Test loss: 0.0003156602324452251\n",
            "Epoch: 739 | Loss: 0.00024697184562683105    | Test loss: 0.000157839065650478\n",
            "Epoch: 740 | Loss: 0.00012898891873192042    | Test loss: 0.00023360848717857152\n",
            "Epoch: 741 | Loss: 0.00016324296302627772    | Test loss: 0.0003363847790751606\n",
            "Epoch: 742 | Loss: 0.00024009049229789525    | Test loss: 0.00017971397028304636\n",
            "Epoch: 743 | Loss: 0.00012338385568000376    | Test loss: 0.00021009445481467992\n",
            "Epoch: 744 | Loss: 0.000167240941664204      | Test loss: 0.0003119528410024941\n",
            "Epoch: 745 | Loss: 0.0002430893509881571     | Test loss: 0.00015503764734603465\n",
            "Epoch: 746 | Loss: 0.00012592152052093297    | Test loss: 0.00023455024347640574\n",
            "Epoch: 747 | Loss: 0.00016473532014060766    | Test loss: 0.00033667683601379395\n",
            "Epoch: 748 | Loss: 0.0002410016895737499     | Test loss: 0.00018050074868369848\n",
            "Epoch: 749 | Loss: 0.00012461170263122767    | Test loss: 0.00020783543004654348\n",
            "Epoch: 750 | Loss: 0.00016489849076606333    | Test loss: 0.0003094017447438091\n",
            "Epoch: 751 | Loss: 0.0002405621053185314     | Test loss: 0.00015320777310989797\n",
            "Epoch: 752 | Loss: 0.0001240305573446676     | Test loss: 0.00023462176613975316\n",
            "Epoch: 753 | Loss: 0.00016521289944648743    | Test loss: 0.0003362596035003662\n",
            "Epoch: 754 | Loss: 0.00024103894247673452    | Test loss: 0.00018064976029563695\n",
            "Epoch: 755 | Loss: 0.00012506768689490855    | Test loss: 0.0002061545819742605\n",
            "Epoch: 756 | Loss: 0.00016327128105331212    | Test loss: 0.00030741095542907715\n",
            "Epoch: 757 | Loss: 0.00023871884332038462    | Test loss: 0.000151902437210083\n",
            "Epoch: 758 | Loss: 0.00012276097550056875    | Test loss: 0.00023431182489730418\n",
            "Epoch: 759 | Loss: 0.00016517937183380127    | Test loss: 0.00033546090708114207\n",
            "Epoch: 760 | Loss: 0.00024060680880211294    | Test loss: 0.0001804113417165354\n",
            "Epoch: 761 | Loss: 0.00012506246275734156    | Test loss: 0.00020492076873779297\n",
            "Epoch: 762 | Loss: 0.00016213208436965942    | Test loss: 0.0003058016300201416\n",
            "Epoch: 763 | Loss: 0.00023730024986434728    | Test loss: 0.0001509606809122488\n",
            "Epoch: 764 | Loss: 0.00012187287211418152    | Test loss: 0.00023371577844955027\n",
            "Epoch: 765 | Loss: 0.00016481056809425354    | Test loss: 0.0003344714641571045\n",
            "Epoch: 766 | Loss: 0.000239901986788027      | Test loss: 0.00018001199350692332\n",
            "Epoch: 767 | Loss: 0.00012480467557907104    | Test loss: 0.00020388365373946726\n",
            "Epoch: 768 | Loss: 0.0001612439809832722     | Test loss: 0.0003043949545826763\n",
            "Epoch: 769 | Loss: 0.00023612528457306325    | Test loss: 0.00015015601820778102\n",
            "Epoch: 770 | Loss: 0.00012118518498027697    | Test loss: 0.00023301839246414602\n",
            "Epoch: 771 | Loss: 0.00016432031407020986    | Test loss: 0.00033333897590637207\n",
            "Epoch: 772 | Loss: 0.00023907348804641515    | Test loss: 0.00017943978309631348\n",
            "Epoch: 773 | Loss: 0.00012441352009773254    | Test loss: 0.0002030253381235525\n",
            "Epoch: 774 | Loss: 0.00016049444093368948    | Test loss: 0.00030313729075714946\n",
            "Epoch: 775 | Loss: 0.00023506581783294678    | Test loss: 0.00014948248281143606\n",
            "Epoch: 776 | Loss: 0.00012060776498401538    | Test loss: 0.00023227930068969727\n",
            "Epoch: 777 | Loss: 0.0001637660025153309     | Test loss: 0.0003322541597299278\n",
            "Epoch: 778 | Loss: 0.0002382308302912861     | Test loss: 0.0001789391099009663\n",
            "Epoch: 779 | Loss: 0.00012402534775901586    | Test loss: 0.00020210146612953395\n",
            "Epoch: 780 | Loss: 0.00015975683345459402    | Test loss: 0.00030185579089447856\n",
            "Epoch: 781 | Loss: 0.0002340577484574169     | Test loss: 0.00014876722707413137\n",
            "Epoch: 782 | Loss: 0.00012004226300632581    | Test loss: 0.00023155807866714895\n",
            "Epoch: 783 | Loss: 0.0001632027269806713     | Test loss: 0.00033115147380158305\n",
            "Epoch: 784 | Loss: 0.00023735687136650085    | Test loss: 0.0001783549814717844\n",
            "Epoch: 785 | Loss: 0.00012357757077552378    | Test loss: 0.0002012789191212505\n",
            "Epoch: 786 | Loss: 0.00015911311493255198    | Test loss: 0.00030069946660660207\n",
            "Epoch: 787 | Loss: 0.00023312792473006994    | Test loss: 0.00014815926260780543\n",
            "Epoch: 788 | Loss: 0.00011956244998145849    | Test loss: 0.00023081302060745656\n",
            "Epoch: 789 | Loss: 0.00016261562996078283    | Test loss: 0.000330078590195626\n",
            "Epoch: 790 | Loss: 0.00023652240633964539    | Test loss: 0.00017783045768737793\n",
            "Epoch: 791 | Loss: 0.0001231856585945934     | Test loss: 0.00020048022270202637\n",
            "Epoch: 792 | Loss: 0.00015845746384002268    | Test loss: 0.00029949547024443746\n",
            "Epoch: 793 | Loss: 0.00023218915157485753    | Test loss: 0.0001475512981414795\n",
            "Epoch: 794 | Loss: 0.0001190520852105692     | Test loss: 0.00023010969744063914\n",
            "Epoch: 795 | Loss: 0.00016208365559577942    | Test loss: 0.0003289938031230122\n",
            "Epoch: 796 | Loss: 0.00023567676544189453    | Test loss: 0.00017725229554343969\n",
            "Epoch: 797 | Loss: 0.00012274086475372314    | Test loss: 0.000199800735572353\n",
            "Epoch: 798 | Loss: 0.000157887494424358      | Test loss: 0.0002985119936056435\n",
            "Epoch: 799 | Loss: 0.00023137405514717102    | Test loss: 0.00014703869237564504\n",
            "Epoch: 800 | Loss: 0.00011862888641189784    | Test loss: 0.00022936463938094676\n",
            "Epoch: 801 | Loss: 0.00016149580187629908    | Test loss: 0.00032790302066132426\n",
            "Epoch: 802 | Loss: 0.00023483484983444214    | Test loss: 0.00017668008513282984\n",
            "Epoch: 803 | Loss: 0.00012229158892296255    | Test loss: 0.0001990795135498047\n",
            "Epoch: 804 | Loss: 0.00015732421888969839    | Test loss: 0.000297480815788731\n",
            "Epoch: 805 | Loss: 0.00023055523342918605    | Test loss: 0.00014653801918029785\n",
            "Epoch: 806 | Loss: 0.00011821687076007947    | Test loss: 0.00022855401039123535\n",
            "Epoch: 807 | Loss: 0.00016088112897705287    | Test loss: 0.00032679439755156636\n",
            "Epoch: 808 | Loss: 0.00023396014876198024    | Test loss: 0.00017610788927413523\n",
            "Epoch: 809 | Loss: 0.00012184008664917201    | Test loss: 0.00019841789617203176\n",
            "Epoch: 810 | Loss: 0.0001568220613989979     | Test loss: 0.0002964615705423057\n",
            "Epoch: 811 | Loss: 0.0002297721803188324     | Test loss: 0.00014597774134017527\n",
            "Epoch: 812 | Loss: 0.00011781677312683314    | Test loss: 0.00022795199765823781\n",
            "Epoch: 813 | Loss: 0.00016036331362556666    | Test loss: 0.0003258645592723042\n",
            "Epoch: 814 | Loss: 0.00023320168838836253    | Test loss: 0.0001756012497935444\n",
            "Epoch: 815 | Loss: 0.00012143477943027392    | Test loss: 0.00019765496836043894\n",
            "Epoch: 816 | Loss: 0.0001562304823892191     | Test loss: 0.0002953827497549355\n",
            "Epoch: 817 | Loss: 0.0002289287804160267     | Test loss: 0.0001453995646443218\n",
            "Epoch: 818 | Loss: 0.00011736601300071925    | Test loss: 0.00022730231285095215\n",
            "Epoch: 819 | Loss: 0.000159858915139921      | Test loss: 0.00032489298610016704\n",
            "Epoch: 820 | Loss: 0.00023243874602485448    | Test loss: 0.00017511248006485403\n",
            "Epoch: 821 | Loss: 0.0001210533082485199     | Test loss: 0.0001970231533050537\n",
            "Epoch: 822 | Loss: 0.00015572310076095164    | Test loss: 0.0002944648149423301\n",
            "Epoch: 823 | Loss: 0.00022819414152763784    | Test loss: 0.00014494657807517797\n",
            "Epoch: 824 | Loss: 0.00011699423339450732    | Test loss: 0.0002266228257212788\n",
            "Epoch: 825 | Loss: 0.0001593224733369425     | Test loss: 0.00032392144203186035\n",
            "Epoch: 826 | Loss: 0.0002316810132469982     | Test loss: 0.0001746594934957102\n",
            "Epoch: 827 | Loss: 0.00012068748765159398    | Test loss: 0.00019634366617538035\n",
            "Epoch: 828 | Loss: 0.00015519112639594823    | Test loss: 0.0002935171069111675\n",
            "Epoch: 829 | Loss: 0.00022743269801139832    | Test loss: 0.00014440416998695582\n",
            "Epoch: 830 | Loss: 0.00011657476716209203    | Test loss: 0.00022602677927352488\n",
            "Epoch: 831 | Loss: 0.0001588419108884409     | Test loss: 0.00032303930493071675\n",
            "Epoch: 832 | Loss: 0.0002309784322278574     | Test loss: 0.00017420649237465113\n",
            "Epoch: 833 | Loss: 0.00012031271762680262    | Test loss: 0.00019566416449379176\n",
            "Epoch: 834 | Loss: 0.00015465542674064636    | Test loss: 0.00029253363027237356\n",
            "Epoch: 835 | Loss: 0.00022667422308586538    | Test loss: 0.00014390349679160863\n",
            "Epoch: 836 | Loss: 0.00011618286225711927    | Test loss: 0.00022545456886291504\n",
            "Epoch: 837 | Loss: 0.00015836060629226267    | Test loss: 0.00032213926897384226\n",
            "Epoch: 838 | Loss: 0.0002302713692188263     | Test loss: 0.00017374157323502004\n",
            "Epoch: 839 | Loss: 0.00011994689702987671    | Test loss: 0.00019503831572365016\n",
            "Epoch: 840 | Loss: 0.0001541823148727417     | Test loss: 0.0002916276571340859\n",
            "Epoch: 841 | Loss: 0.00022597835049964488    | Test loss: 0.00014348029799293727\n",
            "Epoch: 842 | Loss: 0.00011585056927287951    | Test loss: 0.00022476911544799805\n",
            "Epoch: 843 | Loss: 0.0001578606606926769     | Test loss: 0.0003212153969798237\n",
            "Epoch: 844 | Loss: 0.000229536002734676      | Test loss: 0.00017324090003967285\n",
            "Epoch: 845 | Loss: 0.00011955275840591639    | Test loss: 0.00019448398961685598\n",
            "Epoch: 846 | Loss: 0.00015373453788924962    | Test loss: 0.0002907693269662559\n",
            "Epoch: 847 | Loss: 0.0002253003476653248     | Test loss: 0.00014303326315712184\n",
            "Epoch: 848 | Loss: 0.00011550933413673192    | Test loss: 0.0002241671027150005\n",
            "Epoch: 849 | Loss: 0.0001573808549437672     | Test loss: 0.00032033323077484965\n",
            "Epoch: 850 | Loss: 0.0002288348914589733     | Test loss: 0.00017276406288146973\n",
            "Epoch: 851 | Loss: 0.00011919140524696559    | Test loss: 0.00019388794316910207\n",
            "Epoch: 852 | Loss: 0.00015328079462051392    | Test loss: 0.00028991102590225637\n",
            "Epoch: 853 | Loss: 0.00022462158813141286    | Test loss: 0.000142580276587978\n",
            "Epoch: 854 | Loss: 0.00011513754725456238    | Test loss: 0.00022355913824867457\n",
            "Epoch: 855 | Loss: 0.00015690848522353917    | Test loss: 0.00031945109367370605\n",
            "Epoch: 856 | Loss: 0.00022816583805251867    | Test loss: 0.00017233491234946996\n",
            "Epoch: 857 | Loss: 0.00011885389540111646    | Test loss: 0.00019333958334755152\n",
            "Epoch: 858 | Loss: 0.00015282184176612645    | Test loss: 0.0002891123294830322\n",
            "Epoch: 859 | Loss: 0.0002239719033241272     | Test loss: 0.00014216304407455027\n",
            "Epoch: 860 | Loss: 0.00011480152898002416    | Test loss: 0.0002229750098194927\n",
            "Epoch: 861 | Loss: 0.0001564607082400471     | Test loss: 0.0003185927926097065\n",
            "Epoch: 862 | Loss: 0.00022749304480385035    | Test loss: 0.0001718819112284109\n",
            "Epoch: 863 | Loss: 0.00011849329166579992    | Test loss: 0.00019281505956314504\n",
            "Epoch: 864 | Loss: 0.00015240759239532053    | Test loss: 0.00028827786445617676\n",
            "Epoch: 865 | Loss: 0.00022332370281219482    | Test loss: 0.00014173387899063528\n",
            "Epoch: 866 | Loss: 0.00011446922871982679    | Test loss: 0.00022243261628318578\n",
            "Epoch: 867 | Loss: 0.00015603304200340062    | Test loss: 0.0003178417682647705\n",
            "Epoch: 868 | Loss: 0.00022688732133246958    | Test loss: 0.0001714765967335552\n",
            "Epoch: 869 | Loss: 0.00011818036728072912    | Test loss: 0.00019224881543777883\n",
            "Epoch: 870 | Loss: 0.00015194938168860972    | Test loss: 0.0002874612691812217\n",
            "Epoch: 871 | Loss: 0.00022268221073318273    | Test loss: 0.0001412987767253071\n",
            "Epoch: 872 | Loss: 0.00011413171887397766    | Test loss: 0.00022190212621353567\n",
            "Epoch: 873 | Loss: 0.00015561506734229624    | Test loss: 0.00031700730323791504\n",
            "Epoch: 874 | Loss: 0.0002262607158627361     | Test loss: 0.00017101169214583933\n",
            "Epoch: 875 | Loss: 0.00011784509115386754    | Test loss: 0.00019171833992004395\n",
            "Epoch: 876 | Loss: 0.0001515254407422617     | Test loss: 0.0002866923750843853\n",
            "Epoch: 877 | Loss: 0.0002220548631157726     | Test loss: 0.00014093518257141113\n",
            "Epoch: 878 | Loss: 0.00011381357762729749    | Test loss: 0.00022135376639198512\n",
            "Epoch: 879 | Loss: 0.0001551978348288685     | Test loss: 0.0003162503126077354\n",
            "Epoch: 880 | Loss: 0.00022566839470528066    | Test loss: 0.000170654064277187\n",
            "Epoch: 881 | Loss: 0.00011757314496207982    | Test loss: 0.00019115806207992136\n",
            "Epoch: 882 | Loss: 0.0001510761649115011     | Test loss: 0.0002859175147023052\n",
            "Epoch: 883 | Loss: 0.00022142977104522288    | Test loss: 0.00014050603203941137\n",
            "Epoch: 884 | Loss: 0.00011346638348186389    | Test loss: 0.00022085309319663793\n",
            "Epoch: 885 | Loss: 0.00015483722381759435    | Test loss: 0.0003154933510813862\n",
            "Epoch: 886 | Loss: 0.0002251125843031332     | Test loss: 0.00017028450383804739\n",
            "Epoch: 887 | Loss: 0.00011729374818969518    | Test loss: 0.00019063949002884328\n",
            "Epoch: 888 | Loss: 0.00015064849867485464    | Test loss: 0.0002851307508535683\n",
            "Epoch: 889 | Loss: 0.00022080466442275792    | Test loss: 0.00014010071754455566\n",
            "Epoch: 890 | Loss: 0.00011313035793136805    | Test loss: 0.0002203464537160471\n",
            "Epoch: 891 | Loss: 0.0001544482947792858     | Test loss: 0.00031473039416596293\n",
            "Epoch: 892 | Loss: 0.00022452771372627467    | Test loss: 0.00016986131959129125\n",
            "Epoch: 893 | Loss: 0.00011699423339450732    | Test loss: 0.00019015670113731176\n",
            "Epoch: 894 | Loss: 0.0001502498926129192     | Test loss: 0.00028440356254577637\n",
            "Epoch: 895 | Loss: 0.00022022053599357605    | Test loss: 0.00013971925363875926\n",
            "Epoch: 896 | Loss: 0.00011281743354629725    | Test loss: 0.00021986365027260035\n",
            "Epoch: 897 | Loss: 0.0001540899247629568     | Test loss: 0.00031407474307343364\n",
            "Epoch: 898 | Loss: 0.0002239905297756195     | Test loss: 0.00016954541206359863\n",
            "Epoch: 899 | Loss: 0.00011673643894027919    | Test loss: 0.0001896142930490896\n",
            "Epoch: 900 | Loss: 0.00014982893480919302    | Test loss: 0.0002836883068084717\n",
            "Epoch: 901 | Loss: 0.0002196595014538616     | Test loss: 0.00013932585716247559\n",
            "Epoch: 902 | Loss: 0.00011250823445152491    | Test loss: 0.00021941066370345652\n",
            "Epoch: 903 | Loss: 0.00015371813788078725    | Test loss: 0.0003133535501547158\n",
            "Epoch: 904 | Loss: 0.00022343770251609385    | Test loss: 0.00016915201558731496\n",
            "Epoch: 905 | Loss: 0.00011642500612651929    | Test loss: 0.00018919110880233347\n",
            "Epoch: 906 | Loss: 0.00014949217438697815    | Test loss: 0.00028301478596404195\n",
            "Epoch: 907 | Loss: 0.00021911859221290797    | Test loss: 0.00013900994963478297\n",
            "Epoch: 908 | Loss: 0.00011225789785385132    | Test loss: 0.00021889805793762207\n",
            "Epoch: 909 | Loss: 0.00015332549810409546    | Test loss: 0.00031261442927643657\n",
            "Epoch: 910 | Loss: 0.00022287442698143423    | Test loss: 0.0001687824697000906\n",
            "Epoch: 911 | Loss: 0.00011615305993473157    | Test loss: 0.00018872618966270238\n",
            "Epoch: 912 | Loss: 0.0001491256116423756     | Test loss: 0.0002823829709086567\n",
            "Epoch: 913 | Loss: 0.0002186067431466654     | Test loss: 0.00013869404210709035\n",
            "Epoch: 914 | Loss: 0.00011199786968063563    | Test loss: 0.00021838545217178762\n",
            "Epoch: 915 | Loss: 0.00015293881006073207    | Test loss: 0.0003118932363577187\n",
            "Epoch: 916 | Loss: 0.00022230968170333654    | Test loss: 0.00016838908777572215\n",
            "Epoch: 917 | Loss: 0.00011584386083995923    | Test loss: 0.00018830299086403102\n",
            "Epoch: 918 | Loss: 0.0001487828849349171     | Test loss: 0.00028170348377898335\n",
            "Epoch: 919 | Loss: 0.00021807625307701528    | Test loss: 0.00013832449621986598\n",
            "Epoch: 920 | Loss: 0.00011171847290825099    | Test loss: 0.00021796226792503148\n",
            "Epoch: 921 | Loss: 0.00015260055079124868    | Test loss: 0.00031125545501708984\n",
            "Epoch: 922 | Loss: 0.00022180676751304418    | Test loss: 0.00016807913198135793\n",
            "Epoch: 923 | Loss: 0.00011559129052329808    | Test loss: 0.0001877963513834402\n",
            "Epoch: 924 | Loss: 0.00014839321374893188    | Test loss: 0.0002809703291859478\n",
            "Epoch: 925 | Loss: 0.00021751895837951452    | Test loss: 0.0001379549503326416\n",
            "Epoch: 926 | Loss: 0.00011144056770717725    | Test loss: 0.00021750331507064402\n",
            "Epoch: 927 | Loss: 0.00015223250375129282    | Test loss: 0.000310617673676461\n",
            "Epoch: 928 | Loss: 0.00022129490389488637    | Test loss: 0.00016776323900558054\n",
            "Epoch: 929 | Loss: 0.00011533498764038086    | Test loss: 0.00018730759620666504\n",
            "Epoch: 930 | Loss: 0.00014803410158492625    | Test loss: 0.0002803146780934185\n",
            "Epoch: 931 | Loss: 0.00021702199592255056    | Test loss: 0.00013759135617874563\n",
            "Epoch: 932 | Loss: 0.00011116340465378016    | Test loss: 0.0002170860825572163\n",
            "Epoch: 933 | Loss: 0.0001518935023341328     | Test loss: 0.0003099620225839317\n",
            "Epoch: 934 | Loss: 0.00022079348855186254    | Test loss: 0.0001673758088145405\n",
            "Epoch: 935 | Loss: 0.00011505112343002111    | Test loss: 0.00018697380437515676\n",
            "Epoch: 936 | Loss: 0.00014774427108932287    | Test loss: 0.0002797901688609272\n",
            "Epoch: 937 | Loss: 0.00021658465266227722    | Test loss: 0.0001373589038848877\n",
            "Epoch: 938 | Loss: 0.00011096000525867566    | Test loss: 0.00021657944307662547\n",
            "Epoch: 939 | Loss: 0.00015153363347053528    | Test loss: 0.0003093242703471333\n",
            "Epoch: 940 | Loss: 0.00022029131650924683    | Test loss: 0.00016702413267921656\n",
            "Epoch: 941 | Loss: 0.0001147694856626913     | Test loss: 0.00018652676953934133\n",
            "Epoch: 942 | Loss: 0.00014740973711013794    | Test loss: 0.00027915238752029836\n",
            "Epoch: 943 | Loss: 0.00021608472161460668    | Test loss: 0.00013698935799766332\n",
            "Epoch: 944 | Loss: 0.00011068060848629102    | Test loss: 0.00021625161753036082\n",
            "Epoch: 945 | Loss: 0.0001512467861175537     | Test loss: 0.00030872822389937937\n",
            "Epoch: 946 | Loss: 0.00021984055638313293    | Test loss: 0.00016679168038535863\n",
            "Epoch: 947 | Loss: 0.00011455863568698987    | Test loss: 0.0001860439806478098\n",
            "Epoch: 948 | Loss: 0.00014704540080856532    | Test loss: 0.00027849077014252543\n",
            "Epoch: 949 | Loss: 0.00021557956642936915    | Test loss: 0.00013667941675521433\n",
            "Epoch: 950 | Loss: 0.00011042803816962987    | Test loss: 0.00021579265012405813\n",
            "Epoch: 951 | Loss: 0.00015091597742866725    | Test loss: 0.0003081142785958946\n",
            "Epoch: 952 | Loss: 0.00021936147822998464    | Test loss: 0.00016644597053527832\n",
            "Epoch: 953 | Loss: 0.00011432245082687587    | Test loss: 0.00018568038649391383\n",
            "Epoch: 954 | Loss: 0.00014673545956611633    | Test loss: 0.00027788878651335835\n",
            "Epoch: 955 | Loss: 0.0002151005028281361     | Test loss: 0.0001363575429422781\n",
            "Epoch: 956 | Loss: 0.00011017695214832202    | Test loss: 0.00021539926819968969\n",
            "Epoch: 957 | Loss: 0.00015061124577187002    | Test loss: 0.00030754803447052836\n",
            "Epoch: 958 | Loss: 0.00021891816868446767    | Test loss: 0.00016616583161521703\n",
            "Epoch: 959 | Loss: 0.00011409446597099304    | Test loss: 0.0001852333516580984\n",
            "Epoch: 960 | Loss: 0.0001464061497244984     | Test loss: 0.00027727484120987356\n",
            "Epoch: 961 | Loss: 0.00021463260054588318    | Test loss: 0.00013602376566268504\n",
            "Epoch: 962 | Loss: 0.0001099236324080266     | Test loss: 0.00021505355834960938\n",
            "Epoch: 963 | Loss: 0.0001503221719758585     | Test loss: 0.00030698179034516215\n",
            "Epoch: 964 | Loss: 0.00021846368326805532    | Test loss: 0.0001658797264099121\n",
            "Epoch: 965 | Loss: 0.00011387393169570714    | Test loss: 0.00018483400344848633\n",
            "Epoch: 966 | Loss: 0.00014609098434448242    | Test loss: 0.00027669669361785054\n",
            "Epoch: 967 | Loss: 0.0002141818404197693     | Test loss: 0.00013573766045738012\n",
            "Epoch: 968 | Loss: 0.00010970234870910645    | Test loss: 0.0002146601618733257\n",
            "Epoch: 969 | Loss: 0.00015001147403381765    | Test loss: 0.00030643344507552683\n",
            "Epoch: 970 | Loss: 0.00021804570860695094    | Test loss: 0.00016563534154556692\n",
            "Epoch: 971 | Loss: 0.00011367797560524195    | Test loss: 0.00018441080464981496\n",
            "Epoch: 972 | Loss: 0.00014575570821762085    | Test loss: 0.00027607084484770894\n",
            "Epoch: 973 | Loss: 0.00021370202011894435    | Test loss: 0.00013537406630348414\n",
            "Epoch: 974 | Loss: 0.00010941922664642334    | Test loss: 0.00021435618691612035\n",
            "Epoch: 975 | Loss: 0.00014976337843108922    | Test loss: 0.0003059566079173237\n",
            "Epoch: 976 | Loss: 0.00021764860139228404    | Test loss: 0.00016540885553695261\n",
            "Epoch: 977 | Loss: 0.00011347532563377172    | Test loss: 0.00018395781808067113\n",
            "Epoch: 978 | Loss: 0.0001454234152333811     | Test loss: 0.0002754509332589805\n",
            "Epoch: 979 | Loss: 0.00021322890825103968    | Test loss: 0.0001350402890238911\n",
            "Epoch: 980 | Loss: 0.00010916739847743884    | Test loss: 0.000213998559047468\n",
            "Epoch: 981 | Loss: 0.00014947354793548584    | Test loss: 0.0003053903637919575\n",
            "Epoch: 982 | Loss: 0.00021719485812354833    | Test loss: 0.00016508102999068797\n",
            "Epoch: 983 | Loss: 0.00011321828060317785    | Test loss: 0.0001836717128753662\n",
            "Epoch: 984 | Loss: 0.00014520212425850332    | Test loss: 0.000275021797278896\n",
            "Epoch: 985 | Loss: 0.00021288990683387965    | Test loss: 0.00013487934484146535\n",
            "Epoch: 986 | Loss: 0.00010903105430770665    | Test loss: 0.00021352172188926488\n",
            "Epoch: 987 | Loss: 0.00014911219477653503    | Test loss: 0.0003047645150218159\n",
            "Epoch: 988 | Loss: 0.0002167180209653452     | Test loss: 0.00016475320444442332\n",
            "Epoch: 989 | Loss: 0.000112973153591156      | Test loss: 0.0001833558053476736\n",
            "Epoch: 990 | Loss: 0.00014494583592750132    | Test loss: 0.00027454496012069285\n",
            "Epoch: 991 | Loss: 0.0002124935417668894     | Test loss: 0.0001346409262623638\n",
            "Epoch: 992 | Loss: 0.00010882913920795545    | Test loss: 0.0002131521760020405\n",
            "Epoch: 993 | Loss: 0.00014883652329444885    | Test loss: 0.00030425190925598145\n",
            "Epoch: 994 | Loss: 0.0002163439930882305     | Test loss: 0.00016441941261291504\n",
            "Epoch: 995 | Loss: 0.00011275485303485766    | Test loss: 0.00018301009549759328\n",
            "Epoch: 996 | Loss: 0.0001446612150175497     | Test loss: 0.00027402042178437114\n",
            "Epoch: 997 | Loss: 0.0002120725839631632     | Test loss: 0.00013428329839371145\n",
            "Epoch: 998 | Loss: 0.00010858029418159276    | Test loss: 0.0002128362684743479\n",
            "Epoch: 999 | Loss: 0.0001485824614064768     | Test loss: 0.00030375123606063426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " print(list(model0.parameters()))   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p04N4uB2mcDI",
        "outputId": "b53058e6-80a0-4653-ea5a-a6e03cc35b42"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([0.6998], requires_grad=True), Parameter containing:\n",
            "tensor([0.2999], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "xQdBr4NkNqWU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count=torch.tensor(epoch_count).cpu().numpy()\n",
        "loss_values=torch.tensor(loss_values).cpu().numpy()\n",
        "test_loss_values=torch.tensor(test_loss_values).cpu().numpy()"
      ],
      "metadata": {
        "id": "BZJmwfm2Nu1y"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_count, loss_values, label='Train loss')\n",
        "plt.plot(epoch_count, test_loss_values, label='Test loss')\n",
        "plt.title('loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "VTa8KfFVNKWh",
        "outputId": "4a3bde40-d840-4f5f-fa16-ca05b4250749"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'loss')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  y_pred_new = model0(X_test)\n",
        "len(y_pred_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQvVSa8-4KyD",
        "outputId": "2190ee8e-b049-4e65-9a8f-50c09271de2b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idJXcA5S4-ni",
        "outputId": "11989a9d-f31b-4f0a-b13d-769d481fd4a5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(predictions=y_pred_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "9zc6GxSm5Ko9",
        "outputId": "71566f0f-b64a-4046-8b4f-5b9284af9ff1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving a model in pytorch"
      ],
      "metadata": {
        "id": "Oc18TIxpQGOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = [p.item() for p in list(model0.parameters())]\n",
        "params # -> to text file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiKj9hPF_1Wd",
        "outputId": "b27d1234-6b44-4a73-d9f2-a016dc7395e7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.699824333190918, 0.299852579832077]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "model_path = Path('models')\n",
        "model_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model_name = \"01_workflow.pth\"\n",
        "model_save_path = model_path / model_name\n",
        "\n",
        "torch.save(obj=model0.state_dict(), f=model_save_path)"
      ],
      "metadata": {
        "id": "pfG-sH12Q_Ej"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfLtNvcVSnZj",
        "outputId": "91dbac97-0854-4da1-d05f-20681c7cb717"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 1079 Mar  5 15:11 01_workflow.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded1 = torch.load('models/01_workflow.pth')"
      ],
      "metadata": {
        "id": "JxyGF-APTYWi"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1=LinearRegressionModel()\n",
        "model1.load_state_dict(torch.load(model_save_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30t73qCTTycd",
        "outputId": "8058cbec-b426-4e7b-b474-858c9b01a329"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1(torch.tensor([0, 1, 2, 3, 4, 5, 0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh-zf3z2T5yI",
        "outputId": "e661affe-51c9-4590-dce8-4179284a83ea"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2999, 0.9997, 1.6995, 2.3993, 3.0991, 3.7990, 0.2999],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model0(torch.tensor([0, 1, 2, 3, 4, 5, 0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIBI56e3f6cz",
        "outputId": "0545b2b0-7292-4b6d-d1a8-f4d4f1f720a5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2999, 0.9997, 1.6995, 2.3993, 3.0991, 3.7990, 0.2999],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Excercises"
      ],
      "metadata": {
        "id": "4vSJ8liNkVqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.tensor([2.3, 1.9])\n",
        "\n",
        "#create data\n",
        "start = 0\n",
        "end = 4\n",
        "step = 0.02\n",
        "args = torch.arange(start, end, step)\n",
        "x0 = torch.ones(len(args))\n",
        "x1 = args\n",
        "x2 = args**2\n",
        "X = torch.stack([x1, x2], dim=1)\n",
        "y=X @ weights.T + 0.4\n",
        "X\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRXNinIZki9l",
        "outputId": "11e1775d-78d0-4dfd-face-2ce050badafb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-4d49156c4e7c>:12: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3277.)\n",
            "  y=X @ weights.T + 0.4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000e+00, 0.0000e+00],\n",
              "        [2.0000e-02, 4.0000e-04],\n",
              "        [4.0000e-02, 1.6000e-03],\n",
              "        [6.0000e-02, 3.6000e-03],\n",
              "        [8.0000e-02, 6.4000e-03],\n",
              "        [1.0000e-01, 1.0000e-02],\n",
              "        [1.2000e-01, 1.4400e-02],\n",
              "        [1.4000e-01, 1.9600e-02],\n",
              "        [1.6000e-01, 2.5600e-02],\n",
              "        [1.8000e-01, 3.2400e-02],\n",
              "        [2.0000e-01, 4.0000e-02],\n",
              "        [2.2000e-01, 4.8400e-02],\n",
              "        [2.4000e-01, 5.7600e-02],\n",
              "        [2.6000e-01, 6.7600e-02],\n",
              "        [2.8000e-01, 7.8400e-02],\n",
              "        [3.0000e-01, 9.0000e-02],\n",
              "        [3.2000e-01, 1.0240e-01],\n",
              "        [3.4000e-01, 1.1560e-01],\n",
              "        [3.6000e-01, 1.2960e-01],\n",
              "        [3.8000e-01, 1.4440e-01],\n",
              "        [4.0000e-01, 1.6000e-01],\n",
              "        [4.2000e-01, 1.7640e-01],\n",
              "        [4.4000e-01, 1.9360e-01],\n",
              "        [4.6000e-01, 2.1160e-01],\n",
              "        [4.8000e-01, 2.3040e-01],\n",
              "        [5.0000e-01, 2.5000e-01],\n",
              "        [5.2000e-01, 2.7040e-01],\n",
              "        [5.4000e-01, 2.9160e-01],\n",
              "        [5.6000e-01, 3.1360e-01],\n",
              "        [5.8000e-01, 3.3640e-01],\n",
              "        [6.0000e-01, 3.6000e-01],\n",
              "        [6.2000e-01, 3.8440e-01],\n",
              "        [6.4000e-01, 4.0960e-01],\n",
              "        [6.6000e-01, 4.3560e-01],\n",
              "        [6.8000e-01, 4.6240e-01],\n",
              "        [7.0000e-01, 4.9000e-01],\n",
              "        [7.2000e-01, 5.1840e-01],\n",
              "        [7.4000e-01, 5.4760e-01],\n",
              "        [7.6000e-01, 5.7760e-01],\n",
              "        [7.8000e-01, 6.0840e-01],\n",
              "        [8.0000e-01, 6.4000e-01],\n",
              "        [8.2000e-01, 6.7240e-01],\n",
              "        [8.4000e-01, 7.0560e-01],\n",
              "        [8.6000e-01, 7.3960e-01],\n",
              "        [8.8000e-01, 7.7440e-01],\n",
              "        [9.0000e-01, 8.1000e-01],\n",
              "        [9.2000e-01, 8.4640e-01],\n",
              "        [9.4000e-01, 8.8360e-01],\n",
              "        [9.6000e-01, 9.2160e-01],\n",
              "        [9.8000e-01, 9.6040e-01],\n",
              "        [1.0000e+00, 1.0000e+00],\n",
              "        [1.0200e+00, 1.0404e+00],\n",
              "        [1.0400e+00, 1.0816e+00],\n",
              "        [1.0600e+00, 1.1236e+00],\n",
              "        [1.0800e+00, 1.1664e+00],\n",
              "        [1.1000e+00, 1.2100e+00],\n",
              "        [1.1200e+00, 1.2544e+00],\n",
              "        [1.1400e+00, 1.2996e+00],\n",
              "        [1.1600e+00, 1.3456e+00],\n",
              "        [1.1800e+00, 1.3924e+00],\n",
              "        [1.2000e+00, 1.4400e+00],\n",
              "        [1.2200e+00, 1.4884e+00],\n",
              "        [1.2400e+00, 1.5376e+00],\n",
              "        [1.2600e+00, 1.5876e+00],\n",
              "        [1.2800e+00, 1.6384e+00],\n",
              "        [1.3000e+00, 1.6900e+00],\n",
              "        [1.3200e+00, 1.7424e+00],\n",
              "        [1.3400e+00, 1.7956e+00],\n",
              "        [1.3600e+00, 1.8496e+00],\n",
              "        [1.3800e+00, 1.9044e+00],\n",
              "        [1.4000e+00, 1.9600e+00],\n",
              "        [1.4200e+00, 2.0164e+00],\n",
              "        [1.4400e+00, 2.0736e+00],\n",
              "        [1.4600e+00, 2.1316e+00],\n",
              "        [1.4800e+00, 2.1904e+00],\n",
              "        [1.5000e+00, 2.2500e+00],\n",
              "        [1.5200e+00, 2.3104e+00],\n",
              "        [1.5400e+00, 2.3716e+00],\n",
              "        [1.5600e+00, 2.4336e+00],\n",
              "        [1.5800e+00, 2.4964e+00],\n",
              "        [1.6000e+00, 2.5600e+00],\n",
              "        [1.6200e+00, 2.6244e+00],\n",
              "        [1.6400e+00, 2.6896e+00],\n",
              "        [1.6600e+00, 2.7556e+00],\n",
              "        [1.6800e+00, 2.8224e+00],\n",
              "        [1.7000e+00, 2.8900e+00],\n",
              "        [1.7200e+00, 2.9584e+00],\n",
              "        [1.7400e+00, 3.0276e+00],\n",
              "        [1.7600e+00, 3.0976e+00],\n",
              "        [1.7800e+00, 3.1684e+00],\n",
              "        [1.8000e+00, 3.2400e+00],\n",
              "        [1.8200e+00, 3.3124e+00],\n",
              "        [1.8400e+00, 3.3856e+00],\n",
              "        [1.8600e+00, 3.4596e+00],\n",
              "        [1.8800e+00, 3.5344e+00],\n",
              "        [1.9000e+00, 3.6100e+00],\n",
              "        [1.9200e+00, 3.6864e+00],\n",
              "        [1.9400e+00, 3.7636e+00],\n",
              "        [1.9600e+00, 3.8416e+00],\n",
              "        [1.9800e+00, 3.9204e+00],\n",
              "        [2.0000e+00, 4.0000e+00],\n",
              "        [2.0200e+00, 4.0804e+00],\n",
              "        [2.0400e+00, 4.1616e+00],\n",
              "        [2.0600e+00, 4.2436e+00],\n",
              "        [2.0800e+00, 4.3264e+00],\n",
              "        [2.1000e+00, 4.4100e+00],\n",
              "        [2.1200e+00, 4.4944e+00],\n",
              "        [2.1400e+00, 4.5796e+00],\n",
              "        [2.1600e+00, 4.6656e+00],\n",
              "        [2.1800e+00, 4.7524e+00],\n",
              "        [2.2000e+00, 4.8400e+00],\n",
              "        [2.2200e+00, 4.9284e+00],\n",
              "        [2.2400e+00, 5.0176e+00],\n",
              "        [2.2600e+00, 5.1076e+00],\n",
              "        [2.2800e+00, 5.1984e+00],\n",
              "        [2.3000e+00, 5.2900e+00],\n",
              "        [2.3200e+00, 5.3824e+00],\n",
              "        [2.3400e+00, 5.4756e+00],\n",
              "        [2.3600e+00, 5.5696e+00],\n",
              "        [2.3800e+00, 5.6644e+00],\n",
              "        [2.4000e+00, 5.7600e+00],\n",
              "        [2.4200e+00, 5.8564e+00],\n",
              "        [2.4400e+00, 5.9536e+00],\n",
              "        [2.4600e+00, 6.0516e+00],\n",
              "        [2.4800e+00, 6.1504e+00],\n",
              "        [2.5000e+00, 6.2500e+00],\n",
              "        [2.5200e+00, 6.3504e+00],\n",
              "        [2.5400e+00, 6.4516e+00],\n",
              "        [2.5600e+00, 6.5536e+00],\n",
              "        [2.5800e+00, 6.6564e+00],\n",
              "        [2.6000e+00, 6.7600e+00],\n",
              "        [2.6200e+00, 6.8644e+00],\n",
              "        [2.6400e+00, 6.9696e+00],\n",
              "        [2.6600e+00, 7.0756e+00],\n",
              "        [2.6800e+00, 7.1824e+00],\n",
              "        [2.7000e+00, 7.2900e+00],\n",
              "        [2.7200e+00, 7.3984e+00],\n",
              "        [2.7400e+00, 7.5076e+00],\n",
              "        [2.7600e+00, 7.6176e+00],\n",
              "        [2.7800e+00, 7.7284e+00],\n",
              "        [2.8000e+00, 7.8400e+00],\n",
              "        [2.8200e+00, 7.9524e+00],\n",
              "        [2.8400e+00, 8.0656e+00],\n",
              "        [2.8600e+00, 8.1796e+00],\n",
              "        [2.8800e+00, 8.2944e+00],\n",
              "        [2.9000e+00, 8.4100e+00],\n",
              "        [2.9200e+00, 8.5264e+00],\n",
              "        [2.9400e+00, 8.6436e+00],\n",
              "        [2.9600e+00, 8.7616e+00],\n",
              "        [2.9800e+00, 8.8804e+00],\n",
              "        [3.0000e+00, 9.0000e+00],\n",
              "        [3.0200e+00, 9.1204e+00],\n",
              "        [3.0400e+00, 9.2416e+00],\n",
              "        [3.0600e+00, 9.3636e+00],\n",
              "        [3.0800e+00, 9.4864e+00],\n",
              "        [3.1000e+00, 9.6100e+00],\n",
              "        [3.1200e+00, 9.7344e+00],\n",
              "        [3.1400e+00, 9.8596e+00],\n",
              "        [3.1600e+00, 9.9856e+00],\n",
              "        [3.1800e+00, 1.0112e+01],\n",
              "        [3.2000e+00, 1.0240e+01],\n",
              "        [3.2200e+00, 1.0368e+01],\n",
              "        [3.2400e+00, 1.0498e+01],\n",
              "        [3.2600e+00, 1.0628e+01],\n",
              "        [3.2800e+00, 1.0758e+01],\n",
              "        [3.3000e+00, 1.0890e+01],\n",
              "        [3.3200e+00, 1.1022e+01],\n",
              "        [3.3400e+00, 1.1156e+01],\n",
              "        [3.3600e+00, 1.1290e+01],\n",
              "        [3.3800e+00, 1.1424e+01],\n",
              "        [3.4000e+00, 1.1560e+01],\n",
              "        [3.4200e+00, 1.1696e+01],\n",
              "        [3.4400e+00, 1.1834e+01],\n",
              "        [3.4600e+00, 1.1972e+01],\n",
              "        [3.4800e+00, 1.2110e+01],\n",
              "        [3.5000e+00, 1.2250e+01],\n",
              "        [3.5200e+00, 1.2390e+01],\n",
              "        [3.5400e+00, 1.2532e+01],\n",
              "        [3.5600e+00, 1.2674e+01],\n",
              "        [3.5800e+00, 1.2816e+01],\n",
              "        [3.6000e+00, 1.2960e+01],\n",
              "        [3.6200e+00, 1.3104e+01],\n",
              "        [3.6400e+00, 1.3250e+01],\n",
              "        [3.6600e+00, 1.3396e+01],\n",
              "        [3.6800e+00, 1.3542e+01],\n",
              "        [3.7000e+00, 1.3690e+01],\n",
              "        [3.7200e+00, 1.3838e+01],\n",
              "        [3.7400e+00, 1.3988e+01],\n",
              "        [3.7600e+00, 1.4138e+01],\n",
              "        [3.7800e+00, 1.4288e+01],\n",
              "        [3.8000e+00, 1.4440e+01],\n",
              "        [3.8200e+00, 1.4592e+01],\n",
              "        [3.8400e+00, 1.4746e+01],\n",
              "        [3.8600e+00, 1.4900e+01],\n",
              "        [3.8800e+00, 1.5054e+01],\n",
              "        [3.9000e+00, 1.5210e+01],\n",
              "        [3.9200e+00, 1.5366e+01],\n",
              "        [3.9400e+00, 1.5524e+01],\n",
              "        [3.9600e+00, 1.5682e+01],\n",
              "        [3.9800e+00, 1.5840e+01]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Splitting data in train and test\n",
        "train_split = int(0.8 * len(X))\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEso2fRtkVGV",
        "outputId": "010d55dc-8e97-4c8d-9fd9-292c7461f4d9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160, 160, 40, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(train_data = X_train[:, 0],\n",
        "                     train_labels=y_train, \n",
        "                     test_data=X_test[:, 0],\n",
        "                     test_labels=y_test, predictions=None);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "j7e_lo_umzzI",
        "outputId": "add588b9-24a2-4ceb-edee-2257b8848f3c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegessionModel1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear_layer = nn.Linear(in_features=2, out_features=1)\n",
        "\n",
        "  def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "    return self.linear_layer(x)"
      ],
      "metadata": {
        "id": "OvbtuTm-ndhm"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_1 = LinearRegessionModel1()\n",
        "model_1, model_1.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dMQ3ZANozpl",
        "outputId": "877103f3-d2bd-4b26-e2f8-b33b7b48fa6e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(LinearRegessionModel1(\n",
              "   (linear_layer): Linear(in_features=2, out_features=1, bias=True)\n",
              " ),\n",
              " OrderedDict([('linear_layer.weight', tensor([[0.5406, 0.5869]])),\n",
              "              ('linear_layer.bias', tensor([-0.1657]))]))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdiK9LMEqRVK",
        "outputId": "514f2579-e260-4d64-e040-24b85a19e867"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not\n",
        "next(model_1.parameters()).device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mke9tVWcqWs4",
        "outputId": "2a4239ad-54fa-44aa-b0ea-6894b8674a59"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.L1Loss()\n",
        "optimizer_fn = torch.optim.Adam(params=model_1.parameters())"
      ],
      "metadata": {
        "id": "IG4HSPSzqce8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1(X_test).squeeze()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "8wsSMyE_AW15",
        "outputId": "5e78533d-38f9-4b76-fcf2-3ab33f81087a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-cf8e5520c7d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-ccf61d62c72f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "epochs = 1000\n",
        "\n",
        "X_train = X_train.to(device)\n",
        "X_test = X_test.to(device)\n",
        "y_train = y_train.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "epoch_count=[]\n",
        "loss_values=[]\n",
        "test_loss_values = []\n",
        "\n",
        "\n",
        "### Training\n",
        "#0. loop through the data\n",
        "for epoch in range(epochs):\n",
        "  model_1.train()\n",
        "\n",
        "  #1. forward\n",
        "  y_pred = model_1(X_train).squeeze()\n",
        "  #2. calc loss\n",
        "  \n",
        "  loss = loss_fn(y_pred, y_train)\n",
        "  #3. optimizer zero grad\n",
        "  optimizer_fn.zero_grad()\n",
        "  #4. Backpropagation\n",
        "  loss.backward()\n",
        "  #5. Step optimizer\n",
        "  optimizer_fn.step()\n",
        "  #Testing\n",
        "  model_1.eval()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    test_pred = model_1(X_test).squeeze()\n",
        "    test_loss=loss_fn(test_pred, y_test)\n",
        "  print(f\"Epoch: {epoch :<3} | Loss: {loss:<25} | Test loss: {test_loss}\")\n",
        "\n",
        "  epoch_count.append(epoch)\n",
        "  loss_values.append(loss)\n",
        "  test_loss_values.append(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JzaXsEQqvZe",
        "outputId": "f703e24d-941e-4b56-9db2-fbb43959131a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0   | Loss: 0.049928080290555954      | Test loss: 0.1875036209821701\n",
            "Epoch: 1   | Loss: 0.049886785447597504      | Test loss: 0.18731307983398438\n",
            "Epoch: 2   | Loss: 0.04984532296657562       | Test loss: 0.18709497153759003\n",
            "Epoch: 3   | Loss: 0.049803927540779114      | Test loss: 0.18695631623268127\n",
            "Epoch: 4   | Loss: 0.049762383103370667      | Test loss: 0.1867837905883789\n",
            "Epoch: 5   | Loss: 0.049721017479896545      | Test loss: 0.18671122193336487\n",
            "Epoch: 6   | Loss: 0.04967993125319481       | Test loss: 0.18659888207912445\n",
            "Epoch: 7   | Loss: 0.04963841661810875       | Test loss: 0.18645010888576508\n",
            "Epoch: 8   | Loss: 0.04959680512547493       | Test loss: 0.18626832962036133\n",
            "Epoch: 9   | Loss: 0.04955531284213066       | Test loss: 0.18608208000659943\n",
            "Epoch: 10  | Loss: 0.04951367899775505       | Test loss: 0.18589544296264648\n",
            "Epoch: 11  | Loss: 0.049472302198410034      | Test loss: 0.18570351600646973\n",
            "Epoch: 12  | Loss: 0.049430880695581436      | Test loss: 0.18561415374279022\n",
            "Epoch: 13  | Loss: 0.04938911646604538       | Test loss: 0.18551169335842133\n",
            "Epoch: 14  | Loss: 0.04934747517108917       | Test loss: 0.18539538979530334\n",
            "Epoch: 15  | Loss: 0.04930579662322998       | Test loss: 0.18524514138698578\n",
            "Epoch: 16  | Loss: 0.049264177680015564      | Test loss: 0.18506084382534027\n",
            "Epoch: 17  | Loss: 0.04922257363796234       | Test loss: 0.1848505586385727\n",
            "Epoch: 18  | Loss: 0.04918091371655464       | Test loss: 0.1846359819173813\n",
            "Epoch: 19  | Loss: 0.0491391196846962        | Test loss: 0.18442106246948242\n",
            "Epoch: 20  | Loss: 0.049097638577222824      | Test loss: 0.18431197106838226\n",
            "Epoch: 21  | Loss: 0.04905562102794647       | Test loss: 0.18419142067432404\n",
            "Epoch: 22  | Loss: 0.04901404306292534       | Test loss: 0.1840350180864334\n",
            "Epoch: 23  | Loss: 0.04897192493081093       | Test loss: 0.1838451474905014\n",
            "Epoch: 24  | Loss: 0.048930179327726364      | Test loss: 0.18362627923488617\n",
            "Epoch: 25  | Loss: 0.048888612538576126      | Test loss: 0.18351507186889648\n",
            "Epoch: 26  | Loss: 0.04884651303291321       | Test loss: 0.1834454983472824\n",
            "Epoch: 27  | Loss: 0.04880498722195625       | Test loss: 0.18333430588245392\n",
            "Epoch: 28  | Loss: 0.048763152211904526      | Test loss: 0.18318720161914825\n",
            "Epoch: 29  | Loss: 0.04872111976146698       | Test loss: 0.18300409615039825\n",
            "Epoch: 30  | Loss: 0.04867914691567421       | Test loss: 0.18279366195201874\n",
            "Epoch: 31  | Loss: 0.04863707348704338       | Test loss: 0.18255400657653809\n",
            "Epoch: 32  | Loss: 0.04859500750899315       | Test loss: 0.18242454528808594\n",
            "Epoch: 33  | Loss: 0.0485532283782959        | Test loss: 0.18233847618103027\n",
            "Epoch: 34  | Loss: 0.04851119592785835       | Test loss: 0.18223844468593597\n",
            "Epoch: 35  | Loss: 0.04846910014748573       | Test loss: 0.1820996254682541\n",
            "Epoch: 36  | Loss: 0.04842707887291908       | Test loss: 0.18192791938781738\n",
            "Epoch: 37  | Loss: 0.04838475212454796       | Test loss: 0.18172378838062286\n",
            "Epoch: 38  | Loss: 0.04834295064210892       | Test loss: 0.1815166026353836\n",
            "Epoch: 39  | Loss: 0.04830082878470421       | Test loss: 0.18130922317504883\n",
            "Epoch: 40  | Loss: 0.048258762806653976      | Test loss: 0.18120746314525604\n",
            "Epoch: 41  | Loss: 0.04821639880537987       | Test loss: 0.18109212815761566\n",
            "Epoch: 42  | Loss: 0.0481741763651371        | Test loss: 0.18096843361854553\n",
            "Epoch: 43  | Loss: 0.048132091760635376      | Test loss: 0.18080654740333557\n",
            "Epoch: 44  | Loss: 0.04808974266052246       | Test loss: 0.1806115210056305\n",
            "Epoch: 45  | Loss: 0.0480472594499588        | Test loss: 0.18041376769542694\n",
            "Epoch: 46  | Loss: 0.048005081713199615      | Test loss: 0.180214986205101\n",
            "Epoch: 47  | Loss: 0.047962889075279236      | Test loss: 0.1800117939710617\n",
            "Epoch: 48  | Loss: 0.04792075976729393       | Test loss: 0.179914191365242\n",
            "Epoch: 49  | Loss: 0.04787818714976311       | Test loss: 0.179779052734375\n",
            "Epoch: 50  | Loss: 0.04783599078655243       | Test loss: 0.17960834503173828\n",
            "Epoch: 51  | Loss: 0.04779355973005295       | Test loss: 0.17940421402454376\n",
            "Epoch: 52  | Loss: 0.047750890254974365      | Test loss: 0.17917399108409882\n",
            "Epoch: 53  | Loss: 0.047708988189697266      | Test loss: 0.1790504902601242\n",
            "Epoch: 54  | Loss: 0.04766617342829704       | Test loss: 0.17902636528015137\n",
            "Epoch: 55  | Loss: 0.04762399569153786       | Test loss: 0.17895542085170746\n",
            "Epoch: 56  | Loss: 0.04758138582110405       | Test loss: 0.17884159088134766\n",
            "Epoch: 57  | Loss: 0.0475391261279583        | Test loss: 0.17868967354297638\n",
            "Epoch: 58  | Loss: 0.04749659076333046       | Test loss: 0.1785055696964264\n",
            "Epoch: 59  | Loss: 0.04745408892631531       | Test loss: 0.178289994597435\n",
            "Epoch: 60  | Loss: 0.04741114750504494       | Test loss: 0.17804685235023499\n",
            "Epoch: 61  | Loss: 0.04736846685409546       | Test loss: 0.177805095911026\n",
            "Epoch: 62  | Loss: 0.047325752675533295      | Test loss: 0.17767415940761566\n",
            "Epoch: 63  | Loss: 0.04728330299258232       | Test loss: 0.17753271758556366\n",
            "Epoch: 64  | Loss: 0.0472407229244709        | Test loss: 0.1773831844329834\n",
            "Epoch: 65  | Loss: 0.04719792678952217       | Test loss: 0.1772243082523346\n",
            "Epoch: 66  | Loss: 0.04715513437986374       | Test loss: 0.17706046998500824\n",
            "Epoch: 67  | Loss: 0.047112513333559036      | Test loss: 0.17688994109630585\n",
            "Epoch: 68  | Loss: 0.04706977680325508       | Test loss: 0.17671170830726624\n",
            "Epoch: 69  | Loss: 0.04702680930495262       | Test loss: 0.17652840912342072\n",
            "Epoch: 70  | Loss: 0.04698416590690613       | Test loss: 0.1763441562652588\n",
            "Epoch: 71  | Loss: 0.04694129899144173       | Test loss: 0.1762342005968094\n",
            "Epoch: 72  | Loss: 0.046898480504751205      | Test loss: 0.17608729004859924\n",
            "Epoch: 73  | Loss: 0.04685581848025322       | Test loss: 0.17590661346912384\n",
            "Epoch: 74  | Loss: 0.046812739223241806      | Test loss: 0.17569322884082794\n",
            "Epoch: 75  | Loss: 0.04676975682377815       | Test loss: 0.175506591796875\n",
            "Epoch: 76  | Loss: 0.04672693833708763       | Test loss: 0.17539934813976288\n",
            "Epoch: 77  | Loss: 0.046683914959430695      | Test loss: 0.17525258660316467\n",
            "Epoch: 78  | Loss: 0.04664093255996704       | Test loss: 0.17507128417491913\n",
            "Epoch: 79  | Loss: 0.046597838401794434      | Test loss: 0.174885556101799\n",
            "Epoch: 80  | Loss: 0.046554937958717346      | Test loss: 0.1748034954071045\n",
            "Epoch: 81  | Loss: 0.046511854976415634      | Test loss: 0.1746828556060791\n",
            "Epoch: 82  | Loss: 0.04646888002753258       | Test loss: 0.1745213121175766\n",
            "Epoch: 83  | Loss: 0.04642575606703758       | Test loss: 0.17432908713817596\n",
            "Epoch: 84  | Loss: 0.04638286679983139       | Test loss: 0.1741313487291336\n",
            "Epoch: 85  | Loss: 0.04633958265185356       | Test loss: 0.17393045127391815\n",
            "Epoch: 86  | Loss: 0.0462968535721302        | Test loss: 0.17383646965026855\n",
            "Epoch: 87  | Loss: 0.04625337943434715       | Test loss: 0.1737288534641266\n",
            "Epoch: 88  | Loss: 0.04621027037501335       | Test loss: 0.1736106425523758\n",
            "Epoch: 89  | Loss: 0.046166978776454926      | Test loss: 0.17345361411571503\n",
            "Epoch: 90  | Loss: 0.046123843640089035      | Test loss: 0.17326174676418304\n",
            "Epoch: 91  | Loss: 0.04608048498630524       | Test loss: 0.17303982377052307\n",
            "Epoch: 92  | Loss: 0.04603734612464905       | Test loss: 0.17281484603881836\n",
            "Epoch: 93  | Loss: 0.04599393159151077       | Test loss: 0.17270159721374512\n",
            "Epoch: 94  | Loss: 0.04595056548714638       | Test loss: 0.17257729172706604\n",
            "Epoch: 95  | Loss: 0.04590745270252228       | Test loss: 0.1724393367767334\n",
            "Epoch: 96  | Loss: 0.045864131301641464      | Test loss: 0.17226620018482208\n",
            "Epoch: 97  | Loss: 0.04582055285573006       | Test loss: 0.1720602959394455\n",
            "Epoch: 98  | Loss: 0.04577699676156044       | Test loss: 0.1718519777059555\n",
            "Epoch: 99  | Loss: 0.04573358595371246       | Test loss: 0.17163978517055511\n",
            "Epoch: 100 | Loss: 0.04569029435515404       | Test loss: 0.17153750360012054\n",
            "Epoch: 101 | Loss: 0.04564671218395233       | Test loss: 0.17139534652233124\n",
            "Epoch: 102 | Loss: 0.04560336098074913       | Test loss: 0.17121677100658417\n",
            "Epoch: 103 | Loss: 0.04555979743599892       | Test loss: 0.17100535333156586\n",
            "Epoch: 104 | Loss: 0.04551617428660393       | Test loss: 0.17078113555908203\n",
            "Epoch: 105 | Loss: 0.045472852885723114      | Test loss: 0.17066459357738495\n",
            "Epoch: 106 | Loss: 0.045429158955812454      | Test loss: 0.17064796388149261\n",
            "Epoch: 107 | Loss: 0.04538562893867493       | Test loss: 0.17058253288269043\n",
            "Epoch: 108 | Loss: 0.04534250497817993       | Test loss: 0.17047277092933655\n",
            "Epoch: 109 | Loss: 0.045298703014850616      | Test loss: 0.1703256219625473\n",
            "Epoch: 110 | Loss: 0.04525528848171234       | Test loss: 0.17014136910438538\n",
            "Epoch: 111 | Loss: 0.04521157592535019       | Test loss: 0.16992540657520294\n",
            "Epoch: 112 | Loss: 0.04516773670911789       | Test loss: 0.16968078911304474\n",
            "Epoch: 113 | Loss: 0.045123759657144547      | Test loss: 0.1694089025259018\n",
            "Epoch: 114 | Loss: 0.045080043375492096      | Test loss: 0.16914062201976776\n",
            "Epoch: 115 | Loss: 0.04503659904003143       | Test loss: 0.1689898520708084\n",
            "Epoch: 116 | Loss: 0.0449928380548954        | Test loss: 0.16893883049488068\n",
            "Epoch: 117 | Loss: 0.04494849964976311       | Test loss: 0.16887202858924866\n",
            "Epoch: 118 | Loss: 0.044904716312885284      | Test loss: 0.1687873899936676\n",
            "Epoch: 119 | Loss: 0.0448610894382           | Test loss: 0.16865992546081543\n",
            "Epoch: 120 | Loss: 0.044817108660936356      | Test loss: 0.1684952825307846\n",
            "Epoch: 121 | Loss: 0.044773414731025696      | Test loss: 0.1682971566915512\n",
            "Epoch: 122 | Loss: 0.04472915455698967       | Test loss: 0.16809383034706116\n",
            "Epoch: 123 | Loss: 0.0446852408349514        | Test loss: 0.167886883020401\n",
            "Epoch: 124 | Loss: 0.04464138671755791       | Test loss: 0.16767846047878265\n",
            "Epoch: 125 | Loss: 0.04459722340106964       | Test loss: 0.1674659252166748\n",
            "Epoch: 126 | Loss: 0.044553257524967194      | Test loss: 0.16725173592567444\n",
            "Epoch: 127 | Loss: 0.044509369879961014      | Test loss: 0.16703586280345917\n",
            "Epoch: 128 | Loss: 0.044465478509664536      | Test loss: 0.16692934930324554\n",
            "Epoch: 129 | Loss: 0.04442096874117851       | Test loss: 0.16680875420570374\n",
            "Epoch: 130 | Loss: 0.04437684267759323       | Test loss: 0.16665206849575043\n",
            "Epoch: 131 | Loss: 0.044333018362522125      | Test loss: 0.16645793616771698\n",
            "Epoch: 132 | Loss: 0.0442885160446167        | Test loss: 0.16623282432556152\n",
            "Epoch: 133 | Loss: 0.044244419783353806      | Test loss: 0.16598005592823029\n",
            "Epoch: 134 | Loss: 0.04420073702931404       | Test loss: 0.16584014892578125\n",
            "Epoch: 135 | Loss: 0.04415612295269966       | Test loss: 0.16580381989479065\n",
            "Epoch: 136 | Loss: 0.04411178454756737       | Test loss: 0.16571927070617676\n",
            "Epoch: 137 | Loss: 0.044067706912755966      | Test loss: 0.16559357941150665\n",
            "Epoch: 138 | Loss: 0.044023722410202026      | Test loss: 0.16542716324329376\n",
            "Epoch: 139 | Loss: 0.043979205191135406      | Test loss: 0.16522721946239471\n",
            "Epoch: 140 | Loss: 0.043934766203165054      | Test loss: 0.16499438881874084\n",
            "Epoch: 141 | Loss: 0.04389037564396858       | Test loss: 0.16473375260829926\n",
            "Epoch: 142 | Loss: 0.043845903128385544      | Test loss: 0.16459093987941742\n",
            "Epoch: 143 | Loss: 0.04380158707499504       | Test loss: 0.16443638503551483\n",
            "Epoch: 144 | Loss: 0.043757304549217224      | Test loss: 0.16427412629127502\n",
            "Epoch: 145 | Loss: 0.04371267184615135       | Test loss: 0.16410546004772186\n",
            "Epoch: 146 | Loss: 0.043668340891599655      | Test loss: 0.16392889618873596\n",
            "Epoch: 147 | Loss: 0.04362379387021065       | Test loss: 0.16374531388282776\n",
            "Epoch: 148 | Loss: 0.04357930272817612       | Test loss: 0.16355624794960022\n",
            "Epoch: 149 | Loss: 0.04353460296988487       | Test loss: 0.16347412765026093\n",
            "Epoch: 150 | Loss: 0.04349031671881676       | Test loss: 0.163349911570549\n",
            "Epoch: 151 | Loss: 0.04344598203897476       | Test loss: 0.16318731009960175\n",
            "Epoch: 152 | Loss: 0.04340137168765068       | Test loss: 0.16298842430114746\n",
            "Epoch: 153 | Loss: 0.0433565154671669        | Test loss: 0.16275767982006073\n",
            "Epoch: 154 | Loss: 0.04331178590655327       | Test loss: 0.1624978631734848\n",
            "Epoch: 155 | Loss: 0.04326741024851799       | Test loss: 0.16235481202602386\n",
            "Epoch: 156 | Loss: 0.04322274029254913       | Test loss: 0.16231408715248108\n",
            "Epoch: 157 | Loss: 0.04317808523774147       | Test loss: 0.16222643852233887\n",
            "Epoch: 158 | Loss: 0.04313352331519127       | Test loss: 0.16209740936756134\n",
            "Epoch: 159 | Loss: 0.043088775128126144      | Test loss: 0.16192837059497833\n",
            "Epoch: 160 | Loss: 0.043043892830610275      | Test loss: 0.16172461211681366\n",
            "Epoch: 161 | Loss: 0.04299922287464142       | Test loss: 0.1614878624677658\n",
            "Epoch: 162 | Loss: 0.042954254895448685      | Test loss: 0.16122440993785858\n",
            "Epoch: 163 | Loss: 0.04290939122438431       | Test loss: 0.16107836365699768\n",
            "Epoch: 164 | Loss: 0.04286443814635277       | Test loss: 0.16092228889465332\n",
            "Epoch: 165 | Loss: 0.042819585651159286      | Test loss: 0.1607569307088852\n",
            "Epoch: 166 | Loss: 0.04277483746409416       | Test loss: 0.16058401763439178\n",
            "Epoch: 167 | Loss: 0.042729783803224564      | Test loss: 0.16040535271167755\n",
            "Epoch: 168 | Loss: 0.042684607207775116      | Test loss: 0.16021828353405\n",
            "Epoch: 169 | Loss: 0.04263980686664581       | Test loss: 0.16014228761196136\n",
            "Epoch: 170 | Loss: 0.04259485751390457       | Test loss: 0.1600511521100998\n",
            "Epoch: 171 | Loss: 0.04255016893148422       | Test loss: 0.15991583466529846\n",
            "Epoch: 172 | Loss: 0.04250509291887283       | Test loss: 0.1597404032945633\n",
            "Epoch: 173 | Loss: 0.042459890246391296      | Test loss: 0.1595296859741211\n",
            "Epoch: 174 | Loss: 0.04241476207971573       | Test loss: 0.15928998589515686\n",
            "Epoch: 175 | Loss: 0.042369645088911057      | Test loss: 0.15901890397071838\n",
            "Epoch: 176 | Loss: 0.042324360460042953      | Test loss: 0.15886831283569336\n",
            "Epoch: 177 | Loss: 0.04227907583117485       | Test loss: 0.15870586037635803\n",
            "Epoch: 178 | Loss: 0.04223385453224182       | Test loss: 0.1585378646850586\n",
            "Epoch: 179 | Loss: 0.042188860476017         | Test loss: 0.15836095809936523\n",
            "Epoch: 180 | Loss: 0.04214346036314964       | Test loss: 0.15817765891551971\n",
            "Epoch: 181 | Loss: 0.042098309844732285      | Test loss: 0.1579897403717041\n",
            "Epoch: 182 | Loss: 0.04205306991934776       | Test loss: 0.1578398197889328\n",
            "Epoch: 183 | Loss: 0.04200798273086548       | Test loss: 0.157650426030159\n",
            "Epoch: 184 | Loss: 0.04196256771683693       | Test loss: 0.1574290245771408\n",
            "Epoch: 185 | Loss: 0.04191745072603226       | Test loss: 0.15729112923145294\n",
            "Epoch: 186 | Loss: 0.04187190160155296       | Test loss: 0.15722985565662384\n",
            "Epoch: 187 | Loss: 0.04182671755552292       | Test loss: 0.15712423622608185\n",
            "Epoch: 188 | Loss: 0.04178156703710556       | Test loss: 0.15697526931762695\n",
            "Epoch: 189 | Loss: 0.04173622652888298       | Test loss: 0.15678906440734863\n",
            "Epoch: 190 | Loss: 0.04169034585356712       | Test loss: 0.15656690299510956\n",
            "Epoch: 191 | Loss: 0.041644852608442307      | Test loss: 0.15631742775440216\n",
            "Epoch: 192 | Loss: 0.04159921035170555       | Test loss: 0.1560664176940918\n",
            "Epoch: 193 | Loss: 0.041554391384124756      | Test loss: 0.15592999756336212\n",
            "Epoch: 194 | Loss: 0.04150877520442009       | Test loss: 0.1559021919965744\n",
            "Epoch: 195 | Loss: 0.041463084518909454      | Test loss: 0.15585055947303772\n",
            "Epoch: 196 | Loss: 0.04141738638281822       | Test loss: 0.15575122833251953\n",
            "Epoch: 197 | Loss: 0.041371818631887436      | Test loss: 0.15561147034168243\n",
            "Epoch: 198 | Loss: 0.04132616147398949       | Test loss: 0.15542998909950256\n",
            "Epoch: 199 | Loss: 0.04128086194396019       | Test loss: 0.1552143096923828\n",
            "Epoch: 200 | Loss: 0.041235171258449554      | Test loss: 0.15499725937843323\n",
            "Epoch: 201 | Loss: 0.041189346462488174      | Test loss: 0.15477566421031952\n",
            "Epoch: 202 | Loss: 0.0411437451839447        | Test loss: 0.15455122292041779\n",
            "Epoch: 203 | Loss: 0.04109761863946915       | Test loss: 0.1543262004852295\n",
            "Epoch: 204 | Loss: 0.04105202481150627       | Test loss: 0.15409718453884125\n",
            "Epoch: 205 | Loss: 0.04100629314780235       | Test loss: 0.1539836972951889\n",
            "Epoch: 206 | Loss: 0.040960412472486496      | Test loss: 0.15385861694812775\n",
            "Epoch: 207 | Loss: 0.04091450572013855       | Test loss: 0.15372014045715332\n",
            "Epoch: 208 | Loss: 0.04086862877011299       | Test loss: 0.15356865525245667\n",
            "Epoch: 209 | Loss: 0.040822796523571014      | Test loss: 0.15337935090065002\n",
            "Epoch: 210 | Loss: 0.04077687859535217       | Test loss: 0.15315747261047363\n",
            "Epoch: 211 | Loss: 0.040730807930231094      | Test loss: 0.15290336310863495\n",
            "Epoch: 212 | Loss: 0.04068490490317345       | Test loss: 0.15265069901943207\n",
            "Epoch: 213 | Loss: 0.04063904285430908       | Test loss: 0.15251508355140686\n",
            "Epoch: 214 | Loss: 0.04059293493628502       | Test loss: 0.15236759185791016\n",
            "Epoch: 215 | Loss: 0.040546733886003494      | Test loss: 0.15218153595924377\n",
            "Epoch: 216 | Loss: 0.04050106555223465       | Test loss: 0.15196004509925842\n",
            "Epoch: 217 | Loss: 0.04045494645833969       | Test loss: 0.15185432136058807\n",
            "Epoch: 218 | Loss: 0.040408700704574585      | Test loss: 0.1517057865858078\n",
            "Epoch: 219 | Loss: 0.04036282002925873       | Test loss: 0.15151730179786682\n",
            "Epoch: 220 | Loss: 0.04031666740775108       | Test loss: 0.15129528939723969\n",
            "Epoch: 221 | Loss: 0.04027026519179344       | Test loss: 0.15104055404663086\n",
            "Epoch: 222 | Loss: 0.04022477939724922       | Test loss: 0.15090452134609222\n",
            "Epoch: 223 | Loss: 0.04017828032374382       | Test loss: 0.1508762389421463\n",
            "Epoch: 224 | Loss: 0.04013187438249588       | Test loss: 0.1507953703403473\n",
            "Epoch: 225 | Loss: 0.040085747838020325      | Test loss: 0.15066857635974884\n",
            "Epoch: 226 | Loss: 0.0400397852063179        | Test loss: 0.1505039781332016\n",
            "Epoch: 227 | Loss: 0.03999347239732742       | Test loss: 0.15029840171337128\n",
            "Epoch: 228 | Loss: 0.039947010576725006      | Test loss: 0.1500624269247055\n",
            "Epoch: 229 | Loss: 0.03990064188838005       | Test loss: 0.14982271194458008\n",
            "Epoch: 230 | Loss: 0.03985430300235748       | Test loss: 0.1495833843946457\n",
            "Epoch: 231 | Loss: 0.03980788215994835       | Test loss: 0.149458646774292\n",
            "Epoch: 232 | Loss: 0.03976164385676384       | Test loss: 0.14932255446910858\n",
            "Epoch: 233 | Loss: 0.039715155959129333      | Test loss: 0.14917445182800293\n",
            "Epoch: 234 | Loss: 0.03966860845685005       | Test loss: 0.149016335606575\n",
            "Epoch: 235 | Loss: 0.039621975272893906      | Test loss: 0.14884987473487854\n",
            "Epoch: 236 | Loss: 0.03957566246390343       | Test loss: 0.14865894615650177\n",
            "Epoch: 237 | Loss: 0.03952914848923683       | Test loss: 0.14843320846557617\n",
            "Epoch: 238 | Loss: 0.03948236256837845       | Test loss: 0.14820341765880585\n",
            "Epoch: 239 | Loss: 0.03943610563874245       | Test loss: 0.14797262847423553\n",
            "Epoch: 240 | Loss: 0.039389368146657944      | Test loss: 0.14786028861999512\n",
            "Epoch: 241 | Loss: 0.039342980831861496      | Test loss: 0.14770403504371643\n",
            "Epoch: 242 | Loss: 0.03929641470313072       | Test loss: 0.14750848710536957\n",
            "Epoch: 243 | Loss: 0.039249587804079056      | Test loss: 0.14727579057216644\n",
            "Epoch: 244 | Loss: 0.03920263797044754       | Test loss: 0.14707517623901367\n",
            "Epoch: 245 | Loss: 0.03915640711784363       | Test loss: 0.14698806405067444\n",
            "Epoch: 246 | Loss: 0.03910950943827629       | Test loss: 0.14685530960559845\n",
            "Epoch: 247 | Loss: 0.03906288370490074       | Test loss: 0.1466812640428543\n",
            "Epoch: 248 | Loss: 0.039016105234622955      | Test loss: 0.1464691162109375\n",
            "Epoch: 249 | Loss: 0.038969170302152634      | Test loss: 0.14622282981872559\n",
            "Epoch: 250 | Loss: 0.03892265632748604       | Test loss: 0.14609651267528534\n",
            "Epoch: 251 | Loss: 0.03887554630637169       | Test loss: 0.1459580510854721\n",
            "Epoch: 252 | Loss: 0.03882872313261032       | Test loss: 0.14580808579921722\n",
            "Epoch: 253 | Loss: 0.03878176584839821       | Test loss: 0.14564691483974457\n",
            "Epoch: 254 | Loss: 0.03873470053076744       | Test loss: 0.14544664323329926\n",
            "Epoch: 255 | Loss: 0.038687970489263535      | Test loss: 0.1452416479587555\n",
            "Epoch: 256 | Loss: 0.03864077851176262       | Test loss: 0.1450311243534088\n",
            "Epoch: 257 | Loss: 0.038594041019678116      | Test loss: 0.14481692016124725\n",
            "Epoch: 258 | Loss: 0.03854713961482048       | Test loss: 0.1447191685438156\n",
            "Epoch: 259 | Loss: 0.03850008547306061       | Test loss: 0.1445765495300293\n",
            "Epoch: 260 | Loss: 0.03845314681529999       | Test loss: 0.14439068734645844\n",
            "Epoch: 261 | Loss: 0.03840602561831474       | Test loss: 0.14417128264904022\n",
            "Epoch: 262 | Loss: 0.03835894539952278       | Test loss: 0.14391742646694183\n",
            "Epoch: 263 | Loss: 0.03831150010228157       | Test loss: 0.14378438889980316\n",
            "Epoch: 264 | Loss: 0.038264740258455276      | Test loss: 0.14360962808132172\n",
            "Epoch: 265 | Loss: 0.0382172092795372        | Test loss: 0.14339704811573029\n",
            "Epoch: 266 | Loss: 0.038170214742422104      | Test loss: 0.14330001175403595\n",
            "Epoch: 267 | Loss: 0.03812301158905029       | Test loss: 0.14315854012966156\n",
            "Epoch: 268 | Loss: 0.038075800985097885      | Test loss: 0.14297643303871155\n",
            "Epoch: 269 | Loss: 0.03802848979830742       | Test loss: 0.1427583247423172\n",
            "Epoch: 270 | Loss: 0.03798135370016098       | Test loss: 0.14253249764442444\n",
            "Epoch: 271 | Loss: 0.03793403506278992       | Test loss: 0.14242887496948242\n",
            "Epoch: 272 | Loss: 0.037886835634708405      | Test loss: 0.14230771362781525\n",
            "Epoch: 273 | Loss: 0.037839412689208984      | Test loss: 0.14217305183410645\n",
            "Epoch: 274 | Loss: 0.03779209405183792       | Test loss: 0.1419953852891922\n",
            "Epoch: 275 | Loss: 0.037744708359241486      | Test loss: 0.14178085327148438\n",
            "Epoch: 276 | Loss: 0.037697140127420425      | Test loss: 0.1415628045797348\n",
            "Epoch: 277 | Loss: 0.03764982149004936       | Test loss: 0.14133992791175842\n",
            "Epoch: 278 | Loss: 0.03760233148932457       | Test loss: 0.14111338555812836\n",
            "Epoch: 279 | Loss: 0.03755471110343933       | Test loss: 0.14088550209999084\n",
            "Epoch: 280 | Loss: 0.03750750795006752       | Test loss: 0.14077363908290863\n",
            "Epoch: 281 | Loss: 0.037459637969732285      | Test loss: 0.1406482309103012\n",
            "Epoch: 282 | Loss: 0.03741234168410301       | Test loss: 0.14047871530056\n",
            "Epoch: 283 | Loss: 0.037364762276411057      | Test loss: 0.14027075469493866\n",
            "Epoch: 284 | Loss: 0.037317171692848206      | Test loss: 0.1400279551744461\n",
            "Epoch: 285 | Loss: 0.037269458174705505      | Test loss: 0.13975444436073303\n",
            "Epoch: 286 | Loss: 0.0372219942510128        | Test loss: 0.13960400223731995\n",
            "Epoch: 287 | Loss: 0.03717419132590294       | Test loss: 0.13956522941589355\n",
            "Epoch: 288 | Loss: 0.03712645173072815       | Test loss: 0.13947243988513947\n",
            "Epoch: 289 | Loss: 0.037078797817230225      | Test loss: 0.1393357813358307\n",
            "Epoch: 290 | Loss: 0.03703131899237633       | Test loss: 0.1391552984714508\n",
            "Epoch: 291 | Loss: 0.03698338568210602       | Test loss: 0.1389392912387848\n",
            "Epoch: 292 | Loss: 0.03693576902151108       | Test loss: 0.13868622481822968\n",
            "Epoch: 293 | Loss: 0.03688769415020943       | Test loss: 0.13840460777282715\n",
            "Epoch: 294 | Loss: 0.03683954104781151       | Test loss: 0.13824759423732758\n",
            "Epoch: 295 | Loss: 0.03679174929857254       | Test loss: 0.13813959062099457\n",
            "Epoch: 296 | Loss: 0.03674402832984924       | Test loss: 0.1380174607038498\n",
            "Epoch: 297 | Loss: 0.03669619932770729       | Test loss: 0.13787975907325745\n",
            "Epoch: 298 | Loss: 0.036648135632276535      | Test loss: 0.13770222663879395\n",
            "Epoch: 299 | Loss: 0.036600273102521896      | Test loss: 0.13748422265052795\n",
            "Epoch: 300 | Loss: 0.0365523099899292        | Test loss: 0.13726234436035156\n",
            "Epoch: 301 | Loss: 0.03650398552417755       | Test loss: 0.13703803718090057\n",
            "Epoch: 302 | Loss: 0.03645602613687515       | Test loss: 0.13680768013000488\n",
            "Epoch: 303 | Loss: 0.03640786558389664       | Test loss: 0.1366364061832428\n",
            "Epoch: 304 | Loss: 0.036359839141368866      | Test loss: 0.13645553588867188\n",
            "Epoch: 305 | Loss: 0.03631172701716423       | Test loss: 0.13626761734485626\n",
            "Epoch: 306 | Loss: 0.03626377508044243       | Test loss: 0.13607044517993927\n",
            "Epoch: 307 | Loss: 0.036215540021657944      | Test loss: 0.1358390897512436\n",
            "Epoch: 308 | Loss: 0.036167439073324203      | Test loss: 0.13569651544094086\n",
            "Epoch: 309 | Loss: 0.036119233816862106      | Test loss: 0.13550888001918793\n",
            "Epoch: 310 | Loss: 0.03607088699936867       | Test loss: 0.13528814911842346\n",
            "Epoch: 311 | Loss: 0.03602292761206627       | Test loss: 0.1351546347141266\n",
            "Epoch: 312 | Loss: 0.03597428649663925       | Test loss: 0.13497857749462128\n",
            "Epoch: 313 | Loss: 0.035926301032304764      | Test loss: 0.13476267457008362\n",
            "Epoch: 314 | Loss: 0.035877954214811325      | Test loss: 0.1346658319234848\n",
            "Epoch: 315 | Loss: 0.03582967072725296       | Test loss: 0.1345227211713791\n",
            "Epoch: 316 | Loss: 0.035781197249889374      | Test loss: 0.13433466851711273\n",
            "Epoch: 317 | Loss: 0.03573286533355713       | Test loss: 0.13414107263088226\n",
            "Epoch: 318 | Loss: 0.03568441793322563       | Test loss: 0.13393883407115936\n",
            "Epoch: 319 | Loss: 0.035635966807603836      | Test loss: 0.13373027741909027\n",
            "Epoch: 320 | Loss: 0.035587381571531296      | Test loss: 0.13351774215698242\n",
            "Epoch: 321 | Loss: 0.03553934395313263       | Test loss: 0.13342447578907013\n",
            "Epoch: 322 | Loss: 0.03549044951796532       | Test loss: 0.13331261277198792\n",
            "Epoch: 323 | Loss: 0.035442035645246506      | Test loss: 0.13315434753894806\n",
            "Epoch: 324 | Loss: 0.03539368137717247       | Test loss: 0.13295522332191467\n",
            "Epoch: 325 | Loss: 0.03534524515271187       | Test loss: 0.13272075355052948\n",
            "Epoch: 326 | Loss: 0.03529654070734978       | Test loss: 0.13245359063148499\n",
            "Epoch: 327 | Loss: 0.035247672349214554      | Test loss: 0.13218370079994202\n",
            "Epoch: 328 | Loss: 0.03519956022500992       | Test loss: 0.1320415586233139\n",
            "Epoch: 329 | Loss: 0.03515053167939186       | Test loss: 0.13200826942920685\n",
            "Epoch: 330 | Loss: 0.03510177507996559       | Test loss: 0.13192319869995117\n",
            "Epoch: 331 | Loss: 0.03505323454737663       | Test loss: 0.13178983330726624\n",
            "Epoch: 332 | Loss: 0.0350046381354332        | Test loss: 0.13161306083202362\n",
            "Epoch: 333 | Loss: 0.03495604917407036       | Test loss: 0.1313963383436203\n",
            "Epoch: 334 | Loss: 0.03490694984793663       | Test loss: 0.13114415109157562\n",
            "Epoch: 335 | Loss: 0.03485797345638275       | Test loss: 0.1308581382036209\n",
            "Epoch: 336 | Loss: 0.03480920195579529       | Test loss: 0.13057713210582733\n",
            "Epoch: 337 | Loss: 0.0347607284784317        | Test loss: 0.13041968643665314\n",
            "Epoch: 338 | Loss: 0.03471154347062111       | Test loss: 0.13037700951099396\n",
            "Epoch: 339 | Loss: 0.03466232493519783       | Test loss: 0.13031211495399475\n",
            "Epoch: 340 | Loss: 0.03461369127035141       | Test loss: 0.1301967203617096\n",
            "Epoch: 341 | Loss: 0.0345647931098938        | Test loss: 0.13003544509410858\n",
            "Epoch: 342 | Loss: 0.03451591357588768       | Test loss: 0.12983274459838867\n",
            "Epoch: 343 | Loss: 0.03446698188781738       | Test loss: 0.1295914649963379\n",
            "Epoch: 344 | Loss: 0.034417811781167984      | Test loss: 0.12934909760951996\n",
            "Epoch: 345 | Loss: 0.03436886519193649       | Test loss: 0.12910380959510803\n",
            "Epoch: 346 | Loss: 0.034319717437028885      | Test loss: 0.1288568526506424\n",
            "Epoch: 347 | Loss: 0.03427049145102501       | Test loss: 0.12860555946826935\n",
            "Epoch: 348 | Loss: 0.03422150760889053       | Test loss: 0.12847900390625\n",
            "Epoch: 349 | Loss: 0.03417224436998367       | Test loss: 0.12833762168884277\n",
            "Epoch: 350 | Loss: 0.03412311151623726       | Test loss: 0.1281837522983551\n",
            "Epoch: 351 | Loss: 0.0340736024081707        | Test loss: 0.12801742553710938\n",
            "Epoch: 352 | Loss: 0.03402423486113548       | Test loss: 0.12782727181911469\n",
            "Epoch: 353 | Loss: 0.03397534415125847       | Test loss: 0.12759704887866974\n",
            "Epoch: 354 | Loss: 0.03392612561583519       | Test loss: 0.1273309290409088\n",
            "Epoch: 355 | Loss: 0.03387634828686714       | Test loss: 0.12706589698791504\n",
            "Epoch: 356 | Loss: 0.033827412873506546      | Test loss: 0.12692594528198242\n",
            "Epoch: 357 | Loss: 0.03377770632505417       | Test loss: 0.12677335739135742\n",
            "Epoch: 358 | Loss: 0.03372862562537193       | Test loss: 0.12657585740089417\n",
            "Epoch: 359 | Loss: 0.03367907926440239       | Test loss: 0.12634168565273285\n",
            "Epoch: 360 | Loss: 0.033629756420850754      | Test loss: 0.12607207894325256\n",
            "Epoch: 361 | Loss: 0.033580634742975235      | Test loss: 0.12592950463294983\n",
            "Epoch: 362 | Loss: 0.033530768007040024      | Test loss: 0.12587080895900726\n",
            "Epoch: 363 | Loss: 0.033481474965810776      | Test loss: 0.12576031684875488\n",
            "Epoch: 364 | Loss: 0.033432167023420334      | Test loss: 0.1256018728017807\n",
            "Epoch: 365 | Loss: 0.03338247165083885       | Test loss: 0.12540321052074432\n",
            "Epoch: 366 | Loss: 0.033332984894514084      | Test loss: 0.12516164779663086\n",
            "Epoch: 367 | Loss: 0.033283036202192307      | Test loss: 0.12488770484924316\n",
            "Epoch: 368 | Loss: 0.03323337063193321       | Test loss: 0.12461543083190918\n",
            "Epoch: 369 | Loss: 0.03318461403250694       | Test loss: 0.12446746975183487\n",
            "Epoch: 370 | Loss: 0.03313441574573517       | Test loss: 0.1244349479675293\n",
            "Epoch: 371 | Loss: 0.033084671944379807      | Test loss: 0.12437806278467178\n",
            "Epoch: 372 | Loss: 0.033035002648830414      | Test loss: 0.12426938861608505\n",
            "Epoch: 373 | Loss: 0.032985713332891464      | Test loss: 0.12411437183618546\n",
            "Epoch: 374 | Loss: 0.03293593227863312       | Test loss: 0.12391366809606552\n",
            "Epoch: 375 | Loss: 0.03288600966334343       | Test loss: 0.1236761137843132\n",
            "Epoch: 376 | Loss: 0.03283592686057091       | Test loss: 0.12340302765369415\n",
            "Epoch: 377 | Loss: 0.03278600797057152       | Test loss: 0.12313046306371689\n",
            "Epoch: 378 | Loss: 0.03273648023605347       | Test loss: 0.12285647541284561\n",
            "Epoch: 379 | Loss: 0.03268641233444214       | Test loss: 0.1227114200592041\n",
            "Epoch: 380 | Loss: 0.032636385411024094      | Test loss: 0.12255401909351349\n",
            "Epoch: 381 | Loss: 0.03258674964308739       | Test loss: 0.12238416820764542\n",
            "Epoch: 382 | Loss: 0.03253650665283203       | Test loss: 0.12220320850610733\n",
            "Epoch: 383 | Loss: 0.03248675540089607       | Test loss: 0.12201237678527832\n",
            "Epoch: 384 | Loss: 0.032436713576316833      | Test loss: 0.12181434780359268\n",
            "Epoch: 385 | Loss: 0.03238658979535103       | Test loss: 0.12160768359899521\n",
            "Epoch: 386 | Loss: 0.03233661130070686       | Test loss: 0.12139449268579483\n",
            "Epoch: 387 | Loss: 0.03228619322180748       | Test loss: 0.12117262184619904\n",
            "Epoch: 388 | Loss: 0.03223608061671257       | Test loss: 0.12091626971960068\n",
            "Epoch: 389 | Loss: 0.03218604996800423       | Test loss: 0.12077007442712784\n",
            "Epoch: 390 | Loss: 0.03213592991232872       | Test loss: 0.12057981640100479\n",
            "Epoch: 391 | Loss: 0.032085832208395004      | Test loss: 0.12034893035888672\n",
            "Epoch: 392 | Loss: 0.032035645097494125      | Test loss: 0.12008114159107208\n",
            "Epoch: 393 | Loss: 0.03198568895459175       | Test loss: 0.11991047859191895\n",
            "Epoch: 394 | Loss: 0.031935177743434906      | Test loss: 0.1198272705078125\n",
            "Epoch: 395 | Loss: 0.031885188072919846      | Test loss: 0.11969456821680069\n",
            "Epoch: 396 | Loss: 0.031834762543439865      | Test loss: 0.11951422691345215\n",
            "Epoch: 397 | Loss: 0.031784333288669586      | Test loss: 0.11929398030042648\n",
            "Epoch: 398 | Loss: 0.03173403814435005       | Test loss: 0.11906518787145615\n",
            "Epoch: 399 | Loss: 0.03168357163667679       | Test loss: 0.11883335560560226\n",
            "Epoch: 400 | Loss: 0.03163337707519531       | Test loss: 0.11872396618127823\n",
            "Epoch: 401 | Loss: 0.03158275783061981       | Test loss: 0.11859893798828125\n",
            "Epoch: 402 | Loss: 0.0315321646630764        | Test loss: 0.11845922470092773\n",
            "Epoch: 403 | Loss: 0.031481917947530746      | Test loss: 0.11827397346496582\n",
            "Epoch: 404 | Loss: 0.03143160045146942       | Test loss: 0.11804652214050293\n",
            "Epoch: 405 | Loss: 0.03138081729412079       | Test loss: 0.11778483539819717\n",
            "Epoch: 406 | Loss: 0.03133048117160797       | Test loss: 0.11751856654882431\n",
            "Epoch: 407 | Loss: 0.03127969801425934       | Test loss: 0.11738152801990509\n",
            "Epoch: 408 | Loss: 0.031229186803102493      | Test loss: 0.11723103374242783\n",
            "Epoch: 409 | Loss: 0.031178539618849754      | Test loss: 0.11706676334142685\n",
            "Epoch: 410 | Loss: 0.031127939000725746      | Test loss: 0.11685986816883087\n",
            "Epoch: 411 | Loss: 0.031077200546860695      | Test loss: 0.11661338806152344\n",
            "Epoch: 412 | Loss: 0.031026480719447136      | Test loss: 0.11636185646057129\n",
            "Epoch: 413 | Loss: 0.030975913628935814      | Test loss: 0.11624088138341904\n",
            "Epoch: 414 | Loss: 0.03092503547668457       | Test loss: 0.11610203236341476\n",
            "Epoch: 415 | Loss: 0.030874570831656456      | Test loss: 0.11591611057519913\n",
            "Epoch: 416 | Loss: 0.03082370199263096       | Test loss: 0.11568975448608398\n",
            "Epoch: 417 | Loss: 0.030772924423217773      | Test loss: 0.1154259666800499\n",
            "Epoch: 418 | Loss: 0.03072165511548519       | Test loss: 0.11522622406482697\n",
            "Epoch: 419 | Loss: 0.030670836567878723      | Test loss: 0.1151484027504921\n",
            "Epoch: 420 | Loss: 0.030620193108916283      | Test loss: 0.11501765251159668\n",
            "Epoch: 421 | Loss: 0.03056940995156765       | Test loss: 0.11484098434448242\n",
            "Epoch: 422 | Loss: 0.03051871620118618       | Test loss: 0.11462336033582687\n",
            "Epoch: 423 | Loss: 0.030467564240098         | Test loss: 0.11436562985181808\n",
            "Epoch: 424 | Loss: 0.030416471883654594      | Test loss: 0.11407452076673508\n",
            "Epoch: 425 | Loss: 0.03036569617688656       | Test loss: 0.1139138713479042\n",
            "Epoch: 426 | Loss: 0.030314505100250244      | Test loss: 0.1138063445687294\n",
            "Epoch: 427 | Loss: 0.030263284221291542      | Test loss: 0.11368241161108017\n",
            "Epoch: 428 | Loss: 0.03021230176091194       | Test loss: 0.11351098865270615\n",
            "Epoch: 429 | Loss: 0.03016136959195137       | Test loss: 0.11329507827758789\n",
            "Epoch: 430 | Loss: 0.030110040679574013      | Test loss: 0.11307354271411896\n",
            "Epoch: 431 | Loss: 0.030058855190873146      | Test loss: 0.11284255981445312\n",
            "Epoch: 432 | Loss: 0.030007654801011086      | Test loss: 0.11260924488306046\n",
            "Epoch: 433 | Loss: 0.029956579208374023      | Test loss: 0.11236710846424103\n",
            "Epoch: 434 | Loss: 0.029905280098319054      | Test loss: 0.11225571483373642\n",
            "Epoch: 435 | Loss: 0.029853805899620056      | Test loss: 0.11212539672851562\n",
            "Epoch: 436 | Loss: 0.029802752658724785      | Test loss: 0.11194858700037003\n",
            "Epoch: 437 | Loss: 0.02975142002105713       | Test loss: 0.11172914505004883\n",
            "Epoch: 438 | Loss: 0.02970031276345253       | Test loss: 0.11147203296422958\n",
            "Epoch: 439 | Loss: 0.02964850142598152       | Test loss: 0.11117754131555557\n",
            "Epoch: 440 | Loss: 0.029597420245409012      | Test loss: 0.11101584881544113\n",
            "Epoch: 441 | Loss: 0.029545998200774193      | Test loss: 0.1108427569270134\n",
            "Epoch: 442 | Loss: 0.02949448488652706       | Test loss: 0.11065793037414551\n",
            "Epoch: 443 | Loss: 0.02944287098944187       | Test loss: 0.11043000221252441\n",
            "Epoch: 444 | Loss: 0.029391301795840263      | Test loss: 0.11019911617040634\n",
            "Epoch: 445 | Loss: 0.029339993372559547      | Test loss: 0.11009178310632706\n",
            "Epoch: 446 | Loss: 0.02928866818547249       | Test loss: 0.10993509739637375\n",
            "Epoch: 447 | Loss: 0.029236987233161926      | Test loss: 0.10973405838012695\n",
            "Epoch: 448 | Loss: 0.029185457155108452      | Test loss: 0.1094910129904747\n",
            "Epoch: 449 | Loss: 0.029133854433894157      | Test loss: 0.10921154171228409\n",
            "Epoch: 450 | Loss: 0.029082421213388443      | Test loss: 0.10906529426574707\n",
            "Epoch: 451 | Loss: 0.029030269011855125      | Test loss: 0.10895257443189621\n",
            "Epoch: 452 | Loss: 0.02897883765399456       | Test loss: 0.10879049450159073\n",
            "Epoch: 453 | Loss: 0.028926992788910866      | Test loss: 0.10858416557312012\n",
            "Epoch: 454 | Loss: 0.028875267133116722      | Test loss: 0.10833907127380371\n",
            "Epoch: 455 | Loss: 0.02882337011396885       | Test loss: 0.10808859020471573\n",
            "Epoch: 456 | Loss: 0.02877199649810791       | Test loss: 0.10796771198511124\n",
            "Epoch: 457 | Loss: 0.028720123693346977      | Test loss: 0.10782895237207413\n",
            "Epoch: 458 | Loss: 0.028668124228715897      | Test loss: 0.1076740249991417\n",
            "Epoch: 459 | Loss: 0.028616508468985558      | Test loss: 0.10747623443603516\n",
            "Epoch: 460 | Loss: 0.028564566746354103      | Test loss: 0.10723390430212021\n",
            "Epoch: 461 | Loss: 0.02851264551281929       | Test loss: 0.10698805004358292\n",
            "Epoch: 462 | Loss: 0.02846040204167366       | Test loss: 0.10673947632312775\n",
            "Epoch: 463 | Loss: 0.028408626094460487      | Test loss: 0.10661983489990234\n",
            "Epoch: 464 | Loss: 0.028356552124023438      | Test loss: 0.10648055374622345\n",
            "Epoch: 465 | Loss: 0.02830452285706997       | Test loss: 0.10629548877477646\n",
            "Epoch: 466 | Loss: 0.028252607211470604      | Test loss: 0.10606775432825089\n",
            "Epoch: 467 | Loss: 0.028200602158904076      | Test loss: 0.10580072551965714\n",
            "Epoch: 468 | Loss: 0.028148293495178223      | Test loss: 0.10553117096424103\n",
            "Epoch: 469 | Loss: 0.028096526861190796      | Test loss: 0.10539264976978302\n",
            "Epoch: 470 | Loss: 0.028044182807207108      | Test loss: 0.10522332042455673\n",
            "Epoch: 471 | Loss: 0.02799190580844879       | Test loss: 0.10500822216272354\n",
            "Epoch: 472 | Loss: 0.02793962135910988       | Test loss: 0.10475368797779083\n",
            "Epoch: 473 | Loss: 0.027887511998414993      | Test loss: 0.10462722927331924\n",
            "Epoch: 474 | Loss: 0.0278353001922369        | Test loss: 0.10445456951856613\n",
            "Epoch: 475 | Loss: 0.027782827615737915      | Test loss: 0.1042366549372673\n",
            "Epoch: 476 | Loss: 0.027730807662010193      | Test loss: 0.10400962829589844\n",
            "Epoch: 477 | Loss: 0.027678368613123894      | Test loss: 0.10377778857946396\n",
            "Epoch: 478 | Loss: 0.027626115828752518      | Test loss: 0.10367465019226074\n",
            "Epoch: 479 | Loss: 0.027573812752962112      | Test loss: 0.1035190150141716\n",
            "Epoch: 480 | Loss: 0.027521435171365738      | Test loss: 0.10331721603870392\n",
            "Epoch: 481 | Loss: 0.027469182386994362      | Test loss: 0.10307440906763077\n",
            "Epoch: 482 | Loss: 0.027416696771979332      | Test loss: 0.10282588005065918\n",
            "Epoch: 483 | Loss: 0.027364259585738182      | Test loss: 0.10257196426391602\n",
            "Epoch: 484 | Loss: 0.027311861515045166      | Test loss: 0.10244970768690109\n",
            "Epoch: 485 | Loss: 0.02725920081138611       | Test loss: 0.1023111343383789\n",
            "Epoch: 486 | Loss: 0.027206391096115112      | Test loss: 0.1021239310503006\n",
            "Epoch: 487 | Loss: 0.02715401165187359       | Test loss: 0.10189147293567657\n",
            "Epoch: 488 | Loss: 0.027101218700408936      | Test loss: 0.1016230583190918\n",
            "Epoch: 489 | Loss: 0.027048815041780472      | Test loss: 0.10148511081933975\n",
            "Epoch: 490 | Loss: 0.026995915919542313      | Test loss: 0.10133085399866104\n",
            "Epoch: 491 | Loss: 0.02694343402981758       | Test loss: 0.10113158077001572\n",
            "Epoch: 492 | Loss: 0.026890799403190613      | Test loss: 0.10088854283094406\n",
            "Epoch: 493 | Loss: 0.026837890967726707      | Test loss: 0.10064110904932022\n",
            "Epoch: 494 | Loss: 0.02678518369793892       | Test loss: 0.1004556193947792\n",
            "Epoch: 495 | Loss: 0.026732295751571655      | Test loss: 0.10026078671216965\n",
            "Epoch: 496 | Loss: 0.026679743081331253      | Test loss: 0.10005249828100204\n",
            "Epoch: 497 | Loss: 0.026626694947481155      | Test loss: 0.09980487823486328\n",
            "Epoch: 498 | Loss: 0.026574095711112022      | Test loss: 0.09968681633472443\n",
            "Epoch: 499 | Loss: 0.026521241292357445      | Test loss: 0.0995182991027832\n",
            "Epoch: 500 | Loss: 0.02646845392882824       | Test loss: 0.09930329769849777\n",
            "Epoch: 501 | Loss: 0.02641541324555874       | Test loss: 0.09904823452234268\n",
            "Epoch: 502 | Loss: 0.026362484320998192      | Test loss: 0.09875641018152237\n",
            "Epoch: 503 | Loss: 0.026309818029403687      | Test loss: 0.0985986739397049\n",
            "Epoch: 504 | Loss: 0.02625664509832859       | Test loss: 0.09856343269348145\n",
            "Epoch: 505 | Loss: 0.02620370127260685       | Test loss: 0.09847006946802139\n",
            "Epoch: 506 | Loss: 0.026150772348046303      | Test loss: 0.0983215793967247\n",
            "Epoch: 507 | Loss: 0.026097968220710754      | Test loss: 0.0981261283159256\n",
            "Epoch: 508 | Loss: 0.026044566184282303      | Test loss: 0.09788656234741211\n",
            "Epoch: 509 | Loss: 0.025991518050432205      | Test loss: 0.09761057049036026\n",
            "Epoch: 510 | Loss: 0.0259380042552948        | Test loss: 0.0973304733633995\n",
            "Epoch: 511 | Loss: 0.025884946808218956      | Test loss: 0.09704699367284775\n",
            "Epoch: 512 | Loss: 0.025832081213593483      | Test loss: 0.09690123051404953\n",
            "Epoch: 513 | Loss: 0.025778135284781456      | Test loss: 0.09673815220594406\n",
            "Epoch: 514 | Loss: 0.02572506107389927       | Test loss: 0.09656091034412384\n",
            "Epoch: 515 | Loss: 0.02567191794514656       | Test loss: 0.09637361019849777\n",
            "Epoch: 516 | Loss: 0.02561863698065281       | Test loss: 0.09617271274328232\n",
            "Epoch: 517 | Loss: 0.025565166026353836      | Test loss: 0.09596361964941025\n",
            "Epoch: 518 | Loss: 0.025511616840958595      | Test loss: 0.09571085125207901\n",
            "Epoch: 519 | Loss: 0.025458205491304398      | Test loss: 0.09543676674365997\n",
            "Epoch: 520 | Loss: 0.02540486864745617       | Test loss: 0.09529709815979004\n",
            "Epoch: 521 | Loss: 0.025351423770189285      | Test loss: 0.09510884433984756\n",
            "Epoch: 522 | Loss: 0.025298047810792923      | Test loss: 0.09487590938806534\n",
            "Epoch: 523 | Loss: 0.02524462342262268       | Test loss: 0.09460177272558212\n",
            "Epoch: 524 | Loss: 0.02519129030406475       | Test loss: 0.0944644957780838\n",
            "Epoch: 525 | Loss: 0.025137627497315407      | Test loss: 0.09427399933338165\n",
            "Epoch: 526 | Loss: 0.025083793327212334      | Test loss: 0.09404263645410538\n",
            "Epoch: 527 | Loss: 0.025030428543686867      | Test loss: 0.09380202740430832\n",
            "Epoch: 528 | Loss: 0.024977004155516624      | Test loss: 0.09369444847106934\n",
            "Epoch: 529 | Loss: 0.024923175573349         | Test loss: 0.09353270381689072\n",
            "Epoch: 530 | Loss: 0.02486967295408249       | Test loss: 0.09332447499036789\n",
            "Epoch: 531 | Loss: 0.02481580339372158       | Test loss: 0.09307374805212021\n",
            "Epoch: 532 | Loss: 0.024761982262134552      | Test loss: 0.09281568974256516\n",
            "Epoch: 533 | Loss: 0.02470853179693222       | Test loss: 0.09269137680530548\n",
            "Epoch: 534 | Loss: 0.024654446169734         | Test loss: 0.09255176037549973\n",
            "Epoch: 535 | Loss: 0.02460080198943615       | Test loss: 0.09235973656177521\n",
            "Epoch: 536 | Loss: 0.02454720251262188       | Test loss: 0.0921257957816124\n",
            "Epoch: 537 | Loss: 0.024493258446455002      | Test loss: 0.09184765815734863\n",
            "Epoch: 538 | Loss: 0.024439118802547455      | Test loss: 0.09156908839941025\n",
            "Epoch: 539 | Loss: 0.02438567765057087       | Test loss: 0.09142623096704483\n",
            "Epoch: 540 | Loss: 0.024331437423825264      | Test loss: 0.09140496701002121\n",
            "Epoch: 541 | Loss: 0.02427762560546398       | Test loss: 0.09132208675146103\n",
            "Epoch: 542 | Loss: 0.024223877117037773      | Test loss: 0.09118533134460449\n",
            "Epoch: 543 | Loss: 0.02417014352977276       | Test loss: 0.09099578857421875\n",
            "Epoch: 544 | Loss: 0.024115828797221184      | Test loss: 0.090763621032238\n",
            "Epoch: 545 | Loss: 0.024061614647507668      | Test loss: 0.09048686176538467\n",
            "Epoch: 546 | Loss: 0.02400747500360012       | Test loss: 0.09017415344715118\n",
            "Epoch: 547 | Loss: 0.0239532720297575        | Test loss: 0.08986363559961319\n",
            "Epoch: 548 | Loss: 0.023899218067526817      | Test loss: 0.08955173939466476\n",
            "Epoch: 549 | Loss: 0.02384558692574501       | Test loss: 0.0893821269273758\n",
            "Epoch: 550 | Loss: 0.02379130944609642       | Test loss: 0.08933568000793457\n",
            "Epoch: 551 | Loss: 0.02373679168522358       | Test loss: 0.08926482498645782\n",
            "Epoch: 552 | Loss: 0.023682627826929092      | Test loss: 0.08917007595300674\n",
            "Epoch: 553 | Loss: 0.02362830936908722       | Test loss: 0.0890204906463623\n",
            "Epoch: 554 | Loss: 0.023574223741889         | Test loss: 0.08882074803113937\n",
            "Epoch: 555 | Loss: 0.02352011390030384       | Test loss: 0.0885767936706543\n",
            "Epoch: 556 | Loss: 0.02346566505730152       | Test loss: 0.08829193562269211\n",
            "Epoch: 557 | Loss: 0.023410871624946594      | Test loss: 0.08800530433654785\n",
            "Epoch: 558 | Loss: 0.02335653267800808       | Test loss: 0.08771558105945587\n",
            "Epoch: 559 | Loss: 0.0233018659055233        | Test loss: 0.0874248519539833\n",
            "Epoch: 560 | Loss: 0.023247385397553444      | Test loss: 0.08713250607252121\n",
            "Epoch: 561 | Loss: 0.023193230852484703      | Test loss: 0.08697748184204102\n",
            "Epoch: 562 | Loss: 0.023138608783483505      | Test loss: 0.08680839836597443\n",
            "Epoch: 563 | Loss: 0.023083968088030815      | Test loss: 0.0866248607635498\n",
            "Epoch: 564 | Loss: 0.023029258474707603      | Test loss: 0.0864286944270134\n",
            "Epoch: 565 | Loss: 0.02297469601035118       | Test loss: 0.0861886516213417\n",
            "Epoch: 566 | Loss: 0.0229202713817358        | Test loss: 0.08590593189001083\n",
            "Epoch: 567 | Loss: 0.02286527492105961       | Test loss: 0.0857606902718544\n",
            "Epoch: 568 | Loss: 0.02281091921031475       | Test loss: 0.08556518703699112\n",
            "Epoch: 569 | Loss: 0.022756213322281837      | Test loss: 0.08532591164112091\n",
            "Epoch: 570 | Loss: 0.022701390087604523      | Test loss: 0.08504300564527512\n",
            "Epoch: 571 | Loss: 0.02264685183763504       | Test loss: 0.08489871025085449\n",
            "Epoch: 572 | Loss: 0.022591998800635338      | Test loss: 0.08470344543457031\n",
            "Epoch: 573 | Loss: 0.022536946460604668      | Test loss: 0.08446293324232101\n",
            "Epoch: 574 | Loss: 0.022482194006443024      | Test loss: 0.08435521274805069\n",
            "Epoch: 575 | Loss: 0.022427532821893692      | Test loss: 0.08419366180896759\n",
            "Epoch: 576 | Loss: 0.022372981533408165      | Test loss: 0.08398289978504181\n",
            "Epoch: 577 | Loss: 0.022317759692668915      | Test loss: 0.08372869342565536\n",
            "Epoch: 578 | Loss: 0.022262917831540108      | Test loss: 0.08346714824438095\n",
            "Epoch: 579 | Loss: 0.022207820788025856      | Test loss: 0.0832027941942215\n",
            "Epoch: 580 | Loss: 0.022153154015541077      | Test loss: 0.08307390660047531\n",
            "Epoch: 581 | Loss: 0.022097883746027946      | Test loss: 0.0829256996512413\n",
            "Epoch: 582 | Loss: 0.022042958065867424      | Test loss: 0.08276247978210449\n",
            "Epoch: 583 | Loss: 0.021987713873386383      | Test loss: 0.0825478583574295\n",
            "Epoch: 584 | Loss: 0.021932629868388176      | Test loss: 0.08229084312915802\n",
            "Epoch: 585 | Loss: 0.021877458319067955      | Test loss: 0.08202848583459854\n",
            "Epoch: 586 | Loss: 0.021822502836585045      | Test loss: 0.08175935596227646\n",
            "Epoch: 587 | Loss: 0.021767480298876762      | Test loss: 0.08162941783666611\n",
            "Epoch: 588 | Loss: 0.021712200716137886      | Test loss: 0.08144593238830566\n",
            "Epoch: 589 | Loss: 0.021656762808561325      | Test loss: 0.08121438324451447\n",
            "Epoch: 590 | Loss: 0.021601567044854164      | Test loss: 0.08093929290771484\n",
            "Epoch: 591 | Loss: 0.0215462613850832        | Test loss: 0.0808042511343956\n",
            "Epoch: 592 | Loss: 0.021491164341568947      | Test loss: 0.08061452209949493\n",
            "Epoch: 593 | Loss: 0.021435756236314774      | Test loss: 0.08038025349378586\n",
            "Epoch: 594 | Loss: 0.021380489692091942      | Test loss: 0.08013591915369034\n",
            "Epoch: 595 | Loss: 0.021324902772903442      | Test loss: 0.07988572120666504\n",
            "Epoch: 596 | Loss: 0.02126985788345337       | Test loss: 0.07977195084095001\n",
            "Epoch: 597 | Loss: 0.021214265376329422      | Test loss: 0.07962093502283096\n",
            "Epoch: 598 | Loss: 0.021159065887331963      | Test loss: 0.07941865921020508\n",
            "Epoch: 599 | Loss: 0.021103495731949806      | Test loss: 0.07916899025440216\n",
            "Epoch: 600 | Loss: 0.02104807086288929       | Test loss: 0.07887887954711914\n",
            "Epoch: 601 | Loss: 0.02099245972931385       | Test loss: 0.07872944325208664\n",
            "Epoch: 602 | Loss: 0.020936770364642143      | Test loss: 0.07856211811304092\n",
            "Epoch: 603 | Loss: 0.020881012082099915      | Test loss: 0.07834696769714355\n",
            "Epoch: 604 | Loss: 0.020825615152716637      | Test loss: 0.07812090218067169\n",
            "Epoch: 605 | Loss: 0.020770065486431122      | Test loss: 0.07788462936878204\n",
            "Epoch: 606 | Loss: 0.02071411907672882       | Test loss: 0.07764001190662384\n",
            "Epoch: 607 | Loss: 0.020658573135733604      | Test loss: 0.07738709449768066\n",
            "Epoch: 608 | Loss: 0.02060285583138466       | Test loss: 0.07727207988500595\n",
            "Epoch: 609 | Loss: 0.02054724656045437       | Test loss: 0.0771031379699707\n",
            "Epoch: 610 | Loss: 0.020491786301136017      | Test loss: 0.07688456028699875\n",
            "Epoch: 611 | Loss: 0.020435836166143417      | Test loss: 0.07662196457386017\n",
            "Epoch: 612 | Loss: 0.020380014553666115      | Test loss: 0.07631874084472656\n",
            "Epoch: 613 | Loss: 0.020323822274804115      | Test loss: 0.07613935321569443\n",
            "Epoch: 614 | Loss: 0.020267782732844353      | Test loss: 0.076055146753788\n",
            "Epoch: 615 | Loss: 0.020212309435009956      | Test loss: 0.07591114193201065\n",
            "Epoch: 616 | Loss: 0.020156441256403923      | Test loss: 0.07571611553430557\n",
            "Epoch: 617 | Loss: 0.02010049857199192       | Test loss: 0.07547397911548615\n",
            "Epoch: 618 | Loss: 0.020044153556227684      | Test loss: 0.075189970433712\n",
            "Epoch: 619 | Loss: 0.019988110288977623      | Test loss: 0.07490115612745285\n",
            "Epoch: 620 | Loss: 0.019932672381401062      | Test loss: 0.07475409656763077\n",
            "Epoch: 621 | Loss: 0.019876277074217796      | Test loss: 0.07458911091089249\n",
            "Epoch: 622 | Loss: 0.019820237532258034      | Test loss: 0.07440771907567978\n",
            "Epoch: 623 | Loss: 0.01976407878100872       | Test loss: 0.07421384006738663\n",
            "Epoch: 624 | Loss: 0.019707784056663513      | Test loss: 0.07400660961866379\n",
            "Epoch: 625 | Loss: 0.01965152658522129       | Test loss: 0.0737895518541336\n",
            "Epoch: 626 | Loss: 0.019595550373196602      | Test loss: 0.07355957478284836\n",
            "Epoch: 627 | Loss: 0.019539326429367065      | Test loss: 0.07332330197095871\n",
            "Epoch: 628 | Loss: 0.019483046606183052      | Test loss: 0.07307620346546173\n",
            "Epoch: 629 | Loss: 0.019426511600613594      | Test loss: 0.07278638333082199\n",
            "Epoch: 630 | Loss: 0.019370228052139282      | Test loss: 0.07249575108289719\n",
            "Epoch: 631 | Loss: 0.019314242526888847      | Test loss: 0.07234311103820801\n",
            "Epoch: 632 | Loss: 0.019257444888353348      | Test loss: 0.07213878631591797\n",
            "Epoch: 633 | Loss: 0.019201215356588364      | Test loss: 0.07189035415649414\n",
            "Epoch: 634 | Loss: 0.01914479210972786       | Test loss: 0.07174205780029297\n",
            "Epoch: 635 | Loss: 0.01908847503364086       | Test loss: 0.0715431198477745\n",
            "Epoch: 636 | Loss: 0.019031984731554985      | Test loss: 0.07129406929016113\n",
            "Epoch: 637 | Loss: 0.0189753919839859        | Test loss: 0.0710047259926796\n",
            "Epoch: 638 | Loss: 0.018919309601187706      | Test loss: 0.07082019001245499\n",
            "Epoch: 639 | Loss: 0.01886250078678131       | Test loss: 0.07073283195495605\n",
            "Epoch: 640 | Loss: 0.01880577765405178       | Test loss: 0.07058697193861008\n",
            "Epoch: 641 | Loss: 0.018749423325061798      | Test loss: 0.07038813084363937\n",
            "Epoch: 642 | Loss: 0.018692634999752045      | Test loss: 0.0701407939195633\n",
            "Epoch: 643 | Loss: 0.018636126071214676      | Test loss: 0.06988459080457687\n",
            "Epoch: 644 | Loss: 0.01857927441596985       | Test loss: 0.06962241977453232\n",
            "Epoch: 645 | Loss: 0.018523013219237328      | Test loss: 0.06950130313634872\n",
            "Epoch: 646 | Loss: 0.01846599020063877       | Test loss: 0.06935868412256241\n",
            "Epoch: 647 | Loss: 0.01840931922197342       | Test loss: 0.0691976547241211\n",
            "Epoch: 648 | Loss: 0.018352622166275978      | Test loss: 0.0690188929438591\n",
            "Epoch: 649 | Loss: 0.018295545130968094      | Test loss: 0.06882891803979874\n",
            "Epoch: 650 | Loss: 0.018238862976431847      | Test loss: 0.0685877799987793\n",
            "Epoch: 651 | Loss: 0.01818196102976799       | Test loss: 0.06832070648670197\n",
            "Epoch: 652 | Loss: 0.018125122413039207      | Test loss: 0.06804599612951279\n",
            "Epoch: 653 | Loss: 0.01806822419166565       | Test loss: 0.06776652485132217\n",
            "Epoch: 654 | Loss: 0.018011197447776794      | Test loss: 0.06748447567224503\n",
            "Epoch: 655 | Loss: 0.01795404963195324       | Test loss: 0.06719503551721573\n",
            "Epoch: 656 | Loss: 0.017897412180900574      | Test loss: 0.0670502707362175\n",
            "Epoch: 657 | Loss: 0.017839699983596802      | Test loss: 0.06688771396875381\n",
            "Epoch: 658 | Loss: 0.017783183604478836      | Test loss: 0.06667132675647736\n",
            "Epoch: 659 | Loss: 0.017725862562656403      | Test loss: 0.06641049683094025\n",
            "Epoch: 660 | Loss: 0.017668750137090683      | Test loss: 0.06610646098852158\n",
            "Epoch: 661 | Loss: 0.017611749470233917      | Test loss: 0.0659119114279747\n",
            "Epoch: 662 | Loss: 0.01755453087389469       | Test loss: 0.06581325829029083\n",
            "Epoch: 663 | Loss: 0.017497433349490166      | Test loss: 0.06565580517053604\n",
            "Epoch: 664 | Loss: 0.017440423369407654      | Test loss: 0.06544627994298935\n",
            "Epoch: 665 | Loss: 0.017382869496941566      | Test loss: 0.06518854945898056\n",
            "Epoch: 666 | Loss: 0.017325347289443016      | Test loss: 0.06488919258117676\n",
            "Epoch: 667 | Loss: 0.017268074676394463      | Test loss: 0.06465897709131241\n",
            "Epoch: 668 | Loss: 0.01721096783876419       | Test loss: 0.06456770747900009\n",
            "Epoch: 669 | Loss: 0.0171533040702343        | Test loss: 0.06445398181676865\n",
            "Epoch: 670 | Loss: 0.01709633879363537       | Test loss: 0.06428084522485733\n",
            "Epoch: 671 | Loss: 0.017039133235812187      | Test loss: 0.06405472755432129\n",
            "Epoch: 672 | Loss: 0.016981340944767         | Test loss: 0.0637851282954216\n",
            "Epoch: 673 | Loss: 0.016923746094107628      | Test loss: 0.06350865215063095\n",
            "Epoch: 674 | Loss: 0.016866564750671387      | Test loss: 0.06322427093982697\n",
            "Epoch: 675 | Loss: 0.016809016466140747      | Test loss: 0.06293725967407227\n",
            "Epoch: 676 | Loss: 0.016751568764448166      | Test loss: 0.06279444694519043\n",
            "Epoch: 677 | Loss: 0.016693904995918274      | Test loss: 0.06263227760791779\n",
            "Epoch: 678 | Loss: 0.016636094078421593      | Test loss: 0.06245556101202965\n",
            "Epoch: 679 | Loss: 0.016578568145632744      | Test loss: 0.06226024776697159\n",
            "Epoch: 680 | Loss: 0.016521021723747253      | Test loss: 0.06201639398932457\n",
            "Epoch: 681 | Loss: 0.016463113948702812      | Test loss: 0.06172647699713707\n",
            "Epoch: 682 | Loss: 0.016405461356043816      | Test loss: 0.06143026426434517\n",
            "Epoch: 683 | Loss: 0.01634746789932251       | Test loss: 0.06113271787762642\n",
            "Epoch: 684 | Loss: 0.016290321946144104      | Test loss: 0.06097984313964844\n",
            "Epoch: 685 | Loss: 0.016231855377554893      | Test loss: 0.06084737926721573\n",
            "Epoch: 686 | Loss: 0.016174254938960075      | Test loss: 0.06065964698791504\n",
            "Epoch: 687 | Loss: 0.016116473823785782      | Test loss: 0.0604211799800396\n",
            "Epoch: 688 | Loss: 0.01605868712067604       | Test loss: 0.060135651379823685\n",
            "Epoch: 689 | Loss: 0.01600029319524765       | Test loss: 0.05980978161096573\n",
            "Epoch: 690 | Loss: 0.015942944213747978      | Test loss: 0.059632014483213425\n",
            "Epoch: 691 | Loss: 0.015884915366768837      | Test loss: 0.05958738550543785\n",
            "Epoch: 692 | Loss: 0.015826895833015442      | Test loss: 0.0594785213470459\n",
            "Epoch: 693 | Loss: 0.01576896570622921       | Test loss: 0.05931062623858452\n",
            "Epoch: 694 | Loss: 0.01571098528802395       | Test loss: 0.05909080430865288\n",
            "Epoch: 695 | Loss: 0.01565290056169033       | Test loss: 0.05882158502936363\n",
            "Epoch: 696 | Loss: 0.015594566240906715      | Test loss: 0.05850820615887642\n",
            "Epoch: 697 | Loss: 0.015536332502961159      | Test loss: 0.058194972574710846\n",
            "Epoch: 698 | Loss: 0.015478200279176235      | Test loss: 0.05795412138104439\n",
            "Epoch: 699 | Loss: 0.015420409850776196      | Test loss: 0.05785379558801651\n",
            "Epoch: 700 | Loss: 0.015361870639026165      | Test loss: 0.057729579508304596\n",
            "Epoch: 701 | Loss: 0.015303472056984901      | Test loss: 0.05758380889892578\n",
            "Epoch: 702 | Loss: 0.01524537242949009       | Test loss: 0.057381775230169296\n",
            "Epoch: 703 | Loss: 0.015187220647931099      | Test loss: 0.057129669934511185\n",
            "Epoch: 704 | Loss: 0.015128609724342823      | Test loss: 0.05683403089642525\n",
            "Epoch: 705 | Loss: 0.015070510096848011      | Test loss: 0.05653395876288414\n",
            "Epoch: 706 | Loss: 0.015012008138000965      | Test loss: 0.05622854456305504\n",
            "Epoch: 707 | Loss: 0.014953610487282276      | Test loss: 0.05607204511761665\n",
            "Epoch: 708 | Loss: 0.014895232394337654      | Test loss: 0.055896855890750885\n",
            "Epoch: 709 | Loss: 0.014836758375167847      | Test loss: 0.05570554733276367\n",
            "Epoch: 710 | Loss: 0.014777982607483864      | Test loss: 0.055498313158750534\n",
            "Epoch: 711 | Loss: 0.014719860628247261      | Test loss: 0.05524282529950142\n",
            "Epoch: 712 | Loss: 0.014661261811852455      | Test loss: 0.05493965372443199\n",
            "Epoch: 713 | Loss: 0.014602497220039368      | Test loss: 0.054634809494018555\n",
            "Epoch: 714 | Loss: 0.014543837867677212      | Test loss: 0.05447583273053169\n",
            "Epoch: 715 | Loss: 0.014485010877251625      | Test loss: 0.054264020174741745\n",
            "Epoch: 716 | Loss: 0.014426584355533123      | Test loss: 0.05400443077087402\n",
            "Epoch: 717 | Loss: 0.014367847703397274      | Test loss: 0.053714703768491745\n",
            "Epoch: 718 | Loss: 0.014309561811387539      | Test loss: 0.053572844713926315\n",
            "Epoch: 719 | Loss: 0.014250397682189941      | Test loss: 0.0533754825592041\n",
            "Epoch: 720 | Loss: 0.01419166661798954       | Test loss: 0.05312604829668999\n",
            "Epoch: 721 | Loss: 0.014132737182080746      | Test loss: 0.05282912403345108\n",
            "Epoch: 722 | Loss: 0.014074333012104034      | Test loss: 0.05268092080950737\n",
            "Epoch: 723 | Loss: 0.014015132561326027      | Test loss: 0.052512504160404205\n",
            "Epoch: 724 | Loss: 0.013956240378320217      | Test loss: 0.05229058489203453\n",
            "Epoch: 725 | Loss: 0.013897395692765713      | Test loss: 0.0520198829472065\n",
            "Epoch: 726 | Loss: 0.01383840013295412       | Test loss: 0.051741935312747955\n",
            "Epoch: 727 | Loss: 0.013779806904494762      | Test loss: 0.05161180719733238\n",
            "Epoch: 728 | Loss: 0.013720649294555187      | Test loss: 0.051458027213811874\n",
            "Epoch: 729 | Loss: 0.013661577366292477      | Test loss: 0.05124802514910698\n",
            "Epoch: 730 | Loss: 0.013602538965642452      | Test loss: 0.05098881945014\n",
            "Epoch: 731 | Loss: 0.013543578796088696      | Test loss: 0.05068497732281685\n",
            "Epoch: 732 | Loss: 0.013484290800988674      | Test loss: 0.05037546157836914\n",
            "Epoch: 733 | Loss: 0.013425946235656738      | Test loss: 0.05021634325385094\n",
            "Epoch: 734 | Loss: 0.01336620282381773       | Test loss: 0.050188686698675156\n",
            "Epoch: 735 | Loss: 0.013307074084877968      | Test loss: 0.050095703452825546\n",
            "Epoch: 736 | Loss: 0.013248211704194546      | Test loss: 0.049940016120672226\n",
            "Epoch: 737 | Loss: 0.013189163990318775      | Test loss: 0.049726393073797226\n",
            "Epoch: 738 | Loss: 0.013129827566444874      | Test loss: 0.049463607370853424\n",
            "Epoch: 739 | Loss: 0.013070585206151009      | Test loss: 0.04915571212768555\n",
            "Epoch: 740 | Loss: 0.013010834343731403      | Test loss: 0.04880819469690323\n",
            "Epoch: 741 | Loss: 0.012951264157891273      | Test loss: 0.04845914989709854\n",
            "Epoch: 742 | Loss: 0.012892143800854683      | Test loss: 0.048264410346746445\n",
            "Epoch: 743 | Loss: 0.012832826934754848      | Test loss: 0.04820866510272026\n",
            "Epoch: 744 | Loss: 0.012773403897881508      | Test loss: 0.048122357577085495\n",
            "Epoch: 745 | Loss: 0.01271388866007328       | Test loss: 0.04801011085510254\n",
            "Epoch: 746 | Loss: 0.01265460904687643       | Test loss: 0.04783911630511284\n",
            "Epoch: 747 | Loss: 0.012595086358487606      | Test loss: 0.04761085659265518\n",
            "Epoch: 748 | Loss: 0.012535554356873035      | Test loss: 0.04733419418334961\n",
            "Epoch: 749 | Loss: 0.012476115487515926      | Test loss: 0.04701213911175728\n",
            "Epoch: 750 | Loss: 0.012416139245033264      | Test loss: 0.04668903350830078\n",
            "Epoch: 751 | Loss: 0.012356370687484741      | Test loss: 0.046361781656742096\n",
            "Epoch: 752 | Loss: 0.012296872213482857      | Test loss: 0.046033669263124466\n",
            "Epoch: 753 | Loss: 0.012237397953867912      | Test loss: 0.045857954770326614\n",
            "Epoch: 754 | Loss: 0.012177417986094952      | Test loss: 0.045662928372621536\n",
            "Epoch: 755 | Loss: 0.012117571197450161      | Test loss: 0.04545560106635094\n",
            "Epoch: 756 | Loss: 0.012057923711836338      | Test loss: 0.04523167759180069\n",
            "Epoch: 757 | Loss: 0.011997999623417854      | Test loss: 0.04499421268701553\n",
            "Epoch: 758 | Loss: 0.011938090436160564      | Test loss: 0.044747065752744675\n",
            "Epoch: 759 | Loss: 0.011878065764904022      | Test loss: 0.0444914810359478\n",
            "Epoch: 760 | Loss: 0.01181834191083908       | Test loss: 0.04418616369366646\n",
            "Epoch: 761 | Loss: 0.011758244596421719      | Test loss: 0.043840982019901276\n",
            "Epoch: 762 | Loss: 0.011699098162353039      | Test loss: 0.04364776611328125\n",
            "Epoch: 763 | Loss: 0.011638893745839596      | Test loss: 0.043557312339544296\n",
            "Epoch: 764 | Loss: 0.011578495614230633      | Test loss: 0.043404437601566315\n",
            "Epoch: 765 | Loss: 0.011518634855747223      | Test loss: 0.04319310188293457\n",
            "Epoch: 766 | Loss: 0.011458839289844036      | Test loss: 0.04293184354901314\n",
            "Epoch: 767 | Loss: 0.011398741975426674      | Test loss: 0.04262151941657066\n",
            "Epoch: 768 | Loss: 0.011338049545884132      | Test loss: 0.04227037355303764\n",
            "Epoch: 769 | Loss: 0.011278382502496243      | Test loss: 0.04207458719611168\n",
            "Epoch: 770 | Loss: 0.011218362487852573      | Test loss: 0.042017318308353424\n",
            "Epoch: 771 | Loss: 0.011157973669469357      | Test loss: 0.04189472272992134\n",
            "Epoch: 772 | Loss: 0.01109781488776207       | Test loss: 0.04170951992273331\n",
            "Epoch: 773 | Loss: 0.011037598364055157      | Test loss: 0.041469525545835495\n",
            "Epoch: 774 | Loss: 0.010977103374898434      | Test loss: 0.04118170961737633\n",
            "Epoch: 775 | Loss: 0.01091662235558033       | Test loss: 0.04088783264160156\n",
            "Epoch: 776 | Loss: 0.010856335051357746      | Test loss: 0.04058866575360298\n",
            "Epoch: 777 | Loss: 0.010796132497489452      | Test loss: 0.0404387004673481\n",
            "Epoch: 778 | Loss: 0.010735798627138138      | Test loss: 0.04026961326599121\n",
            "Epoch: 779 | Loss: 0.010675271041691303      | Test loss: 0.04008007049560547\n",
            "Epoch: 780 | Loss: 0.010614875704050064      | Test loss: 0.039876509457826614\n",
            "Epoch: 781 | Loss: 0.010554407723248005      | Test loss: 0.039617348462343216\n",
            "Epoch: 782 | Loss: 0.010494022630155087      | Test loss: 0.03931155428290367\n",
            "Epoch: 783 | Loss: 0.010433218441903591      | Test loss: 0.03900022432208061\n",
            "Epoch: 784 | Loss: 0.01037269551306963       | Test loss: 0.03868546709418297\n",
            "Epoch: 785 | Loss: 0.010312494821846485      | Test loss: 0.03852195665240288\n",
            "Epoch: 786 | Loss: 0.010251639410853386      | Test loss: 0.038497209548950195\n",
            "Epoch: 787 | Loss: 0.010191045701503754      | Test loss: 0.03840136528015137\n",
            "Epoch: 788 | Loss: 0.010130765847861767      | Test loss: 0.03824038431048393\n",
            "Epoch: 789 | Loss: 0.010070252232253551      | Test loss: 0.0380244255065918\n",
            "Epoch: 790 | Loss: 0.010009623132646084      | Test loss: 0.03775391727685928\n",
            "Epoch: 791 | Loss: 0.009948491118848324      | Test loss: 0.03743886947631836\n",
            "Epoch: 792 | Loss: 0.009887791238725185      | Test loss: 0.03707847744226456\n",
            "Epoch: 793 | Loss: 0.009826341643929482      | Test loss: 0.03668379783630371\n",
            "Epoch: 794 | Loss: 0.00976567342877388       | Test loss: 0.03644590452313423\n",
            "Epoch: 795 | Loss: 0.009704845026135445      | Test loss: 0.0363561175763607\n",
            "Epoch: 796 | Loss: 0.00964367762207985       | Test loss: 0.03623924404382706\n",
            "Epoch: 797 | Loss: 0.009582857601344585      | Test loss: 0.03609743341803551\n",
            "Epoch: 798 | Loss: 0.00952230952680111       | Test loss: 0.03589515760540962\n",
            "Epoch: 799 | Loss: 0.009461148642003536      | Test loss: 0.03564004972577095\n",
            "Epoch: 800 | Loss: 0.009400044567883015      | Test loss: 0.03533539921045303\n",
            "Epoch: 801 | Loss: 0.009338892064988613      | Test loss: 0.035027455538511276\n",
            "Epoch: 802 | Loss: 0.009277905337512493      | Test loss: 0.03471355512738228\n",
            "Epoch: 803 | Loss: 0.009216533973813057      | Test loss: 0.034395407885313034\n",
            "Epoch: 804 | Loss: 0.00915596354752779       | Test loss: 0.034230757504701614\n",
            "Epoch: 805 | Loss: 0.009094446897506714      | Test loss: 0.03404593467712402\n",
            "Epoch: 806 | Loss: 0.009033179841935635      | Test loss: 0.03384385257959366\n",
            "Epoch: 807 | Loss: 0.00897187925875187       | Test loss: 0.03362636640667915\n",
            "Epoch: 808 | Loss: 0.008910696022212505      | Test loss: 0.03335714340209961\n",
            "Epoch: 809 | Loss: 0.00884958729147911       | Test loss: 0.033037569373846054\n",
            "Epoch: 810 | Loss: 0.008787953294813633      | Test loss: 0.03271489217877388\n",
            "Epoch: 811 | Loss: 0.00872726272791624       | Test loss: 0.032546233385801315\n",
            "Epoch: 812 | Loss: 0.008665229193866253      | Test loss: 0.03247947618365288\n",
            "Epoch: 813 | Loss: 0.008604152128100395      | Test loss: 0.032346632331609726\n",
            "Epoch: 814 | Loss: 0.008543241769075394      | Test loss: 0.03215012699365616\n",
            "Epoch: 815 | Loss: 0.008481881581246853      | Test loss: 0.03190026432275772\n",
            "Epoch: 816 | Loss: 0.008419972844421864      | Test loss: 0.03159761428833008\n",
            "Epoch: 817 | Loss: 0.008358467370271683      | Test loss: 0.031252529472112656\n",
            "Epoch: 818 | Loss: 0.008296625688672066      | Test loss: 0.03086562268435955\n",
            "Epoch: 819 | Loss: 0.008235828951001167      | Test loss: 0.03063950501382351\n",
            "Epoch: 820 | Loss: 0.008173967711627483      | Test loss: 0.03056039847433567\n",
            "Epoch: 821 | Loss: 0.008111769333481789      | Test loss: 0.030454445630311966\n",
            "Epoch: 822 | Loss: 0.008050280623137951      | Test loss: 0.030300093814730644\n",
            "Epoch: 823 | Loss: 0.00798871647566557       | Test loss: 0.03008751943707466\n",
            "Epoch: 824 | Loss: 0.007927113212645054      | Test loss: 0.02982182614505291\n",
            "Epoch: 825 | Loss: 0.00786535069346428       | Test loss: 0.02952556684613228\n",
            "Epoch: 826 | Loss: 0.007803262677043676      | Test loss: 0.029222726821899414\n",
            "Epoch: 827 | Loss: 0.007741750683635473      | Test loss: 0.0289151668548584\n",
            "Epoch: 828 | Loss: 0.007679989095777273      | Test loss: 0.028601502999663353\n",
            "Epoch: 829 | Loss: 0.007618511561304331      | Test loss: 0.0284423828125\n",
            "Epoch: 830 | Loss: 0.007555752526968718      | Test loss: 0.028263187035918236\n",
            "Epoch: 831 | Loss: 0.007494259625673294      | Test loss: 0.028063489124178886\n",
            "Epoch: 832 | Loss: 0.007432101760059595      | Test loss: 0.027849722653627396\n",
            "Epoch: 833 | Loss: 0.007370256818830967      | Test loss: 0.02758026123046875\n",
            "Epoch: 834 | Loss: 0.007307988591492176      | Test loss: 0.02726130560040474\n",
            "Epoch: 835 | Loss: 0.007246003951877356      | Test loss: 0.026936961337924004\n",
            "Epoch: 836 | Loss: 0.007184440735727549      | Test loss: 0.026769066229462624\n",
            "Epoch: 837 | Loss: 0.007121920585632324      | Test loss: 0.02654433250427246\n",
            "Epoch: 838 | Loss: 0.007060025818645954      | Test loss: 0.026264095678925514\n",
            "Epoch: 839 | Loss: 0.006997615098953247      | Test loss: 0.02609715424478054\n",
            "Epoch: 840 | Loss: 0.006935509387403727      | Test loss: 0.02587285079061985\n",
            "Epoch: 841 | Loss: 0.0068735540844500065     | Test loss: 0.0255934726446867\n",
            "Epoch: 842 | Loss: 0.00681083370000124       | Test loss: 0.025307893753051758\n",
            "Epoch: 843 | Loss: 0.006748870015144348      | Test loss: 0.025173282250761986\n",
            "Epoch: 844 | Loss: 0.006686357315629721      | Test loss: 0.02497558668255806\n",
            "Epoch: 845 | Loss: 0.006624230649322271      | Test loss: 0.02472386322915554\n",
            "Epoch: 846 | Loss: 0.006561701186001301      | Test loss: 0.024457931518554688\n",
            "Epoch: 847 | Loss: 0.006499504204839468      | Test loss: 0.024183988571166992\n",
            "Epoch: 848 | Loss: 0.0064370157197117805     | Test loss: 0.024060392752289772\n",
            "Epoch: 849 | Loss: 0.006374885328114033      | Test loss: 0.023872947320342064\n",
            "Epoch: 850 | Loss: 0.006312207784503698      | Test loss: 0.02362680435180664\n",
            "Epoch: 851 | Loss: 0.006249809172004461      | Test loss: 0.023329544812440872\n",
            "Epoch: 852 | Loss: 0.0061872354708611965     | Test loss: 0.0230255126953125\n",
            "Epoch: 853 | Loss: 0.006124644540250301      | Test loss: 0.02287731133401394\n",
            "Epoch: 854 | Loss: 0.006061935797333717      | Test loss: 0.022704793140292168\n",
            "Epoch: 855 | Loss: 0.005999444052577019      | Test loss: 0.02247633971273899\n",
            "Epoch: 856 | Loss: 0.0059370058588683605     | Test loss: 0.02219104766845703\n",
            "Epoch: 857 | Loss: 0.00587434833869338       | Test loss: 0.021897315979003906\n",
            "Epoch: 858 | Loss: 0.005811524577438831      | Test loss: 0.021595334634184837\n",
            "Epoch: 859 | Loss: 0.005748743657022715      | Test loss: 0.021449804306030273\n",
            "Epoch: 860 | Loss: 0.0056861951015889645     | Test loss: 0.021280337125062943\n",
            "Epoch: 861 | Loss: 0.005623340141028166      | Test loss: 0.021051883697509766\n",
            "Epoch: 862 | Loss: 0.005560691934078932      | Test loss: 0.020769834518432617\n",
            "Epoch: 863 | Loss: 0.005497912410646677      | Test loss: 0.020436858758330345\n",
            "Epoch: 864 | Loss: 0.005434614606201649      | Test loss: 0.020102454349398613\n",
            "Epoch: 865 | Loss: 0.005372253712266684      | Test loss: 0.019925594329833984\n",
            "Epoch: 866 | Loss: 0.005308953113853931      | Test loss: 0.01989297941327095\n",
            "Epoch: 867 | Loss: 0.0052461931481957436     | Test loss: 0.01978449895977974\n",
            "Epoch: 868 | Loss: 0.00518365204334259       | Test loss: 0.019612694159150124\n",
            "Epoch: 869 | Loss: 0.005120817106217146      | Test loss: 0.019379759207367897\n",
            "Epoch: 870 | Loss: 0.005057589150965214      | Test loss: 0.019093800336122513\n",
            "Epoch: 871 | Loss: 0.004994410090148449      | Test loss: 0.018758011981844902\n",
            "Epoch: 872 | Loss: 0.004931003320962191      | Test loss: 0.018378783017396927\n",
            "Epoch: 873 | Loss: 0.004867818206548691      | Test loss: 0.01800088956952095\n",
            "Epoch: 874 | Loss: 0.004804970230907202      | Test loss: 0.01778721809387207\n",
            "Epoch: 875 | Loss: 0.0047417995519936085     | Test loss: 0.017718886956572533\n",
            "Epoch: 876 | Loss: 0.004678294062614441      | Test loss: 0.01761808432638645\n",
            "Epoch: 877 | Loss: 0.0046149049885571        | Test loss: 0.01745295524597168\n",
            "Epoch: 878 | Loss: 0.004551924765110016      | Test loss: 0.0172258373349905\n",
            "Epoch: 879 | Loss: 0.004488797392696142      | Test loss: 0.01694507710635662\n",
            "Epoch: 880 | Loss: 0.004424916114658117      | Test loss: 0.01661515235900879\n",
            "Epoch: 881 | Loss: 0.0043615601025521755     | Test loss: 0.016280461102724075\n",
            "Epoch: 882 | Loss: 0.0042983414605259895     | Test loss: 0.01593961752951145\n",
            "Epoch: 883 | Loss: 0.004235078115016222      | Test loss: 0.015759659931063652\n",
            "Epoch: 884 | Loss: 0.004171267151832581      | Test loss: 0.015560531988739967\n",
            "Epoch: 885 | Loss: 0.004107784479856491      | Test loss: 0.015343856997787952\n",
            "Epoch: 886 | Loss: 0.004044426139444113      | Test loss: 0.015109920874238014\n",
            "Epoch: 887 | Loss: 0.003980704583227634      | Test loss: 0.014860677532851696\n",
            "Epoch: 888 | Loss: 0.003917166963219643      | Test loss: 0.014599752612411976\n",
            "Epoch: 889 | Loss: 0.003853561356663704      | Test loss: 0.01432414073497057\n",
            "Epoch: 890 | Loss: 0.0037897147703915834     | Test loss: 0.01400136947631836\n",
            "Epoch: 891 | Loss: 0.003726101713255048      | Test loss: 0.013835906982421875\n",
            "Epoch: 892 | Loss: 0.0036624025087803602     | Test loss: 0.013608074747025967\n",
            "Epoch: 893 | Loss: 0.0035988695453852415     | Test loss: 0.013327456079423428\n",
            "Epoch: 894 | Loss: 0.003535038325935602      | Test loss: 0.01299519557505846\n",
            "Epoch: 895 | Loss: 0.0034711367916315794     | Test loss: 0.012785101309418678\n",
            "Epoch: 896 | Loss: 0.0034072950948029757     | Test loss: 0.012681198306381702\n",
            "Epoch: 897 | Loss: 0.0033437979873269796     | Test loss: 0.012509727850556374\n",
            "Epoch: 898 | Loss: 0.003280072007328272      | Test loss: 0.012277078814804554\n",
            "Epoch: 899 | Loss: 0.0032161292620003223     | Test loss: 0.011989975348114967\n",
            "Epoch: 900 | Loss: 0.003151863580569625      | Test loss: 0.011652088724076748\n",
            "Epoch: 901 | Loss: 0.0030881408601999283     | Test loss: 0.011309719644486904\n",
            "Epoch: 902 | Loss: 0.003024784615263343      | Test loss: 0.011131144128739834\n",
            "Epoch: 903 | Loss: 0.00296039879322052       | Test loss: 0.011096573434770107\n",
            "Epoch: 904 | Loss: 0.0028961298521608114     | Test loss: 0.011026144027709961\n",
            "Epoch: 905 | Loss: 0.002832896076142788      | Test loss: 0.010722494684159756\n",
            "Epoch: 906 | Loss: 0.002768556587398052      | Test loss: 0.010243034921586514\n",
            "Epoch: 907 | Loss: 0.0027036911342293024     | Test loss: 0.009776115417480469\n",
            "Epoch: 908 | Loss: 0.0026407584082335234     | Test loss: 0.009481620974838734\n",
            "Epoch: 909 | Loss: 0.0025771630462259054     | Test loss: 0.009515332989394665\n",
            "Epoch: 910 | Loss: 0.002510984893888235      | Test loss: 0.009504938498139381\n",
            "Epoch: 911 | Loss: 0.0024477902334183455     | Test loss: 0.009254837408661842\n",
            "Epoch: 912 | Loss: 0.0023836183827370405     | Test loss: 0.008784866891801357\n",
            "Epoch: 913 | Loss: 0.002318251645192504      | Test loss: 0.00832214392721653\n",
            "Epoch: 914 | Loss: 0.0022551268339157104     | Test loss: 0.008035517297685146\n",
            "Epoch: 915 | Loss: 0.002191258128732443      | Test loss: 0.008074379526078701\n",
            "Epoch: 916 | Loss: 0.0021250462159514427     | Test loss: 0.008029746823012829\n",
            "Epoch: 917 | Loss: 0.0020621567964553833     | Test loss: 0.0077454568818211555\n",
            "Epoch: 918 | Loss: 0.001997540472075343      | Test loss: 0.007245969958603382\n",
            "Epoch: 919 | Loss: 0.0019318460253998637     | Test loss: 0.006925773806869984\n",
            "Epoch: 920 | Loss: 0.0018678915221244097     | Test loss: 0.006764841265976429\n",
            "Epoch: 921 | Loss: 0.0018026995239779353     | Test loss: 0.006748485844582319\n",
            "Epoch: 922 | Loss: 0.0017394230235368013     | Test loss: 0.006490278523415327\n",
            "Epoch: 923 | Loss: 0.001674520201049745      | Test loss: 0.006012153811752796\n",
            "Epoch: 924 | Loss: 0.0016088848933577538     | Test loss: 0.005670070648193359\n",
            "Epoch: 925 | Loss: 0.0015449955826625228     | Test loss: 0.005660820286720991\n",
            "Epoch: 926 | Loss: 0.0014797566691413522     | Test loss: 0.005575037095695734\n",
            "Epoch: 927 | Loss: 0.0014168719062581658     | Test loss: 0.005088329315185547\n",
            "Epoch: 928 | Loss: 0.0013502512592822313     | Test loss: 0.0046096802689135075\n",
            "Epoch: 929 | Loss: 0.0012880020076408982     | Test loss: 0.004652309697121382\n",
            "Epoch: 930 | Loss: 0.001220403821207583      | Test loss: 0.004649830050766468\n",
            "Epoch: 931 | Loss: 0.0011589117348194122     | Test loss: 0.004078340716660023\n",
            "Epoch: 932 | Loss: 0.0010911806020885706     | Test loss: 0.003692770143970847\n",
            "Epoch: 933 | Loss: 0.0010280152782797813     | Test loss: 0.003818941069766879\n",
            "Epoch: 934 | Loss: 0.0009626878309063613     | Test loss: 0.0035234929528087378\n",
            "Epoch: 935 | Loss: 0.0008971529314294457     | Test loss: 0.0028460503090173006\n",
            "Epoch: 936 | Loss: 0.0008372448501177132     | Test loss: 0.003101015230640769\n",
            "Epoch: 937 | Loss: 0.0007687468314543366     | Test loss: 0.002760315081104636\n",
            "Epoch: 938 | Loss: 0.0007023323560133576     | Test loss: 0.0020423412788659334\n",
            "Epoch: 939 | Loss: 0.0006478244322352111     | Test loss: 0.002851867815479636\n",
            "Epoch: 940 | Loss: 0.0006111011025495827     | Test loss: 0.0008783340454101562\n",
            "Epoch: 941 | Loss: 0.0006329910829663277     | Test loss: 0.004947948735207319\n",
            "Epoch: 942 | Loss: 0.0011965114390477538     | Test loss: 0.0013138294452801347\n",
            "Epoch: 943 | Loss: 0.00045406073331832886    | Test loss: 0.0010873795254155993\n",
            "Epoch: 944 | Loss: 0.00046999138430692255    | Test loss: 0.005122137255966663\n",
            "Epoch: 945 | Loss: 0.001014034147374332      | Test loss: 0.0015433788066729903\n",
            "Epoch: 946 | Loss: 0.0004341803432907909     | Test loss: 0.00025472641573287547\n",
            "Epoch: 947 | Loss: 0.0008012399193830788     | Test loss: 0.005384540650993586\n",
            "Epoch: 948 | Loss: 0.0012123429914936423     | Test loss: 0.003295087954029441\n",
            "Epoch: 949 | Loss: 0.0006332436460070312     | Test loss: 0.004330730531364679\n",
            "Epoch: 950 | Loss: 0.0017906854627653956     | Test loss: 0.0041655064560472965\n",
            "Epoch: 951 | Loss: 0.0014797730837017298     | Test loss: 0.0029095651116222143\n",
            "Epoch: 952 | Loss: 0.0011942395940423012     | Test loss: 0.002353191375732422\n",
            "Epoch: 953 | Loss: 0.0012044234899803996     | Test loss: 0.0049665928818285465\n",
            "Epoch: 954 | Loss: 0.001193928881548345      | Test loss: 0.005153751466423273\n",
            "Epoch: 955 | Loss: 0.00120649766176939       | Test loss: 0.001060533570125699\n",
            "Epoch: 956 | Loss: 0.0008833823958411813     | Test loss: 0.00014882087998557836\n",
            "Epoch: 957 | Loss: 0.0004106437845621258     | Test loss: 0.007738924119621515\n",
            "Epoch: 958 | Loss: 0.002311707939952612      | Test loss: 0.00798645056784153\n",
            "Epoch: 959 | Loss: 0.002460298826918006      | Test loss: 0.001649618148803711\n",
            "Epoch: 960 | Loss: 0.00032541248947381973    | Test loss: 0.010527754202485085\n",
            "Epoch: 961 | Loss: 0.0038351044058799744     | Test loss: 0.014946556650102139\n",
            "Epoch: 962 | Loss: 0.005317795556038618      | Test loss: 0.012466478161513805\n",
            "Epoch: 963 | Loss: 0.004421287681907415      | Test loss: 0.0038598538376390934\n",
            "Epoch: 964 | Loss: 0.0015036038821563125     | Test loss: 0.009985066018998623\n",
            "Epoch: 965 | Loss: 0.003322349628433585      | Test loss: 0.016056442633271217\n",
            "Epoch: 966 | Loss: 0.005374784581363201      | Test loss: 0.01521291770040989\n",
            "Epoch: 967 | Loss: 0.00504220649600029       | Test loss: 0.008219718933105469\n",
            "Epoch: 968 | Loss: 0.002591083524748683      | Test loss: 0.004239130299538374\n",
            "Epoch: 969 | Loss: 0.0017903613625094295     | Test loss: 0.00933151226490736\n",
            "Epoch: 970 | Loss: 0.003584784222766757      | Test loss: 0.00776906032115221\n",
            "Epoch: 971 | Loss: 0.003124851733446121      | Test loss: 0.0002986431063618511\n",
            "Epoch: 972 | Loss: 0.0006681824452243745     | Test loss: 0.01234350260347128\n",
            "Epoch: 973 | Loss: 0.003611533436924219      | Test loss: 0.017615748569369316\n",
            "Epoch: 974 | Loss: 0.005314001347869635      | Test loss: 0.016336822882294655\n",
            "Epoch: 975 | Loss: 0.004767098464071751      | Test loss: 0.009225606918334961\n",
            "Epoch: 976 | Loss: 0.002219504676759243      | Test loss: 0.0030694962479174137\n",
            "Epoch: 977 | Loss: 0.002108205808326602      | Test loss: 0.008177376352250576\n",
            "Epoch: 978 | Loss: 0.003945155534893274      | Test loss: 0.006878423970192671\n",
            "Epoch: 979 | Loss: 0.003564839018508792      | Test loss: 0.0003504753112792969\n",
            "Epoch: 980 | Loss: 0.0012095628771930933     | Test loss: 0.012203789316117764\n",
            "Epoch: 981 | Loss: 0.0029025839176028967     | Test loss: 0.01724090613424778\n",
            "Epoch: 982 | Loss: 0.0045906053856015205     | Test loss: 0.015997981652617455\n",
            "Epoch: 983 | Loss: 0.004119733814150095      | Test loss: 0.009163999930024147\n",
            "Epoch: 984 | Loss: 0.001870198524557054      | Test loss: 0.0018805026775225997\n",
            "Epoch: 985 | Loss: 0.001962758367881179      | Test loss: 0.006098699755966663\n",
            "Epoch: 986 | Loss: 0.0033087250776588917     | Test loss: 0.004227638244628906\n",
            "Epoch: 987 | Loss: 0.0025674612261354923     | Test loss: 0.003068304155021906\n",
            "Epoch: 988 | Loss: 0.0005649689701385796     | Test loss: 0.00777544965967536\n",
            "Epoch: 989 | Loss: 0.001938765519298613      | Test loss: 0.006396389100700617\n",
            "Epoch: 990 | Loss: 0.0017154375091195107     | Test loss: 0.00040392877417616546\n",
            "Epoch: 991 | Loss: 0.0005284525686874986     | Test loss: 0.0012447834014892578\n",
            "Epoch: 992 | Loss: 0.0007281079888343811     | Test loss: 0.00321788783185184\n",
            "Epoch: 993 | Loss: 0.0009539513266645372     | Test loss: 0.0017206668853759766\n",
            "Epoch: 994 | Loss: 0.0004372902330942452     | Test loss: 0.0050938609056174755\n",
            "Epoch: 995 | Loss: 0.0019500537309795618     | Test loss: 0.005839967634528875\n",
            "Epoch: 996 | Loss: 0.0022306356113404036     | Test loss: 0.0011374474270269275\n",
            "Epoch: 997 | Loss: 0.0007232172647491097     | Test loss: 0.008319330401718616\n",
            "Epoch: 998 | Loss: 0.002476299414411187      | Test loss: 0.011414289474487305\n",
            "Epoch: 999 | Loss: 0.0034291581250727177     | Test loss: 0.008829260244965553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count=torch.tensor(epoch_count).cpu().numpy()\n",
        "loss_values=torch.tensor(loss_values).cpu().numpy()\n",
        "test_loss_values=torch.tensor(test_loss_values).cpu().numpy()"
      ],
      "metadata": {
        "id": "-PvWjWq3rWB1"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_count, loss_values, label='Train loss')\n",
        "plt.plot(epoch_count, test_loss_values, label='Test loss')\n",
        "plt.title('loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "vNlCwi4c33K7",
        "outputId": "831d9d8a-92fd-40cf-eb31-5e5cfe958486"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'loss')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxvUlEQVR4nO3dd3hUZdr48e+dhCSQQoCEEghSBYIFNGLBgh0ruqKg2FlR17bqzxXfXV9dd/XVXduuFVRsK4JiAXUVxN5QgiBVIPSEFloSQku5f388J2GIgQwhycnM3J/rmiszzzln5j7O7tw8XVQVY4wxkSfK7wCMMcb4wxKAMcZEKEsAxhgToSwBGGNMhLIEYIwxEcoSgDHGRChLAMbsg4gsF5HT/I7DmPpgCcAYYyKUJQBjjIlQlgCMCYKIxInIkyKy2ns8KSJx3rFUEflQRLaIyCYR+UZEorxjd4tInogUichCETnV3zsxZrcYvwMwJkT8GTgG6AMoMBH4C3AvcCeQC6R55x4DqIj0AG4GjlLV1SLSCYhu2LCN2TurARgTnGHAA6q6XlXzgb8CV3jHSoB2wEGqWqKq36hbZKsMiAMyRaSJqi5X1SW+RG9MNSwBGBOcdGBFwOsVXhnAP4EcYIqILBWRkQCqmgP8EbgfWC8i40QkHWMaCUsAxgRnNXBQwOuOXhmqWqSqd6pqF+B84I6Ktn5VHauqx3vXKvBIw4ZtzN5ZAjAmOG8CfxGRNBFJBf4X+A+AiJwrIt1ERIACXNNPuYj0EJFTvM7iHcB2oNyn+I35DUsAxgTn70A2MBuYA/zslQF0B6YCW4EfgGdV9Qtc+//DwAZgLdAauKdhwzZm78Q2hDHGmMhkNQBjjIlQlgCMMSZCWQIwxpgIZQnAGGMiVEgtBZGamqqdOnXyOwxjjAkpM2bM2KCqaVXLQyoBdOrUiezsbL/DMMaYkCIiK6ortyYgY4yJUJYAjDEmQlkCMMaYCGUJwBhjIpQlAGOMiVCWAIwxJkJZAjDGmAgVGQlg7jvwy3iwlU+NMaZSZCSAX8bDeyPg5bNg0WS/ozHGmEYhMhLApW/C2Y9CQS6MvQTeuBhWfG81AmNMRIuMBBAVDf2ug1t+htMfgNzprjbwn4tgQ47f0RljjC8iIwFUiImF/rfB7fPhzIdcInjuWJj6Vyje4Hd0xhjToCIrAVSIbQbH3gQ3Z0Pv38G3j8MTveGbx6B0l9/RGWNMg4jMBFAhqQ38bhTc+D10PwM+ewAe7wXfPgmlO/2Ozhhj6lVkJ4AKbXrDkNfhivcgvS9MvQ+ePRYWf+p3ZMYYU28sAQTqegpcPgGGTQAReGMwjB0CG5f4HZkxxtQ5SwDV6X463PgDnP43WP4tPHsMfPBHWDXd78iMMabOWALYm5hY6H8r3DIDDhkMs8bCS6fBuyNg+2a/ozPGmAMWVAIQkYEislBEckRkZDXHTxSRn0WkVEQGB5SfLCKzAh47ROQC79grIrIs4FifurqpOpXUFi58Dv60FE78k1tW4ul+MPMNKC/3OzpjjKm1GhOAiEQDzwBnAZnApSKSWeW0lcDVwNjAQlX9QlX7qGof4BRgGzAl4JS7Ko6r6qza3kSDiEuEU/4M130OLQ6CiX9wNYLcGX5HZowxtRJMDaAfkKOqS1V1FzAOGBR4gqouV9XZwL7+STwY+FhVt9U62sag3eFw7RS44Hm3tMSLp8Db18C6+X5HZowx+yWYBNAeWBXwOtcr219DgTerlD0oIrNF5AkRiavuIhEZISLZIpKdn59fi4+tB1FR0OdS1z9w/O1uuOhzx8H7f4DCNX5HZ4wxQWmQTmARaQccCgQuxXkP0BM4CmgJ3F3dtao6WlWzVDUrLS2t3mPdL3FJcNr98MfZbmbxnLfh333gozth2ya/ozPGmH0KJgHkARkBrzt4ZfvjEuA9VS2pKFDVNersBF7GNTWFpmYt4cwH4aaf4PChkP0yPHUEzHjFOoqNMY1WMAlgOtBdRDqLSCyuKWfSfn7OpVRp/vFqBYiIABcAc/fzPRuflp3hvH/BDd9C60z44DbXUbx6pt+RGWPMb9SYAFS1FLgZ13yzAHhLVeeJyAMicj6AiBwlIrnAxcAoEZlXcb2IdMLVIL6q8tZviMgcYA6QCvy9Du6ncWiTCVd/BL97AbasghdOcTOKf/3I9iAwxjQaoiH0g5SVlaXZ2dl+h7F/tm+Br/4B89+HwjzoMwzOeRyaxPsdmTEmQojIDFXNqlpuM4HrW9MUGPgQ3DYbThoJs96AJw+B/95lexAYY3xlCaChRMfAyfe4FUc7HQ/TX4Jnjob5E/2OzBgToSwBNLSup8DFr7iO4uYd4K0r3WPzcr8jM8ZEGEsAfmmTCb+fCqf8xZtI1h+m3AtbVvodmTEmQlgC8FN0EzjxLjd/4KDjYNqzbqG575+G8jK/ozPGhDlLAI1BSgYMextunQVdToIpf4YXbaE5Y0z9sgTQmKRkwKXj4KKXYMsKt9DcW1fC1kayBpIxJqxYAmhsRODQwa42MOAeWPiJW1/ow9ttfSFjTJ2yBNBYxSfDgJEw4gvodR7MeNV1FC+aYrOJjTF1whJAY9emN1z4vNuIJjYBxl4Mr18I6+bVfK0xxuyDJYBQkd4HbvweBj7sFpd7/ng3m3hXsd+RGWNClCWAUBITC8fcCLfOhKOug59Gu9nECz60ZiFjzH6zBBCKmrWEs/8B13wCcckwfhj85yJY8oXfkRljQoglgFB20LFw/ddw5kOwZha8fgG8d6MtMmeMCYolgFAXHeO2o7xjgZtVPHsc/LMrjB0Khav9js4Y04hZAggXMXFuXaHrv4aT7oZlX7mN6hd84HdkxphGyhJAuGl7KJz8P3D9N5ByEIy/3PUP5C/0OzJjTCNjCSBcpXaD4Z/C6Q9AbjaMOhF+HGWb1BtjKgWVAERkoIgsFJEcERlZzfETReRnESkVkcFVjpWJyCzvMSmgvLOI/Oi953hvw3lTl2Jiof9tbrXRTifAx3+ClwfC+gV+R2aMaQRqTAAiEg08A5wFZAKXikhmldNWAlcDY6t5i+2q2sd7nB9Q/gjwhKp2AzYDw2sRvwlGUhu32uiFo2DDYjeJ7D8Xwaqf/I7MGOOjYGoA/YAcVV2qqruAccCgwBNUdbmqzgaCal8QEQFOASZ4Ra8CFwQbtKkFETh8KNw8HY6+wS0l8dIZ8PFIm01sTIQKJgG0B1YFvM71yoIVLyLZIjJNRC7wyloBW1S1tKb3FJER3vXZ+fm2LPIBS0iFMx+Em7Oh33Xw43Pw7DGw5HO/IzPGNLCG6AQ+SFWzgMuAJ0Wk6/5crKqjVTVLVbPS0tLqJ8JIFJcIZ/8Trv4vRMe6BeYm3gw7Cv2OzBjTQIJJAHlARsDrDl5ZUFQ1z/u7FPgS6AtsBFJEJKY272nqUKf+cMN3rrN41hvw1JHw42goK635WmNMSAsmAUwHunujdmKBocCkGq4BQERaiEic9zwV6A/MV1UFvgAqRgxdBUzc3+BNHWkS74aLDp8KaT3g47vghZMhz7akNCac1ZgAvHb6m4HJwALgLVWdJyIPiMj5ACJylIjkAhcDo0SkYrH6XkC2iPyC+8F/WFXne8fuBu4QkRxcn8BLdXljphY6HAlXfQAXvwpb18MLp8KHd9hOZMaEKdEQWkY4KytLs7Oz/Q4jMuwohC8edEtOx6fAuU9A7wv8jsoYUwsiMsPri92DzQQ21YtPhrMecUtKtDgI3r4KXhtkO5EZE0YsAZh9a3uIt6TE32DtHLekxGd/g5IdfkdmjDlAlgBMzaKbQP9b4abpcOjF8M2j8Hx/WP6t35EZYw6AJQATvIRWboP6K96DshJ45RyYdKvNHTAmRFkCMPuv6ynwhx/guFtg5uvw5KEw5S+2pIQxIcYSgKmd2AQ44+9w7WToMgC+fxqePwFW/OB3ZMaYIFkCMAcmox9c8ipcNQlKd7jlpidcC9u3+B2ZMaYGlgBM3eh8oltpdMD/wPyJbsnpFd/7HZUxZh8sAZi6E5sAA+6G4VPcyKFXzoGP7oTtm/2OzBhTDUsApu61P9JtTn/U7yF7DPz7CJj+IpSX+R2ZMSaAJQBTP+KS3HLTI76CNr1dTeCl020XMmMaEUsApn61O8wtMPe7F2HLSpcEPv1fW27amEbAEoCpfyJw2MVw6yw48hr47l/weC/47t8QQosRGhNuLAGYhhOXCOc9CUPegLaHwqf3wjvDocD2AjLGD5YATMPrdS5c/g6ceh/MfReeyIT3boSdW/2OzJiIYgnA+EMETrgD/jDNbUc5exy8eCrkL/I7MmMihiUA46/WPd12lFe8D8X5bpXRqfdD6U6/IzMm7AWVAERkoIgsFJEcERlZzfETReRnESkVkcEB5X1E5AcRmScis0VkSMCxV0RkmYjM8h596uSOTGjqcpLbnP6QwfDtEzBmIGxe4XdUxoS1GhOAiEQDzwBnAZnApSKSWeW0lcDVwNgq5duAK1W1NzAQeFJEUgKO36WqfbzHrFrdgQkfye3gwudgyH9g4xIYdQL8/JoNGTWmngRTA+gH5KjqUlXdBYwDBgWeoKrLVXU2UF6lfJGqLvaerwbWA2l1ErkJX73Og+u/hNSDYdIt8OwxkGt7QRtT14JJAO2BVQGvc72y/SIi/YBYYElA8YNe09ATIhK3l+tGiEi2iGTn5+fv78eaUNWyi9uKcsgbULYTXjrDNQut/NHvyIwJGw3SCSwi7YDXgWtUtaKWcA/QEzgKaAncXd21qjpaVbNUNSstzSoPEUXEDRkd8RUcfzsU5MLLZ8G3T0J5eY2XG2P2LZgEkAdkBLzu4JUFRUSSgY+AP6vqtIpyVV2jzk7gZVxTkzG/1awlnHov3Pi9ax6aeh+8OQSK1vkdmTEhLZgEMB3oLiKdRSQWGApMCubNvfPfA15T1QlVjrXz/gpwATB3P+I2kSg+GS5+Bc5+FJZ97foGFnzod1TGhKwaE4CqlgI3A5OBBcBbqjpPRB4QkfMBROQoEckFLgZGicg87/JLgBOBq6sZ7vmGiMwB5gCpwN/r8sZMmBKBftfB9d9ASkcYPwz+exfs2uZ3ZMaEHNEQWowrKytLs7NtNIjxlO5yk8amPQNNW8A5j0Hv37kkYYypJCIzVDWrarnNBDahKyYWBj7kNqZv2dXtRfz6BbDWWhONCYYlABP6Oh4D134CAx+GtXPghZNh2nM2UsiYGlgCMOEhugkccyPc9BN0PQU+GQmvnuc2oTHGVMsSgAkvCalw6Tg4/ylYPROe7gczXrX9iI2phiUAE35E4Igr4aZpkHEUfHArjD4JNuT4HZkxjYolABO+UjrC5e/BhaPcLOLRA+Crf0DJdr8jM6ZRsARgwlt0DBw+1M0b6NQfvngQRp0IeTP8jswY31kCMJEhJQMuG+82ntlVDC+eDt88biOFTESzBGAiS9eT3ZpCmYPgs7/CuEttTSETsSwBmMjTNAUGj4Gz/glLvoCnj4LpL0IIzYo3pi5YAjCRSQSOHgE3fgft+8JHd8LYIbBhsd+RGdNgLAGYyJba3fULnPkQrPgenusPXz9q21CaiGAJwBgROPYmuGUG9BgIn//NrSlUkOt3ZMbUK0sAxlRIagOXvObmDeTNcH0DP71gfQMmbFkCMKaqw4fCH36Ag/rDf/+fW2V0R6HfURlT5ywBGFOdFp3gsrfg1Ptg/vvw1BEw912/ozKmTlkCMGZvoqLghDvg91OheQZMuAbGDYPijX5HZkydCCoBiMhAEVkoIjkiMrKa4yeKyM8iUioig6scu0pEFnuPqwLKjxSROd57/tvbG9iYxqf9kTD8Uzjtflg8BUadAAs/9jsqYw5YjQlARKKBZ4CzgEzgUhHJrHLaSuBqYGyVa1sC9wFHA/2A+0SkhXf4OeA6oLv3GFjruzCmvkXHwPG3w/ApEJcEbw6FSbfAjgK/IzOm1oKpAfQDclR1qaruAsYBgwJPUNXlqjobqLqwypnAp6q6SVU3A58CA0WkHZCsqtPUbUr8GnDBAd6LMfUvva9bWK7/bTDzDbfC6No5fkdlTK0EkwDaA6sCXud6ZcHY27Xtvee1eU9j/BUTC6c/AFd/5JaWfuFUtzm9LTNtQkyj7wQWkREiki0i2fn5+X6HY8xuBx3ragO9L4Bvn4Dnj4f5k/yOypigBZMA8oCMgNcdvLJg7O3aPO95je+pqqNVNUtVs9LS0oL8WGMaSGIa/G40XDkREHjrCvjvn6CsxO/IjKlRMAlgOtBdRDqLSCwwFAj2nzmTgTNEpIXX+XsGMFlV1wCFInKMN/rnSmBiLeI3pnHoMgD+MA2OvRl+GuU2pM9f5HdUxuxTjQlAVUuBm3E/5guAt1R1nog8ICLnA4jIUSKSC1wMjBKRed61m4C/4ZLIdOABrwzgD8CLQA6wBLBxdSa0RcfAmQ/C716A9Qtck9C052zTGdNoiYbQOidZWVmanZ3tdxjG1GzrejdMdNEn0PVUuOBZSGrrd1QmQonIDFXNqlre6DuBjQlJia3h0nFwzmPeMtPHwa8f+R2VMXuwBGBMfRGBo34P138Fyekw7jL44DbYvsXvyIwBLAEYU//SesDvP4PjboUZr7pNZ1b9ZMtMG99ZAjCmIcTEwRl/c4lABF463T22rPQ7MhPBLAEY05A6HAkjvoIz/u6GiY45y4aLGt9YAjCmoSW0guNugas/hLKdMPokeOtK2LzC78hMhLEEYIxf2h3mlpk+fCjkfA4vnQHrf/U7KhNBLAEY46eWneHcJ+D3nwIKY86EpV/5HZWJEJYAjGkMWveCaydDYht4bRBM/jOU7vQ7KhPmLAEY01i07AwjvoCsa+GHp2HMQNi0zO+oTBizBGBMYxKbAOc+DkP+A5uWwPMnwKw3bc6AqReWAIxpjHqdBzd86zqK37/BbUhvm9GbOmYJwJjGKqUjXPUBnPq/sOBDePpI23DG1ClLAMY0ZlHRcMKdcP3X0KKz23Bmyr1QusvvyEwYsARgTChokwnXfuI6iL//N4w5wyaOmQNmCcCYUBET5+YMDPkPbFzqlpie9rx1EJtaswRgTKjpdR7c8A10PAY+uRvGXw7bNtV8nTFVWAIwJhS1OAiGTXCLyi36xG0/ueJ7v6MyISaoBCAiA0VkoYjkiMjIao7Hich47/iPItLJKx8mIrMCHuUi0sc79qX3nhXHWtfljRkT9kTconK/n+qah145B774P2sSMkGrMQGISDTwDHAWkAlcKiKZVU4bDmxW1W7AE8AjAKr6hqr2UdU+wBXAMlWdFXDdsIrjqrr+gO/GmEiU3teNEjpsKHz1MLxwMqyb53dUJgQEUwPoB+So6lJV3QWMAwZVOWcQ8Kr3fAJwqohIlXMu9a41xtS1uCS38fz5T0HhanjxdJj7jt9RmUYumATQHlgV8DrXK6v2HFUtBQqAVlXOGQK8WaXsZa/5595qEgYAIjJCRLJFJDs/Pz+IcI2JUCJwxJWuNtAmEyZc6/YZ2FHod2SmkWqQTmARORrYpqpzA4qHqeqhwAne44rqrlXV0aqapapZaWlpDRCtMSEuqS1c8wmcep+bQfziabBpqd9RmUYomASQB2QEvO7glVV7jojEAM2BwIVLhlLlX/+qmuf9LQLG4pqajDF1IToGTrgDrpwIxeth9MnWJGR+I5gEMB3oLiKdRSQW92NedUGSScBV3vPBwOeqbiiCiEQBlxDQ/i8iMSKS6j1vApwLzMUYU7c6n+A2om/V1TUJvX2NzRkwlWpMAF6b/s3AZGAB8JaqzhORB0TkfO+0l4BWIpID3AEEDhU9EVilqoF10DhgsojMBmbhahAvHOjNGGOq0aorXDsFTrkXFnwAzx4DCz/xOyrTCIiG0JjhrKwszc7O9jsMY0LX2jnw3g2wbi70vRwGPuxGEJmwJiIzVDWrarnNBDYmkrQ9FK77wq0wOmus6xvYvNzvqIxPLAEYE2liYt0eA1d9AMX5bs5A7gy/ozI+sARgTKTqdDwMnwIx8fDS6TD9RVtGIsJYAjAmkqX1gBu+hm6nwUd3uvWEttqEy0hhCcCYSNe0BQwdC+c8Dnk/w+gB8PPrVhuIAJYAjDFu4thRw+HqDyGpDUy6Gf57F5SV+h2ZqUeWAIwxu3XIguFT3TLT01+AV8+DDTl+R2XqiSUAY8yeoqLcRjMXjoL1810HcZ6NEgpHlgCMMdU7fCiM+ALiEmHMQPj8QWsSCjOWAIwxe9eyC1z3JWReAF//w40Sss1mwoYlAGPMviW0gotegItegvULYNRJMGeC31GZOmAJwBgTnEMHw22zIKMfvHsdTP6zrSwa4iwBGGOC16wlDHsbDr0Epj3n5gxsXOJ3VKaWLAEYY/ZPbAL8bhRcOxl2bYWXzoBFU/yOytSCJQBjTO1kHOWSQEIajL3YjRIqL/c7KrMfLAEYY2ovtTtc/xX0udyNEnr7KthV7HdUJkiWAIwxByYmDgY97SaPLfjANQlZv0BICCoBiMhAEVkoIjkiMrKa43EiMt47/qOIdPLKO4nIdhGZ5T2eD7jmSBGZ413zbxGROrsrY0zDEnHLRwybAIV58PwJ8Mu4mq8zvqoxAYhINPAMcBaQCVwqIplVThsObFbVbsATwCMBx5aoah/vcUNA+XPAdUB37zGw9rdhjGkUup8GN3wH6X3hvevhneugZLvfUZm9CKYG0A/IUdWlqroLGAcMqnLOIOBV7/kE4NR9/YteRNoByao6Td2mxK8BF+xv8MH6LmcDU+evY0n+VnaVWieVMfWqeXu4ciIMuAfmvA1jzoT5k2x56UYoJohz2gOrAl7nAkfv7RxVLRWRAqCVd6yziMwECoG/qOo33vm5Vd6zfXUfLiIjgBEAHTt2DCLc33r2yxy+y9kIQHSU0KFFUzqnJtCpVQJd0tzfzqkJpKc0JTrKWqKMOWDRMTBgpNtw5pN74K0r4MS74JS/+B2ZCRBMAjgQa4COqrpRRI4E3heR3vvzBqo6GhgNkJWVVat/Qjxz2REs3VDM8g3FLNtQXPn8p2Wb2LarrPK82OgoDmrVjM6pCZWPTqkJdElNIC0pDuumMGY/9b4Qep4HH/4Rvv6n24P46BuhdU+/IzMElwDygIyA1x28surOyRWRGKA5sNFr3tkJoKozRGQJcLB3foca3rPOpDSL5YiOsRzRscUe5apKftFOlnqJYbmXHJZtKObLhfnsKtvdXJQQG02ngIQQmBxSmsXWV+jGhL7oGDjvX27v4ewxMPMNuPB5t7SE8VUwCWA60F1EOuN+pIcCl1U5ZxJwFfADMBj4XFVVRNKATapaJiJdcJ29S1V1k4gUisgxwI/AlcBTdXNLwRMRWifH0zo5nmO6tNrjWFm5snrLdpcYNhazNN8lhrl5BXw8Zw3lAXWRlGZNXI3Ba0rqHNCslBBX35UsY0JAVDSc8yicdLebK/De9W5GcY+z/I4sookG0TEjImcDTwLRwBhVfVBEHgCyVXWSiMQDrwN9gU3AUFVdKiIXAQ8AJUA5cJ+qfuC9ZxbwCtAU+Bi4RWsIJisrS7Ozs2t1o3VpV2k5qzZvY5mXFJZtLGZZvksUawp27HFu66Q4uqYl0iUtYY+/7VOaEmX9DSYS7SiA1wbBmtmuT6D/H90mNKbeiMgMVc36TXkwCaCxaCwJYF+27SplxcZtLjFscDWHpRu2smT9Vgp37N5MIy4mis6pCb9JDl3SEkm0WoMJdzu3wqRbYN67cMhg1yQU3cTvqMLW3hKA/dLUsWaxMfRql0yvdsl7lKsqG4t3sWT9VpZuKGZp/laW5Bczb3UBH8/ds0mpTXIcXVIT6do6wfubSJfUBKs1mPARlwiDx0DbQ+Gzv8LWdXD2P6F1L78jiyhWA2gEdpaWsXLjNpbkF7Mkf2tQtYauXm3Bag0m5P38Gkz5i9tu8rJx0PlEvyMKO9YEFIL2VmtYmr+VlZu2/abWUNGM1L11Et1aJ9KtdSKtbfiqCQWFa+D1CyB/IRw2BM55zNUSTJ2wBBBmqqs1LMnfypL8rRQF1BqS4mPo1jqR7l5CcM+TrDnJND7FG+Gbx+DH56DtYXDZeEhq63dUYcESQISomNuweP1WcrzH4vVF5KwvZsPWnZXnxTeJoktqIt3bJNItzfvbOpGDWiXQJNpGZBgfLfwE3r7arTI6eAx0O9XviEKeJQDDlm27ApLC7gSRt2X3Yl0xUUKn1AS6pXm1hTaJXp9DIk1jo32M3kSU/EUw4RrYsBjOfdyNFGoS73dUIcsSgNmr4p2lLM0vJie/iMXrdieGFZu2UeZ1NIhAhxZNvdpCEt3S3Oikbq0Tad7Uhu+ZerBtE7wxGPJmQMsucMV70KKT31GFJEsAZr/tLC1jxcZtlUnBNSW5DunAVVVbJ8Xt7mdok8TBrRPp0TbJlsgwB66sBBZNhok3QdMWMHwKJLb2O6qQYwnA1JmycmXVpm2uppC/1SWI/K3krCuiOGBxvbSkOHq0SaJ7m0QObpPEwd7z5HirMZj9tOonePV8iImFE/6f23zGRrcFzRKAqXeqypqCHSxcV8TidUUsWreVRetcs9L2kt2JoV3zeC8huOakiiTRLNbmMph9WD0LvngIFk+GfiPgrH9YEgiSzQQ29U5ESE9pSnpKU07usbuaXl6u5G7ezqJ1RSxaX8SitS45/LB04x5NSRktm3Jw6ySXFNomVs5niG9inc8GSO/jhoZO+Qv88DTsKHQLzMUl+R1ZyLIEYOpdVJTQsVUzOrZqxmmZbSrLy8qVFRuLWbRuK4vXFXk1h618vTifkjJXM40SOKhVAt29foXuXs2hS2oisTE2XDXiiLjN5+OS4cv/g3Vz4fJ3IalNzdea37AmINPolJSVs3xDcWUTUsVj+cbdo5JiooTOqQn0bJdMz7auGalnOzfBzWY+R4icz2D85W6+wCn3wlHD/Y6o0bI+ABPydpaWsTS/uDIhLFxbxK9ri8jdvHseQ1JcDAe3TaKn9+jRNpkebZNsqGq4Wj0LPr0Xln0NZzwIx93sd0SNkiUAE7aKdpSwaJ1LBr+uqUgMhXsspJfePJ6e7VwycMkhmS5pNus5LJSXuUlj8ydCj3PgrEcgJaPm6yKIJQATUSpHJHm1hF/XFrJwbRFL8rdW9i80iRa6piVW1hR6tnXNSG2T460ZKdSU7oSv/gE/jYYmTeHKSbbvcABLAMbgdnNbumErC9cWsWBNEQu9xLA6YCe35PgYerZNpme7pMoaw8Ftkkiy+QuN3/oFbrex8lK45DXodLzfETUKlgCM2YeCbSUsXOdqCr+udc1IC9cWsXXn7makDi2a0qtdMpnehj+Z7ZLJaGmdzo3OxiVuCYlNy+C0+6H/bRE/X+CAEoCIDAT+hdsT+EVVfbjK8TjgNeBIYCMwRFWXi8jpwMNALLALuEtVP/eu+RJoB1T04J2hquv3FYclANOQVN38hYo+hQVri/h1TSFLNxRT8X+bpLgYerZLqkwKvbx+Bpu74LNdxW75iHnvwZHXwNmPQnTkjnqv9UQwEYkGngFOB3KB6SIySVXnB5w2HNisqt1EZCjwCDAE2ACcp6qrReQQYDLQPuC6Yapqv+imURIRMlo2I6PlnvMXtu8qY+G6IuavLmTBmkLmrylkwozcymUwogS6pCUGJIUkMtOTaZ1kq1k2mNgEuGiMWzzu2ydg0xIY/DIkpPodWaMSTErsB+So6lIAERkHDAICE8Ag4H7v+QTgaRERVZ0ZcM48oKmIxKnqTowJUU1jo+mTkUKfjJTKsvJyZeWmbSxYszspzFixmUm/rK48JzUxtrLpKDPdJYcuqQnE2Eik+hEV5ZqAWnWHD2+H0QNgyH/cjGIDBJcA2gOrAl7nAkfv7RxVLRWRAqAVrgZQ4SLg5yo//i+LSBnwDvB3raY9SkRGACMAOnbsGES4xjS8KG8fhU6pCZx1aLvK8i3bdrFgTVFlUliwppCXv1vOrjK3BEZsTBQ92iS5WkJFjSE92RbMq0t9h7nN5sdfDmPOhPP+DYcP8TuqRqFBGsVEpDeuWeiMgOJhqponIkm4BHAFrh9hD6o6GhgNrg+gAcI1ps6kNIvl2K6tOLZrq8qykrJyluRvrWxCWrCmiKkL1vNWdm7lOR1aNCWzXTKHtG9O73T31/Z3PgDtj4ARX7mdxt4bAWt+gVP/N+I3mQkmAeQBgbMqOnhl1Z2TKyIxQHNcZzAi0gF4D7hSVZdUXKCqed7fIhEZi2tq+k0CMCbcNImOcsNM2yZXlqkq64t2Mn+1qynMX1PI/NWFTJm/rvKc1MRYMtObc0h6Mr3Tm3NI+2Q6tmxmSSFYiWluU5lPRsK0Z2D1TBj6BjRr6XdkvgkmAUwHuotIZ9wP/VDgsirnTAKuAn4ABgOfq6qKSArwETBSVb+rONlLEimqukFEmgDnAlMP9GaMCVUiQpvkeNokx3Nyz90rqRbtKGHBmiLmrS5g3upC5uYVMDpnA6XemkhJcTH0Sk/mkPTdNYWuadavsFcxsW6LyYOOg/dvhOePd4vJReiksWCHgZ4NPIkbBjpGVR8UkQeAbFWdJCLxwOtAX2ATMFRVl4rIX4B7gMUBb3cGUAx8DTTx3nMqcIeqlrEPNgzUGNhRUsbidVuZu7qgMjEsWFPIjhLXrxAXE0XPtkn0rmg+Sm9uQ1Ork/czvDkUynbBiXe5PQaiw7PvxSaCGRPGSsvKWbah2CWFvEJXW1hdQJG3HlJ0lNAtLZHe7V3zUe/0ZHqnJ9vs5o1L4N3r3L7Dvc6Di19zo4fCjCUAYyJMxUS2easLmJtX6P6uLiS/aPdAvC6pCRzSvjmHdWhe2eEckUnh+6fcRjN9hrmhow2577CqS0Sp3ertIywBGGMAWF+0g3l5rj9hdl4Bc/MKWOOthSQCnVMTOLR988pH7/bNSYwL81m0qjD1fvjuSYhvDpe9BR2PaZjPXjQFxl4MF7/ilrdu1gr631qnH2EJwBizV/lFO5mbV8CcikduAWsLdyeFLl5ScLWFFHqnJ5MQjklh3Xx46woozodh70DGUfX3WUu/hB0F7jHpFuh9oVu6AuD+gjr9KNsT2BizV2lJcZzcs/UeI5AqksLsXJcUpi3dxPuz3MzmiqRwWIcUDqmoKYRDUmiT6YaKvno+vHKO23P4iCvr/nNW/OBWLQU453H3t2T3irSoNsgCdiH+bRlj6kt1SWF90Q5XU8gtZE7eFr5fsoH3ZrppQSLQNS1xd/NRh+ZktgvBpJDSEX7/Gbwz3P3LfPNyt+VkXf4gvzxw9/ON3vSoqIBRWqU7G2SSWoh9M8YYP7VOiueUnvGc0nP34njrC3dUNh3NzSvgu5zdSSFKoFvrRA7vkMJhGSn06ZBCj7ZJxMY08pE2Ca1g2AT4753wzWNQtBbO+1fdDBMtrzLafeta97c0YJWckm2WAIwxjV/r5HhOTY7n1F6/TQqzcwuYnbuFz39dz9sz3FIXsTFRZLZL5vAOzTk8I4XDM1Lo3CqBqKhGNqM5OgbOfRKS2sGX/werfoShYyGtx4G9787CPV9v9VbB3xawdFrJNqD+ZyhbAjDG1LmqSaFiSOrs3AJ+yd3CL6u28PaMXF79YQXgZjQfluE6mA/vkMLhGc0bx9acIjBgJLTr45qDXj7bzSTudX7tm4S2b9nzdbH3w781f3dZyfbdzxdNhs0r4OgRtfu8fbAEYIypd4F7K5xzmFsttaxcWZK/lVmrXEKYnVvAC18vrVzmonVSHId1SKFPhqspHNY+hebNfJqj0GMgXPMxvHY+vHUlDPgfGHB37d5rx5Y9X2/fvOdfcBvaVBh7iftrCcAYEy6io4SD27j9li/JcutN7igpY8GawsqEMCt3C1MX7F4Qr3NqAod1aF5ZS+id3rzhlrhI7Qa3/Azv3wBfPgQbc+CcxyA+ueZrAwX+uMPuPoCSgPLAGkCF0p0QE7d/n1UDSwDGmEYjvkk0fTu2oG/HFpVlBdtLmJu3u+nop2WbmOgNR43xksjhGa6mcFiHFLq3Tqy/xfCaxLvO4F3bYM5bbgz/4DEQlxj8e+wsqvmckuLflhWtcTuc1SGbCGaMCTnrCndU1hIqEkOht+5R0ybRHNahuZdIUuibkULr5HoYUfP1P+Hzv7uZuyO+dMNHa7JrGzzUrubzDurvNrE55zG4v7kru3MRJLXZ93V7YTOBjTFhS1VZvnEbs3O3MHPlFmat2sK81QWUlLnft/YpTV0y8JJC7/Rk4mIOsOmovBwWTISJN0NcMpz/FHQ/bd/XrF8Az+7HEhP3boS/tYKTRsLJ99Q6VJsJbIwJWyJC59QEOqcmMKhPe8D1J8xfU8jMlVuYuXIzM1du4cPZawCIjY4iMz15d1LISKFDi6b7N+ooKsot39C0BXxwG4wfBmc/Ckdc8dtzV/wAGUf/tv2/Jt886v7GNtu/64JkNQBjTMRYX7iDmau2VCaF2bkFbC9xE7NSE+O8hJBC34wWHNahefCzmDcvh1fPgy0r3dIRZz8GUTHwVF93DCC9r9uFrDbO+ucBjQKyGoAxJuK1To7nzN5tObN3W8Dto7BwXZGXELYwc9VmPvW24YwS6NE2ubIfoW/HFnRJ3cuEtRad4A/T4PkT4OfXYOU0uOL93T/+UPsff+DXdVvpVFJW5yOerAZgjDEBtmzbxayKWsIqV1Oo2FgnOT6GPl6TUd+OKfTJSCGlWeyeb/DuCJg9vsbPmVh2HE+XXsCncX+q8dy/llzBJTc/RK92+znk1HNANQARGQj8C7d944uq+nCV43G4Dd2PxG0GP0RVl3vH7gGGA2XArao6OZj3NMYYP6Q0i2VAj9YM6OEWwSsvV5ZuKHb9CF5ieOrzxXjz1eiSlkDfjBb08WoKPQc9T0xUDMx6Y5+fc1vJTUBwfQ5NKKVn26QDua1q1ZgARCQaeAY4HcgFpovIJFWdH3DacGCzqnYTkaHAI8AQEcnEbSLfG0gHporIwd41Nb2nMcb4LipK6NY6kW6tE7nYm7BWvLOU2bkFzFzlOpe/WrSed352ax3FN4ni0PaX8zZ7JoAzdz7M5LiRASVCdEBz0paoFqSUb+bzsj78Ut6V25u8U3mshJh6WRYjmBpAPyBHVZcCiMg4YBAQ+GM9CLjfez4BeFpctIOAcaq6E1gmIjne+xHEexpjTKOUEBfDsV1bcWzXVsDutY5mrtrCLK8v4bySR/igye7lIpZpO0qJJoYy+uwYxY0DunL7aQfD393x5p37wpLPybrtTR5/8Udu3/EO2eUH8235IST2r/tlICC4BNAeWBXwOhc4em/nqGqpiBQArbzyaVWube89r+k9ARCREcAIgI4dg5hoYYwxDSxwraPzD08HYFfpsfy64TJWL19Ep+JZ/Hz8uUQX/ciyL1/h7oxjuTgrg5joKMqvmEhZYjuaNG8Lq6aTnJrOuNvPY/X88ZQnHMplbVvVz0Q2QmAUkKqOBkaD6wT2ORxjjAlKbEwUPdsm07NtFuD1v8Z1p/PgB+kccF5U1wFULlzhTSRLjIshse9A0us5xmAWzMgDMgJed/DKqj1HRGKA5rjO4L1dG8x7GmOMqUfBJIDpQHcR6SwisbhO3UlVzpkEXOU9Hwx8rm586SRgqIjEiUhnoDvwU5DvaYwxph7V2ATktenfDEzGDdkco6rzROQBIFtVJwEvAa97nbybcD/oeOe9hevcLQVuUtUygOres+5vzxhjzN7YRDBjjAlze5sI1sh3ZjbGGFNfLAEYY0yEsgRgjDERyhKAMcZEqJDqBBaRfGBFLS9PBTbUYTihwO45Mtg9R4YDueeDVDWtamFIJYADISLZ1fWChzO758hg9xwZ6uOerQnIGGMilCUAY4yJUJGUAEb7HYAP7J4jg91zZKjze46YPgBjjDF7iqQagDHGmACWAIwxJkJFRAIQkYEislBEckRkZM1XNH4ikiEiX4jIfBGZJyK3eeUtReRTEVns/W3hlYuI/Nv7bzBbRI7w9w5qT0SiRWSmiHzove4sIj969zbeW2Icbxny8V75jyLSydfAa0lEUkRkgoj8KiILROTYcP+eReR273/Xc0XkTRGJD7fvWUTGiMh6EZkbULbf36uIXOWdv1hErqrus/Ym7BNAwKb2ZwGZwKXeZvWhrhS4U1UzgWOAm7z7Ggl8pqrdgc+81+Duv7v3GAE81/Ah15nbgAUBrx8BnlDVbsBmYLhXPhzY7JU/4Z0Xiv4FfKKqPYHDcfcett+ziLQHbgWyVPUQ3JLxQwm/7/kVYGCVsv36XkWkJXAfbkvdfsB9FUkjKKoa1g/gWGBywOt7gHv8jqse7nMicDqwEGjnlbUDFnrPRwGXBpxfeV4oPXC7x30GnAJ8CAhudmRM1e8bt9/Esd7zGO888fse9vN+mwPLqsYdzt8zu/cYb+l9bx8CZ4bj9wx0AubW9nsFLgVGBZTvcV5Nj7CvAVD9pvbt93JuSPKqvH2BH4E2qrrGO7QWaOM9D5f/Dk8CfwLKvdetgC2qWuq9Dryvynv2jhd454eSzkA+8LLX7PWiiCQQxt+zquYBjwIrgTW4720G4f09V9jf7/WAvu9ISABhTUQSgXeAP6pqYeAxdf8kCJtxviJyLrBeVWf4HUsDigGOAJ5T1b5AMbubBYCw/J5bAINwyS8dSOC3TSVhryG+10hIAGG7Ab2INMH9+L+hqu96xetEpJ13vB2w3isPh/8O/YHzRWQ5MA7XDPQvIEVEKrY3Dbyvynv2jjcHNjZkwHUgF8hV1R+91xNwCSGcv+fTgGWqmq+qJcC7uO8+nL/nCvv7vR7Q9x0JCSAsN6AXEcHtxbxAVR8PODQJqBgJcBWub6Ci/EpvNMExQEFAVTMkqOo9qtpBVTvhvsfPVXUY8AUw2Dut6j1X/LcY7J0fUv9SVtW1wCoR6eEVnYrbYztsv2dc088xItLM+995xT2H7fccYH+/18nAGSLSwqs5neGVBcfvTpAG6mg5G1gELAH+7Hc8dXRPx+Oqh7OBWd7jbFzb52fAYmAq0NI7X3CjoZYAc3AjLHy/jwO4/wHAh97zLsBPQA7wNhDnlcd7r3O84138jruW99oHyPa+6/eBFuH+PQN/BX4F5gKvA3Hh9j0Db+L6OEpwNb3htflegWu9e88BrtmfGGwpCGOMiVCR0ARkjDGmGpYAjDEmQlkCMMaYCGUJwBhjIpQlAGOMiVCWAIwxJkJZAjDGmAj1/wFjuFCz1ckd1wAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(model_1.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uen68Uyt3_sZ",
        "outputId": "678d2ccf-bdbd-428c-98e8-9a38c9f79ccd"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[2.3004, 1.8992]], device='cuda:0', requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0.3994], device='cuda:0', requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  preds = model_1(X_test)\n",
        "preds = torch.tensor(preds).cpu().numpy()\n",
        "X_train=torch.tensor(X_train).cpu().numpy()\n",
        "X_test=torch.tensor(X_test).cpu().numpy()\n",
        "y_test=torch.tensor(y_test).cpu().numpy()\n",
        "y_train=torch.tensor(y_train).cpu().numpy()\n",
        "model_1.to(\"cpu\")\n",
        "\n",
        "\n",
        "plot_predictions(train_data = X_train[:, 0],\n",
        "                     train_labels=y_train, \n",
        "                     test_data=X_test[:, 0],\n",
        "                     test_labels=y_test, predictions=preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "bXq-zmnv5MJX",
        "outputId": "b4f1df27-69c8-44d5-d878-78f4265e88a7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-f56e9e2d3655>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  preds = torch.tensor(preds).cpu().numpy()\n",
            "<ipython-input-51-f56e9e2d3655>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_train=torch.tensor(X_train).cpu().numpy()\n",
            "<ipython-input-51-f56e9e2d3655>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_test=torch.tensor(X_test).cpu().numpy()\n",
            "<ipython-input-51-f56e9e2d3655>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_test=torch.tensor(y_test).cpu().numpy()\n",
            "<ipython-input-51-f56e9e2d3655>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_train=torch.tensor(y_train).cpu().numpy()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# 1. Create models directory \n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Create model save path \n",
        "MODEL_NAME = \"model_1.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# 3. Save the model state dict \n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
        "           f=MODEL_SAVE_PATH) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7MmU9dX5XY3",
        "outputId": "c9632865-0bc4-4df5-fcf4-94ff35bbb969"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to: models/model_1.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a fresh instance of LinearRegressionModelV2\n",
        "loaded_model_1 = LinearRegessionModel1()\n",
        "\n",
        "# Load model state dict \n",
        "loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "\n",
        "# Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions)\n",
        "loaded_model_1.to(device)\n",
        "\n",
        "print(f\"Loaded model:\\n{loaded_model_1}\")\n",
        "print(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ox_X6_1VnEX",
        "outputId": "e628aede-6c90-4758-c7b2-c66ad055910d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model:\n",
            "LinearRegessionModel1(\n",
            "  (linear_layer): Linear(in_features=2, out_features=1, bias=True)\n",
            ")\n",
            "Model on device:\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hfK_GflOV4d8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}